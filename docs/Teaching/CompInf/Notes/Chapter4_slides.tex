\documentclass[11pt, handout]{beamer} % To remove reveals for printing.
%\documentclass[12pt]{beamer}


\usepackage{color}
\usefonttheme{professionalfonts} % using non standard fonts for beamer
\usefonttheme{serif} % default family is serif
%\usepackage{fontspec}
%\setmainfont{Liberation Serif}
\definecolor{blank}{gray}{1} %1 to hide



%\usetheme{CambridgeUS}

\newcommand{\bx}{\mbox{$\mathbf{x}$}}
\usepackage{amsfonts}           % let's be fancy:
\def\BE{\mathbb{E}}             % E(X) -- expectation
\def\BP{\mathbb{P}}             % P(X=1) -- probability
\usepackage{graphicx}           % import and rescale EPS figures
%\usepackage{pnas}
%\usepackage{showkeys}           %shows references
\usepackage{setspace, amsthm}
\usepackage{graphicx}
%\graphicspath{{./}{../Figures/} {../Graphs/} }

\graphicspath{{.}{Figures/}}

\usepackage{amsbsy,amssymb,amsmath, amsthm, psfrag}


\def\htheta{{\hat{\theta}}}
\newcommand{\bs}{\boldsymbol}
\newcommand{\ds}{\displaystyle}
\def\b{$\bullet $}
\def\ds{\displaystyle}
\def\BE{\mathbb{E}}
\def\Var{\mathbb{V}\mbox{ar}}
\def\cov{\mathbb{C}\mbox{ov}}
\def\cor{\mathbb{C}\mbox{or}}

\def\b{$\bullet $}
\def\ds{\displaystyle}
\def\BE{\mathbb{E}}
\def\Var{\mathbb{V}\mbox{ar}}
\newcommand{\re}{\mathrm{e}}
\newcommand{\rd}{\mathrm{d}}

\def\pgfb[#1][#2]{\pi(#1|#2)}
\def\vv{$^\wedge$}
\def\pgfa[#1][#2][#3][#4][#5][#6]{\pi(#1_{0:#2} #3|#4_{0:#5} #6)}

\def\pgf[#1][#2][#3][#4]{\pi(#1_{0:#2} |#3_{0:#4},\theta, \psi )}
%pgf for when we have x and z on separate sizes, with phi and theta in the conditional 
\def\xt {x_{0:t}}
\def\xtp{x_{0:t+1}}
\def\zt{z_{0:t}}
\def\ztp{z_{0:t+1}}

\def\rd {\mathrm{d}}


\addtolength{\topmargin}{0cm}  % decrease the margins
\addtolength{\textheight}{0cm}
\addtolength{\oddsidemargin}{0cm}
\setlength{\evensidemargin}{\oddsidemargin}
\addtolength{\textwidth}{0cm}

\def\ignore#1{{}}
\def\stnote#1{\textbf{\large #1}\marginpar{$\spadesuit$}}
\def\no{\noindent}
\def\etal{{\it et al. }}
\def\CD{\mathcal{D}}
\def\CN{\mathcal{N}}
\def\bm#1{{{\mbox{\boldmath $#1$}}}}  
\def\Var{\mathbb{V}\mbox{ar}}
\def\rd{\rm{d}}
   \newcommand{\eps}[1]{\(\epsilon_#1 \)}
\def\bias{\mbox{bias}}
\def\htheta{\hat{\theta}}
   
  \def\se{\mbox{se}} 
\def\hF{{\hat{F}}}
 \def\BI{\mathbb{I}}
 \def\Xi{X_{i}}
 \def\xi{x_{i}}
 
\newcommand{\e}{\varepsilon}
\newcommand{\al}{\alpha}
\newcommand{\s}{\sum_{i=1}^n}
\newcommand{\shat}{\hat{\sigma}^2}
\newcommand{\bhat}{\hat{\bbeta}}
\newcommand{\ahat}{\hat{\alpha}}
\newcommand{\be}{\mbox{\boldmath{$\varepsilon$}}\unboldmath}
\usepackage{bm}
%\newcommand{\bx}{\mbox{$\mathbf{x}$}}
\newcommand{\bz}{\mbox{$\mathbf{z}$}}
\newcommand{\xvec}{\mbox{$\bx_1,\ldots,\bx_n$}}
\newcommand{\yxvec}{\mbox{$y_1=\eta(\bx_1),\ldots,y_n=\eta(\bx_n)$}}
\newcommand{\by}{\mbox{$\mathbf{y}$}}
\newcommand{\boldm}{\mbox{$\mathbf{m}$}}
\newcommand{\bh}{\mbox{$\mathbf{h}$}}
\newcommand{\bt}{\mbox{$\mathbf{t}$}}
\newcommand{\bbeta}{\pmb{\beta}}
\newcommand{\bdelta}{\pmb{\delta}}
\newcommand{\btheta}{\pmb{\theta}}
\newcommand{\bo}[1]{\mbox{$\mathbf{#1}$}}
\newcommand{\yxpvec}{\mbox{$y_1'=\eta(\bx_1'),\ldots,y_{n'}'=\eta(\bx_{n'}')$}}
\newcommand{\xpvec}{\mbox{$\bx_1',\ldots,\bx_{n'}'$}}
\newcommand{\ri}{\mbox{$^{131}I$} }
\newcommand{\bX}{\bo{X}}
\newcommand{\bY}{\bo{Y}}
\newcommand{\betahat}{\mbox{$\hat{\bbeta}$}}
\newcommand{\sighat}{\mbox{$\hat{\sigma} ^2$}}

 \def\cor{\mbox{cor}}
   
\def\endexample{{\begin{flushright} $\square$ \end{flushright}}}

\setbeamertemplate{footline}[frame number]{}

%gets rid of bottom navigation symbols
\setbeamertemplate{navigation symbols}{}

%gets rid of footer
%will override 'frame number' instruction above
%comment out to revert to previous/default definitions
%\setbeamertemplate{footline}{}


\begin{document}

\frame{\frametitle{MAS472/6004: Computational Inference} 
\begin{center}
\Huge{ 
Chapter IV\\
Bayesian inference}
\end{center}
}

\frame{\frametitle{Bayesian inference}

Unnormalised densities frequently occur when we are doing Bayesian inference.

\medskip

Suppose we are interested in some posterior expectation, for example, the posterior mean:

$$I = \BE(\theta | x) =  \int \theta f(\theta | x) {\rm d} \theta$$

where $$f(\theta | x) = \frac{f(\theta) f(x|\theta)}{f(x)} \qquad \mbox{ by Bayes theorem.}$$
The denominator $f(x) = \int f(\theta) f(x|\theta) {\rm d} x$ is often intractable and unknown, and so we instead work with the unnormalised density

$$f_1(\theta|x) = f(\theta) f(x|\theta) = \mbox{ prior } \times \mbox{ likelihood}$$

}


\frame{ \frametitle{Choice of $g$ and the normal approximation
}

If wish to sample from $f(\btheta|\bx)$, could choose
$g(\btheta)=f(\btheta)$. If we do not know $f(\bx)$ then have
\begin{equation*}
\tilde{w}_i = f(\bx|\btheta_i) \qquad \mbox{and } \quad w(\btheta_i)=\frac{f(\bx|\btheta_i)}{\sum_{i=1}^n
f(\bx|\btheta_i)}.
\end{equation*}


\begin{itemize}
 \item Simulate $\btheta_1, \ldots, \btheta_n $ from the prior $f(\btheta)$
 \item Set $\tilde{w}_i = f(\bx | \btheta)$
 \item Set $w_i = \tilde{w}_i /\sum \tilde{w}_i$ and estimate $\BE(\btheta | \bx)$ by 
 $$\sum_{i=1}^n w_i \btheta_i$$
\end{itemize}






 \pause This is inefficient if the prior is very different to the posterior as we will spend too much time sampling $\btheta_i$ where the likelihood is very
small, and so the weights $w(\btheta_i)$ will also be very small.\pause

\medskip

If this is the case, then the effective sample size will be small, and our estimates of $E(\btheta|\bx)$ will be dominated by just a few of the $\btheta$ samples. }


\frame{

A more efficient alternative to using the prior distribution for $g$, is to build a normal approximation to the posterior and use this as $g$

\medskip

Let $h(\btheta)=\log f(\btheta|\bx)$.
Now define
$\boldm$ to be posterior mode of $\btheta$, so $\boldm$ maximises
both $f(\btheta|\bx)$ and $h(\btheta)$.\pause \\

\medskip
We may need to use numerical
optimisation (such as the {\tt optim} command in R) to find $\boldm$, but note that we don't need to know $f(\bx)$ to do this.


\pause
We can then use a Taylor expansion of $h(\btheta)$ around $\boldm$
\begin{equation*}
h(\btheta)=h(\boldm)+ (\btheta-\boldm)^T\bh'(\boldm)
+\frac{1}{2}(\btheta-\boldm)^T M (\btheta-\boldm) +\ldots
\end{equation*}
to build a Gaussian approximation to the posterior (known as the Laplace approximation).

Here, $h'(\boldm)$ the vector of first derivatives of $h(\btheta)$,
and $M$ the matrix of second derivatives of $h(\btheta)$, both
evaluated at $\btheta=\boldm$.

}

\frame{

Since $\boldm$ maxmises $h(\boldm)$ we have $h'(\boldm)=\bo{0}$.
Hence
\begin{equation} f(\btheta|\bx)=\exp\{h(\btheta)\}\simeq
\exp\{h(\boldm)\}\exp\left\{-\frac{1}{2}(\btheta-\boldm)^T V^{-1}
(\btheta-\boldm)\right\}, \label{taylor}
\end{equation}
where $-V^{-1}=M$.\\ \pause 

\medskip
Thus, our approximation of $f(\btheta|\bx)$ is a
multivariate normal distribution, mean vector $\boldm$, variance matrix
$-M^{-1}$. This will be a good approximation if posterior mass concentrated
around $\boldm$.\\ \pause 

\medskip
NB: We do not need $f(\bx)$ to obtain $M$, since
\begin{equation*}
h(\btheta)=\log f(\btheta|\bx)=\log f(\btheta) + \log f(\bx|\btheta)
- \log f(\bx),
\end{equation*}
so $\log f(\bx)$ will disappear when we differentiate $h(\btheta)$.
\\ %\pause If (\ref{taylor}) is a good approximation to $f(\btheta|\bx)$
%everywhere, then it can be used directly to give approximate
%summaries from $f(\btheta|\bx)$.

}

\frame{

\frametitle{Assessing convergence} Suppose we wish to estimate
$\BE\{r(\btheta)|\bx\}$ for some $r(\btheta)$.  \pause If $f(\bx)$
known, then
\begin{equation*}
\hat{\BE}\{r(\btheta)|\bx\}=\frac{1}{n}\sum_{i=1}^n
r(\btheta_i)w(\btheta_i),
\end{equation*}
and can use central limit theorem to obtain a confidence interval
for $\BE\{r(\btheta)|\bx\}$, as in MC integration.% \pause If $f(\bx)$
%unknown, then
%\begin{equation*}
% \hat{E}\{r(\btheta)|\bx\}=\frac{\sum_{i=1}^n
% r(\btheta_i)\tilde{w}(\btheta_i)}{\sum_{i=1}^n
% \tilde{w}(\btheta_i)},
% \end{equation*}
% then CLT can only be applied under stricter conditions, outside
% scope of course.\pause 

We can check our estimate by
\\1) Increasing the sample size $n$ to
check the stability of any estimate.  \pause \\2) Increasing the standard deviation in
the $g(\btheta)$ density, to check stability to the choice of $g$, e.g., if we're using
a normal approximation, we could multiply $V$ by 4 etc.

}

\frame{ \frametitle{ Example: leukaemia data}Patients
suffering from leukaemia are given a drug, 6-mercaptopurine (6-MP),
and the number of days $x_i$ until freedom from symptoms is recorded
of patient $i$:
$$6^*,6,6,6,7,9^*,10^*,10,11^*,13,16,17^*,$$ $$19^*,20^*,22,23,25^*,32^*,32^*,34^*,35^*. $$
A * denotes censored observation.

 \pause

Will suppose that time $x$ to the event of interest follows a
\textit{Weibull} distribution:
\begin{equation*}
f(x|\alpha,\beta)=\alpha\beta(\beta x)^{\alpha-1}\exp\{-(\beta
x)^{\alpha}\}
\end{equation*}
for $x>0$. \\  \pause For censored observations, we have
\begin{equation*}
P(x>t|\alpha,\beta)=\exp\{-(\beta t)^{\alpha}\}.
\end{equation*}

}

\frame{\frametitle{ Example: leukaemia data}\framesubtitle{Likelihood}

Define \begin{itemize}
        \item 
$d$: number of uncensored observations, \item $\sum_u \log x_i$:
sum of logs of all uncensored observations.

       \end{itemize}
Writing
$\btheta=(\alpha,\beta)^T$, the log likelihood is then given by
\begin{equation*}
\log f(\bx|\btheta)=d\log \alpha + \alpha d \log \beta
+(\alpha-1)\sum_u \log x_i-\beta^{\alpha}\sum_{i=1}^n x_i^{\alpha}.
\end{equation*}

\pause

Suppose our prior distributions for $\alpha$ and $\beta$ are both exponential with 
\begin{eqnarray*}
f(\alpha)&=&0.001\exp(-0.001 \alpha),\\
f(\beta)&=&0.001\exp(-0.001 \beta).
\end{eqnarray*}

}

\frame{\frametitle{ Example: leukaemia data}\framesubtitle{Building an approximation to the posterior}

1) \textbf{Obtain the posterior mode of} $\btheta$. Maximise log
posteior, i.e.
$$
h(\btheta)=d\log \alpha + \alpha d \log \beta +(\alpha-1)\sum_u \log
x_i-\beta^{\alpha}\sum_{i=1}^n
x_i^{\alpha}-0.001\alpha-0.001\beta+K,
$$
for some constant $K$. \\ \pause


\medskip
In R, we can find the mode to be $\boldm=(1.354,0.030)$
using the {\tt optim} command.
}




\frame{

 2) \textbf{Derive the matrix of second
derivatives of} $h(\btheta)$.\\
\begin{equation*}
M=\left(\begin{array}{cc}\frac{\partial^2}{\partial
\alpha^2}h(\btheta) & \frac{\partial^2}{\partial \alpha\partial
\beta}h(\btheta)\\ \frac{\partial^2}{\partial
\alpha\partial\beta}h(\btheta) & \frac{\partial^2}{\partial
\beta^2}h(\btheta)\end{array}\right),
\end{equation*}

evaluated at $\btheta=\boldm$. \pause
\begin{eqnarray*}
\frac{\partial^2}{\partial \alpha^2}h(\btheta) &=&-\frac{d}{\alpha^2}-\sum (\beta x_i)^\alpha (\log(\beta x_i))^2\\
%&-\frac{d}{\alpha^2}- \beta^{\alpha}\left\{(\log
%\beta)^2\sum_{i=1}^nx_i^{\alpha}+2\log
%\beta\sum_{i=1}^nx_i^{\alpha}\log x_i \right.
%&&\left.+\sum_{i=1}^nx_i^{\alpha}(\log
%x_i)^2\right\},\\ 
\pause 
\frac{\partial^2}{\partial
\beta^2}h(\btheta)&=&
\frac{1}{\beta^2}\left\{\beta^\alpha\alpha(1-\alpha)\sum_{i=1}^nx_i^{\alpha}-d\alpha\right\},\\
\pause \frac{\partial^2}{\partial \alpha\partial\beta}h(\btheta)&=&
\frac{1}{\beta}\left[d-\beta^{\alpha}\left\{\alpha\log \beta
\sum_{i=1}^n
x_i^{\alpha}+\sum_{i=1}^nx_i^{\alpha}+\alpha\sum_{i=1}^nx_i^{\alpha}\log
x_i\right\} \right],
\end{eqnarray*}

\pause

\begin{equation*}
M=\left(\begin{array}{cc}-31.618 & 175.442 \\
175.442 & -18806.085\end{array}\right).
\end{equation*}

}

\frame{

3) \textbf{Obtain the normal approximation to use as}
$g(\btheta)$.\\
$g(\btheta)$: bivariate normal, mean $\boldm$, variance matrix
$V=-M^{-1}$:
\begin{equation*}
\btheta \sim N\left\{\left(\begin{array}{c} 1.354\\ 0.030\end{array}\right), \left(\begin{array}{cc}0.0334 & 0.0003 \\
0.0003 & 0.00006\end{array}\right) \right\}
\end{equation*}

\pause

4) \textbf{Sample $\btheta_1,\ldots,\btheta_n$ from $g(\btheta)$ and
compute the importance weights $w(\btheta_1),\ldots,w(\btheta_n)$}.
The weights are given by
\begin{equation*}
w(\btheta_i)=\frac{\tilde{w}(\btheta_i)}{\sum_{i=1}^n
\tilde{w}(\btheta_i)},\mbox{\hspace{1cm} with \hspace{1cm}}
\tilde{w}(\btheta_i)=
\frac{f(\btheta_i)f(\bx|\btheta_i)}{g(\btheta_i)}
\end{equation*}

\pause

NB the Gaussian approximation may give us negative samples. Since $\alpha>0$
and $\beta>0$, we shoould simply discard negative $\btheta$ values, i.e., use  a truncated
normal density for $g(\theta)$. 

\medskip
Note that when  we compute $w(\btheta_i)$, it is not
necessary to rescale $g(\theta)$ so that it integrates to 1, as any
normalising constant in $g(\theta)$ will cancel.

}



\frame{

5) \textbf{Estimate the posterior mean of $\btheta$}\\
We compute the estimate
\begin{equation*}
\hat{E}(\btheta|\bx)=\sum_{i=1}^n \btheta_i w(\btheta_i).
\end{equation*}
 \pause In R, with $n=100000$, this gives
$\hat{E}(\btheta|\bx)=(1.346,0.031)^T$.  \pause \\6) \textbf{Check
for
convergence}\\
We repeat steps 4 and 5 with more dispersion in $g(\btheta)$:
\begin{center}
\begin{tabular}{c|c}
$g(\btheta)$ & $\hat{E}(\btheta|\bx)$ \\ \hline $N(\boldm ,V)$
&$(1.346,0.031)^T$\\
$N(\boldm,4V)$
&$(1.384,0.031)^T$\\
$N(\boldm,16V)$
&$(1.380,0.031)^T$\\
\end{tabular}
\end{center}
 \pause Finally, double the sample size (no effect observed).

\pause

For percentiles, we can do resampling in R.

\medskip
See computer class 5 for more details and code to implement this approach.
}

\end{document}