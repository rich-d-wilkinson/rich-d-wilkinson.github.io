<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>3.4 Similarity measures | Multivariate Statistics</title>
  <meta name="description" content="The lecture notes for MATH3030/4068: Multivariate Analysis / Applied Multivariate Statistics" />
  <meta name="generator" content="bookdown 0.21 and GitBook 2.6.7" />

  <meta property="og:title" content="3.4 Similarity measures | Multivariate Statistics" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="The lecture notes for MATH3030/4068: Multivariate Analysis / Applied Multivariate Statistics" />
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="3.4 Similarity measures | Multivariate Statistics" />
  
  <meta name="twitter:description" content="The lecture notes for MATH3030/4068: Multivariate Analysis / Applied Multivariate Statistics" />
  

<meta name="author" content="Prof.Â Richard Wilkinson" />



  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="3-3-non-classical-mds.html"/>
<link rel="next" href="3-5-non-metric-mds.html"/>
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />









<script src="libs/kePrint-0.0.1/kePrint.js"></script>
<link href="libs/lightable-0.0.1/lightable.css" rel="stylesheet" />


<style type="text/css">
a.sourceLine { display: inline-block; line-height: 1.25; }
a.sourceLine { pointer-events: none; color: inherit; text-decoration: inherit; }
a.sourceLine:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode { white-space: pre; position: relative; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
code.sourceCode { white-space: pre-wrap; }
a.sourceLine { text-indent: -1em; padding-left: 1em; }
}
pre.numberSource a.sourceLine
  { position: relative; left: -4em; }
pre.numberSource a.sourceLine::before
  { content: attr(data-line-number);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; pointer-events: all; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {  }
@media screen {
a.sourceLine::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Applied multivariate statistics</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Introduction</a></li>
<li class="chapter" data-level="1" data-path="1-linalg-prelim.html"><a href="1-linalg-prelim.html"><i class="fa fa-check"></i><b>1</b> Review of linear algebra</a><ul>
<li class="chapter" data-level="1.1" data-path="1-1-linalg-basics.html"><a href="1-1-linalg-basics.html"><i class="fa fa-check"></i><b>1.1</b> Basics</a><ul>
<li class="chapter" data-level="1.1.1" data-path="1-1-linalg-basics.html"><a href="1-1-linalg-basics.html#notation"><i class="fa fa-check"></i><b>1.1.1</b> Notation</a></li>
<li class="chapter" data-level="1.1.2" data-path="1-1-linalg-basics.html"><a href="1-1-linalg-basics.html#elementary-matrix-operations"><i class="fa fa-check"></i><b>1.1.2</b> Elementary matrix operations</a></li>
<li class="chapter" data-level="1.1.3" data-path="1-1-linalg-basics.html"><a href="1-1-linalg-basics.html#special-matrices"><i class="fa fa-check"></i><b>1.1.3</b> Special matrices</a></li>
<li class="chapter" data-level="1.1.4" data-path="1-1-linalg-basics.html"><a href="1-1-linalg-basics.html#vectordiff"><i class="fa fa-check"></i><b>1.1.4</b> Vector Differentiation</a></li>
</ul></li>
<li class="chapter" data-level="1.2" data-path="1-2-linalg-vecspaces.html"><a href="1-2-linalg-vecspaces.html"><i class="fa fa-check"></i><b>1.2</b> Vector spaces</a><ul>
<li class="chapter" data-level="1.2.1" data-path="1-2-linalg-vecspaces.html"><a href="1-2-linalg-vecspaces.html#linear-independence"><i class="fa fa-check"></i><b>1.2.1</b> Linear independence</a></li>
<li class="chapter" data-level="1.2.2" data-path="1-2-linalg-vecspaces.html"><a href="1-2-linalg-vecspaces.html#colsspace"><i class="fa fa-check"></i><b>1.2.2</b> Row and column spaces</a></li>
<li class="chapter" data-level="1.2.3" data-path="1-2-linalg-vecspaces.html"><a href="1-2-linalg-vecspaces.html#linear-transformations"><i class="fa fa-check"></i><b>1.2.3</b> Linear transformations</a></li>
</ul></li>
<li class="chapter" data-level="1.3" data-path="1-3-linalg-innerprod.html"><a href="1-3-linalg-innerprod.html"><i class="fa fa-check"></i><b>1.3</b> Inner product spaces</a><ul>
<li class="chapter" data-level="1.3.1" data-path="1-3-linalg-innerprod.html"><a href="1-3-linalg-innerprod.html#normed"><i class="fa fa-check"></i><b>1.3.1</b> Distances, and angles</a></li>
<li class="chapter" data-level="1.3.2" data-path="1-3-linalg-innerprod.html"><a href="1-3-linalg-innerprod.html#orthogonal-matrices"><i class="fa fa-check"></i><b>1.3.2</b> Orthogonal matrices</a></li>
<li class="chapter" data-level="1.3.3" data-path="1-3-linalg-innerprod.html"><a href="1-3-linalg-innerprod.html#projection-matrix"><i class="fa fa-check"></i><b>1.3.3</b> Projections</a></li>
</ul></li>
<li class="chapter" data-level="1.4" data-path="1-4-centering-matrix.html"><a href="1-4-centering-matrix.html"><i class="fa fa-check"></i><b>1.4</b> The Centering Matrix</a></li>
<li class="chapter" data-level="1.5" data-path="1-5-tasks-ch2.html"><a href="1-5-tasks-ch2.html"><i class="fa fa-check"></i><b>1.5</b> Computer tasks</a></li>
<li class="chapter" data-level="1.6" data-path="1-6-exercises-ch2.html"><a href="1-6-exercises-ch2.html"><i class="fa fa-check"></i><b>1.6</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="2-linalg-decomp.html"><a href="2-linalg-decomp.html"><i class="fa fa-check"></i><b>2</b> Matrix decompositions</a><ul>
<li class="chapter" data-level="2.1" data-path="2-1-matrix-matrix.html"><a href="2-1-matrix-matrix.html"><i class="fa fa-check"></i><b>2.1</b> Matrix-matrix products</a></li>
<li class="chapter" data-level="2.2" data-path="2-2-spectraleigen-decomposition.html"><a href="2-2-spectraleigen-decomposition.html"><i class="fa fa-check"></i><b>2.2</b> Spectral/eigen decomposition</a><ul>
<li class="chapter" data-level="2.2.1" data-path="2-2-spectraleigen-decomposition.html"><a href="2-2-spectraleigen-decomposition.html#eigenvalues-and-eigenvectors"><i class="fa fa-check"></i><b>2.2.1</b> Eigenvalues and eigenvectors</a></li>
<li class="chapter" data-level="2.2.2" data-path="2-2-spectraleigen-decomposition.html"><a href="2-2-spectraleigen-decomposition.html#spectral-decomposition"><i class="fa fa-check"></i><b>2.2.2</b> Spectral decomposition</a></li>
<li class="chapter" data-level="2.2.3" data-path="2-2-spectraleigen-decomposition.html"><a href="2-2-spectraleigen-decomposition.html#matrixroots"><i class="fa fa-check"></i><b>2.2.3</b> Matrix square roots</a></li>
</ul></li>
<li class="chapter" data-level="2.3" data-path="2-3-linalg-SVD.html"><a href="2-3-linalg-SVD.html"><i class="fa fa-check"></i><b>2.3</b> Singular Value Decomposition (SVD)</a><ul>
<li class="chapter" data-level="2.3.1" data-path="2-3-linalg-SVD.html"><a href="2-3-linalg-SVD.html#examples"><i class="fa fa-check"></i><b>2.3.1</b> Examples</a></li>
</ul></li>
<li class="chapter" data-level="2.4" data-path="2-4-svdopt.html"><a href="2-4-svdopt.html"><i class="fa fa-check"></i><b>2.4</b> SVD optimization results</a></li>
<li class="chapter" data-level="2.5" data-path="2-5-low-rank-approximation.html"><a href="2-5-low-rank-approximation.html"><i class="fa fa-check"></i><b>2.5</b> Low-rank approximation</a><ul>
<li class="chapter" data-level="2.5.1" data-path="2-5-low-rank-approximation.html"><a href="2-5-low-rank-approximation.html#matrix-norms"><i class="fa fa-check"></i><b>2.5.1</b> Matrix norms</a></li>
<li class="chapter" data-level="2.5.2" data-path="2-5-low-rank-approximation.html"><a href="2-5-low-rank-approximation.html#eckart-young-mirsky-theorem"><i class="fa fa-check"></i><b>2.5.2</b> Eckart-Young-Mirsky Theorem</a></li>
<li class="chapter" data-level="2.5.3" data-path="2-5-low-rank-approximation.html"><a href="2-5-low-rank-approximation.html#example-image-compression"><i class="fa fa-check"></i><b>2.5.3</b> Example: image compression</a></li>
</ul></li>
<li class="chapter" data-level="2.6" data-path="2-6-tasks-ch3.html"><a href="2-6-tasks-ch3.html"><i class="fa fa-check"></i><b>2.6</b> Computer tasks</a></li>
<li class="chapter" data-level="2.7" data-path="2-7-exercises-ch3.html"><a href="2-7-exercises-ch3.html"><i class="fa fa-check"></i><b>2.7</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="3-mds.html"><a href="3-mds.html"><i class="fa fa-check"></i><b>3</b> Multidimensional Scaling (MDS)</a><ul>
<li class="chapter" data-level="3.1" data-path="3-1-classical-mds.html"><a href="3-1-classical-mds.html"><i class="fa fa-check"></i><b>3.1</b> Classical MDS</a><ul>
<li class="chapter" data-level="3.1.1" data-path="3-1-classical-mds.html"><a href="3-1-classical-mds.html#non-euclidean-distance-matrices"><i class="fa fa-check"></i><b>3.1.1</b> Non-Euclidean distance matrices</a></li>
<li class="chapter" data-level="3.1.2" data-path="3-1-classical-mds.html"><a href="3-1-classical-mds.html#principal-coordinate-analysis"><i class="fa fa-check"></i><b>3.1.2</b> Principal Coordinate Analysis</a></li>
</ul></li>
<li class="chapter" data-level="3.2" data-path="3-2-properties.html"><a href="3-2-properties.html"><i class="fa fa-check"></i><b>3.2</b> Properties</a></li>
<li class="chapter" data-level="3.3" data-path="3-3-non-classical-mds.html"><a href="3-3-non-classical-mds.html"><i class="fa fa-check"></i><b>3.3</b> Non-classical MDS</a></li>
<li class="chapter" data-level="3.4" data-path="3-4-similarity-measures.html"><a href="3-4-similarity-measures.html"><i class="fa fa-check"></i><b>3.4</b> Similarity measures</a><ul>
<li class="chapter" data-level="3.4.1" data-path="3-4-similarity-measures.html"><a href="3-4-similarity-measures.html#binary-attributes"><i class="fa fa-check"></i><b>3.4.1</b> Binary attributes</a></li>
<li class="chapter" data-level="3.4.2" data-path="3-4-similarity-measures.html"><a href="3-4-similarity-measures.html#example"><i class="fa fa-check"></i><b>3.4.2</b> Example</a></li>
</ul></li>
<li class="chapter" data-level="3.5" data-path="3-5-non-metric-mds.html"><a href="3-5-non-metric-mds.html"><i class="fa fa-check"></i><b>3.5</b> Non-metric MDS</a></li>
<li class="chapter" data-level="3.6" data-path="3-6-exercises.html"><a href="3-6-exercises.html"><i class="fa fa-check"></i><b>3.6</b> Exercises</a></li>
<li class="chapter" data-level="3.7" data-path="3-7-computer-tasks.html"><a href="3-7-computer-tasks.html"><i class="fa fa-check"></i><b>3.7</b> Computer Tasks</a></li>
</ul></li>
<li class="divider"></li>
<li> <a href="https://rich-d-wilkinson.github.io/teaching.html" target="blank">University of Nottingham</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Multivariate Statistics</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="similarity-measures" class="section level2">
<h2><span class="header-section-number">3.4</span> Similarity measures</h2>
<p>So far we have presented classical MDS as starting with a distance (or dissimilarity) matrix <span class="math inline">\(\mathbf D=(d_{ij})_{i,j=1}^n\)</span>. In this setting, the larger <span class="math inline">\(d_{ij}\)</span> is, the more distant, or dissimilar, object <span class="math inline">\(i\)</span> is from object <span class="math inline">\(j\)</span>.
We then convert <span class="math inline">\(\mathbf D\)</span> to a centred inner product matrix <span class="math inline">\(\mathbf B\)</span>. We said that we can think of <span class="math inline">\(\mathbf B\)</span> as being a similarity matrix.</p>
<p>We can also use a more general concept of <strong>similarity</strong> in MDS.</p>

<div class="definition">
<p><span id="def:unnamed-chunk-21" class="definition"><strong>Definition 3.4  </strong></span>A <strong>similarity matrix</strong> is defined to be an <span class="math inline">\(n \times n\)</span> matrix <span class="math inline">\(\mathbf=(f_{ij})_{i,j=1}^n\)</span> with the following properties:</p>
<ol style="list-style-type: decimal">
<li>Symmetry, i.e. <span class="math inline">\(f_{ij} =f_{ji}\)</span>, <span class="math inline">\(i,j=1, \ldots , n\)</span>.</li>
<li>For all <span class="math inline">\(i,j=1, \ldots , n\)</span>, <span class="math inline">\(f_{ij} \leq f_{ii}\)</span>.
</div></li>
</ol>
<p>Note that when working with similarities <span class="math inline">\(f_{ij}\)</span>, the larger <span class="math inline">\(f_{ij}\)</span> is, the more similar objects <span class="math inline">\(i\)</span> and <span class="math inline">\(j\)</span> are.</p>
<ul>
<li><p>Condition 1. implies that object <span class="math inline">\(i\)</span> is as similar to object <span class="math inline">\(j\)</span> as object <span class="math inline">\(j\)</span> is to object <span class="math inline">\(i\)</span> (symmetry).</p></li>
<li><p>Condition 2. implies that an object is at least as similar to itself as it is to any other object.</p></li>
</ul>
<p>In this section, we consider the analysis of measures of <strong>similarity</strong> as opposed to measures of dissimilarity. We begin by showing that we can convert a positive semi-definite similarity matrix <span class="math inline">\(\mathbf F\)</span> into a distance matrix <span class="math inline">\(\mathbf D\)</span> and then into a centred inner product matrix <span class="math inline">\(\mathbf B\)</span>, allowing us to use the classical MDS approach from the previous section.</p>

<div class="proposition">
<span id="prp:mds2" class="proposition"><strong>Proposition 3.2  </strong></span>Suppose that <span class="math inline">\(\mathbf F\)</span> is a positive semi-definite similarity matrix. Then the matrix <span class="math inline">\(\mathbf D\)</span> with elements
<span class="math display" id="eq:defD">\[\begin{equation}
d_{ij}=\left ( f_{ii}+f_{jj} -2f_{ij} \right )^{1/2}, \qquad i,j=1, \ldots , n.
\tag{3.5}
\end{equation}\]</span>
is a Euclidean distance matrix. Its centred inner product matrix, <span class="math inline">\(\mathbf B= -\frac{1}{2}\mathbf H(\mathbf D\odot\mathbf D)\mathbf H\)</span>, can be computed via
<span class="math display" id="eq:BHFH">\[\begin{equation}
\mathbf B=\mathbf H\mathbf F\mathbf H.
\tag{3.6}
\end{equation}\]</span>
</div>


<div class="proof">
<p> <span class="proof"><em>Proof. </em></span> 
Firstly, note that as <span class="math inline">\(\mathbf F\)</span> is a similarity matrix, <span class="math inline">\(f_{ii}+f_{jj}-2f_{ij}\geq 0\)</span> by condition 2., and so the <span class="math inline">\(d_{ij}\)</span> are all well-defined (i.e.Â real, not imaginary).</p>
<p>We will now show that Equation <a href="3-4-similarity-measures.html#eq:BHFH">(3.6)</a> holds. Let <span class="math inline">\(\mathbf A= -\frac{1}{2}\mathbf D\odot \mathbf D\)</span> as in Equation <a href="3-1-classical-mds.html#eq:defA">(3.3)</a>. Then
<span class="math display">\[
a_{ij}=-\frac{1}{2}d_{ij}^2 =f_{ij}-\frac{1}{2}(f_{ii}+f_{jj}).
\]</span></p>
<p>Define
<span class="math display">\[
t=n^{-1}\sum_{i=1}^n f_{ii}.
\]</span>
Then, summing over <span class="math inline">\(j=1, \ldots , n\)</span> for fixed <span class="math inline">\(i\)</span>,
<span class="math display">\[
\bar{a}_{i+}=n^{-1}\sum_{j=1}^n a_{ij} = \bar{f}_{i+}-\frac{1}{2}(f_{ii}+t);
\]</span>
similarly,
<span class="math display">\[
\bar{a}_{+j}=n^{-1}\sum_{i=1}^n a_{ij}=\bar{f}_{+j}-\frac{1}{2}(f_{jj}+t),
\]</span>
and also
<span class="math display">\[
\bar{a}_{++}=n^{-2}\sum_{i,j=1}^n a_{ij}=\bar{f}_{++}-\frac{1}{2}(t+t).
\]</span>
Recall property (vii) from Section <a href="1-4-centering-matrix.html#centering-matrix">1.4</a>:
<span class="math display">\[
b_{ij}=a_{ij}-\bar{a}_{i+}-\bar{a}_{+j}+\bar{a}_{++}
\]</span>
noting that <span class="math inline">\(\mathbf A\)</span> is symmetric. Thus
<span class="math display">\[\begin{align*} 
b_{ij}&amp;=f_{ij}-\frac{1}{2}(f_{ii}+f_{jj})-\bar{f}_{i+}+\frac{1}{2}(f_{ii}+t)\\
&amp; \qquad -\bar{f}_{+j}+\frac{1}{2}(f_{jj}+t) +\bar{f}_{++}-t\\
&amp; =\qquad f_{ij}-\bar{f}_{i+}-\bar{f}_{+j}+\bar{f}_{++}.
\end{align*}\]</span>
Consequently, <span class="math inline">\(\mathbf B=\mathbf H\mathbf F\mathbf H\)</span>, again using property (vii) from Section <a href="1-4-centering-matrix.html#centering-matrix">1.4</a>.</p>
So weâve shown that <span class="math inline">\(\mathbf B= \mathbf H\mathbf F\mathbf H\)</span>. It only remains to show <span class="math inline">\(\mathbf D\)</span> is Euclidean. Since <span class="math inline">\(\mathbf F\)</span> is positive semi-definite by assumption, and <span class="math inline">\(\mathbf H^\top =\mathbf H\)</span>, it follows that <span class="math inline">\(\mathbf B=\mathbf H\mathbf F\mathbf H\)</span> must also be positive semi-definite. So by Theorem <a href="3-1-classical-mds.html#thm:five1">3.1</a> <span class="math inline">\(\mathbf D\)</span> is a Euclidean distance matrix.
</div>

<div id="binary-attributes" class="section level3">
<h3><span class="header-section-number">3.4.1</span> Binary attributes</h3>
<p>One important class of problems is when the similarity between any two objects is measured by the number of common attributes. The underlying data on each object is a binary vector of 0s and 1s indicating absence or presence of an attribute. These binary vectors are then converted to similarities by comparing which attributes two objects have in common.</p>
<p>We illustrate this through two examples.</p>

<div class="example">
<p><span id="exm:unnamed-chunk-23" class="example"><strong>Example 3.1  </strong></span>Suppose there are 4 attributes we wish to consider.</p>
<ol style="list-style-type: decimal">
<li>Attribute 1: Carnivore? If yes, put <span class="math inline">\(x_1=1\)</span>; if no, put <span class="math inline">\(x_1=0\)</span>.</li>
<li>Attribute 2: Mammal? If yes, put <span class="math inline">\(x_2=1\)</span>; if no, put <span class="math inline">\(x_2=0\)</span>.</li>
<li>Attribute 3: Natural habitat in Africa? If yes, put <span class="math inline">\(x_3=1\)</span>; if no, put <span class="math inline">\(x_3=0\)</span>.</li>
<li>Attribute 4: Can climb trees? If yes, put <span class="math inline">\(x_4=1\)</span>; if no, put <span class="math inline">\(x_4=0\)</span>.</li>
</ol>
<p>Consider a lion. Each of the attributes is present so <span class="math inline">\(x_1=x_2=x_3=x_4=1\)</span>. Its attribute vector is <span class="math inline">\(\begin{pmatrix} 1&amp;1&amp;1&amp;1\end{pmatrix}^\top\)</span>.</p>
<p>What about a tiger? In this case, 3 of the attributes are present (1, 2 and 4) but 3 is absent.
So for a tiger, <span class="math inline">\(x_1=x_2=x_4=1\)</span> and <span class="math inline">\(x_3=0\)</span> or in vector form, its attributes are <span class="math inline">\(\begin{pmatrix} 1&amp;1&amp;0&amp;1\end{pmatrix}^\top\)</span>.</p>
<p>How might we measure the similarity of lions and tigers based on the presence or absence of these four attributes?</p>
<p>First form a <span class="math inline">\(2 \times 2\)</span> table as follows.
<span class="math display">\[
\begin{array}{cccc}
Tiger\Lion &amp;1 &amp;0\\
1&amp; a &amp; b\\
0&amp; c &amp; d
\end{array}
\]</span>
Here <span class="math inline">\(a\)</span> counts the number of attributes common to both lion and tiger; <span class="math inline">\(b\)</span> counts the number of attributes the lion has but the tiger does not have; <span class="math inline">\(c\)</span> counts the number of attributes the tigher has that the lion does not have; and <span class="math inline">\(d\)</span> counts the number of attributes which neither the lion nor the tiger has.</p>
<p>In the above, <span class="math inline">\(a=3\)</span>, <span class="math inline">\(b=1\)</span> and <span class="math inline">\(c=d=0\)</span>.</p>
<p>How might we make use of the information in the <span class="math inline">\(2 \times 2\)</span> table to construct a measure of similarity?</p>
<p>The simplest measure of similarity is the proportion of the attributes which are shared:
<span class="math display">\[
\frac{a}{a+b+c+d}
\]</span>
which gives <span class="math inline">\(0.75\)</span> in this example.</p>
<p>This is known as the **Jaccard similarity coefficient*. NOT QUITE. JACCARD DOESNâT HAVE d in the denomâ.</p>
<p>A second similarity measure, which gives the same value in this example but not in general, is known as the <strong>simple matching coefficient</strong> and is given by
<span class="math display" id="eq:smc">\[\begin{equation}
\frac{a+d}{a+b+c+d}.
\tag{3.7}
\end{equation}\]</span>
There are many other possibilities, e.g.Â we could consider weighted versions of the above if we wish to weight different attributes differently.</p>
<p>FROM WIKIPEDIA
Difference with the simple matching coefficient (SMC)
When used for binary attributes, the Jaccard index is very similar to the simple matching coefficient. The main difference is that the SMC has the term {M_{00}}M_{00} in its numerator and denominator, whereas the Jaccard index does not. Thus, the SMC counts both mutual presences (when an attribute is present in both sets) and mutual absence (when an attribute is absent in both sets) as matches and compares it to the total number of attributes in the universe, whereas the Jaccard index only counts mutual presence as matches and compares it to the number of attributes that have been chosen by at least one of the two sets.</p>
<p>In market basket analysis, for example, the basket of two consumers who we wish to compare might only contain a small fraction of all the available products in the store, so the SMC will usually return very high values of similarities even when the baskets bear very little resemblance, thus making the Jaccard index a more appropriate measure of similarity in that context. For example, consider a supermarket with 1000 products and two customers. The basket of the first customer contains salt and pepper and the basket of the second contains salt and sugar. In this scenario, the similarity between the two baskets as measured by the Jaccard index would be 1/3, but the similarity becomes 0.998 using the SMC.</p>
<p>In other contexts, where 0 and 1 carry equivalent information (symmetry), the SMC is a better measure of similarity. For example, vectors of demographic variables stored in dummy variables, such as gender, would be better compared with the SMC than with the Jaccard index since the impact of gender on similarity should be equal, independently of whether male is defined as a 0 and female as a 1 or the other way around. However, when we have symmetric dummy variables, one could replicate the behaviour of the SMC by splitting the dummies into two binary attributes (in this case, male and female), thus transforming them into asymmetric attributes, allowing the use of the Jaccard index without introducing any bias. The SMC remains, however, more computationally efficient in the case of symmetric dummy variables since it does not require adding extra dimensions.</p>
</div>


<div class="example">
<span id="exm:mds1" class="example"><strong>Example 3.2  </strong></span>Let us now consider a similar but more complex example with 6 unspecified attributes (not the same attributes as in Example 1) and 5 types of living creature, with the following data matrix, consisting of zeros and ones.
<span class="math display">\[
\begin{array}{lcccccc}
&amp;1&amp;2&amp;3&amp;4&amp;5&amp;6\\
Lion&amp;1&amp;1&amp;0&amp;0&amp;1&amp;1\\
Giraffe&amp;1&amp;1&amp;1&amp;0&amp;0&amp;1\\
Cow&amp;1&amp;0&amp;0&amp;1&amp;0&amp;1\\
Sheep&amp;1&amp;0&amp;0&amp;1&amp;0&amp;1\\
Human&amp;0&amp;0&amp;0&amp;0&amp;1&amp;0
\end{array}
\]</span>
Suppose we decide to use the simple matching coefficient <a href="3-4-similarity-measures.html#eq:smc">(3.7)</a> to measure similarity. Then the following similarity matrix is obtained.
<span class="math display">\[
\mathbf F=\begin{array}{lccccc}
&amp;\text{Lion}&amp;\text{Giraffe}&amp;\text{Cow}&amp;\text{Sheep}&amp;\text{Human}\\
Lion&amp;1&amp;2/3&amp;1/2&amp;1/2&amp;1/2\\
Giraffe&amp;2/3&amp;1&amp;1/2&amp;1/2&amp;1/6\\
Cow&amp;1/2&amp;1/2&amp;1&amp;1&amp;1/3\\
Sheep&amp;1/2&amp;1/2&amp;1&amp;1&amp;1/3\\
Human&amp;1/2&amp;1/6&amp;1/3&amp;1/3&amp;1
\end{array}
\]</span>
It is easily checked from the definition that <span class="math inline">\(\mathbf=(f_{ij})_{i,j=1}^5\)</span> is a similarity matrix.
</div>

<div class="sourceCode" id="cb64"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb64-1" data-line-number="1">animal &lt;-<span class="st"> </span><span class="kw">read.table</span>(<span class="st">&#39;data/animal_similarity&#39;</span>, <span class="dt">sep=</span><span class="st">&#39;&amp;&#39;</span>, <span class="dt">header=</span><span class="ot">TRUE</span>, <span class="dt">row.names=</span><span class="dv">1</span>)</a>
<a class="sourceLine" id="cb64-2" data-line-number="2"></a>
<a class="sourceLine" id="cb64-3" data-line-number="3">SMC &lt;-<span class="st"> </span><span class="cf">function</span>(x,y){</a>
<a class="sourceLine" id="cb64-4" data-line-number="4">  <span class="kw">sum</span>(x<span class="op">==</span>y)<span class="op">/</span><span class="kw">length</span>(x)</a>
<a class="sourceLine" id="cb64-5" data-line-number="5">}</a>
<a class="sourceLine" id="cb64-6" data-line-number="6"><span class="kw">SMC</span>(animal[<span class="dv">1</span>,], animal[<span class="dv">2</span>,])</a></code></pre></div>
<pre><code>## [1] 0.6666667</code></pre>
<div class="sourceCode" id="cb66"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb66-1" data-line-number="1">n=<span class="kw">dim</span>(animal)[<span class="dv">1</span>]</a>
<a class="sourceLine" id="cb66-2" data-line-number="2">F_SMC =<span class="st"> </span><span class="kw">outer</span>(<span class="dv">1</span><span class="op">:</span>n,<span class="dv">1</span><span class="op">:</span>n, <span class="kw">Vectorize</span>(<span class="cf">function</span>(i,j){<span class="kw">SMC</span>(animal[i,], animal[j,])}))</a>
<a class="sourceLine" id="cb66-3" data-line-number="3"></a>
<a class="sourceLine" id="cb66-4" data-line-number="4">F_SMC</a></code></pre></div>
<pre><code>##           [,1]      [,2]      [,3]      [,4]      [,5]
## [1,] 1.0000000 0.6666667 0.5000000 0.5000000 0.5000000
## [2,] 0.6666667 1.0000000 0.5000000 0.5000000 0.1666667
## [3,] 0.5000000 0.5000000 1.0000000 1.0000000 0.3333333
## [4,] 0.5000000 0.5000000 1.0000000 1.0000000 0.3333333
## [5,] 0.5000000 0.1666667 0.3333333 0.3333333 1.0000000</code></pre>
<div class="sourceCode" id="cb68"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb68-1" data-line-number="1"><span class="co"># same as</span></a>
<a class="sourceLine" id="cb68-2" data-line-number="2">(<span class="dv">6</span><span class="op">-</span><span class="kw">dist</span>(animal, <span class="dt">method=</span><span class="st">&quot;manhattan&quot;</span>))<span class="op">/</span><span class="dv">6</span></a></code></pre></div>
<pre><code>##              Lion   Giraffe       Cow     Sheep
## Giraffe 0.6666667                              
## Cow     0.5000000 0.5000000                    
## Sheep   0.5000000 0.5000000 1.0000000          
## Human   0.5000000 0.1666667 0.3333333 0.3333333</code></pre>
<div class="sourceCode" id="cb70"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb70-1" data-line-number="1">jaccard =<span class="st"> </span><span class="cf">function</span>(x, y) {</a>
<a class="sourceLine" id="cb70-2" data-line-number="2">  bt =<span class="st"> </span><span class="kw">table</span>(y, x)</a>
<a class="sourceLine" id="cb70-3" data-line-number="3">  <span class="kw">return</span>((bt[<span class="dv">2</span>, <span class="dv">2</span>])<span class="op">/</span>(bt[<span class="dv">1</span>, <span class="dv">2</span>] <span class="op">+</span><span class="st"> </span>bt[<span class="dv">2</span>, <span class="dv">1</span>] <span class="op">+</span><span class="st"> </span>bt[<span class="dv">2</span>, <span class="dv">2</span>]))</a>
<a class="sourceLine" id="cb70-4" data-line-number="4">}</a>
<a class="sourceLine" id="cb70-5" data-line-number="5"></a>
<a class="sourceLine" id="cb70-6" data-line-number="6"><span class="kw">jaccard</span>(animal[<span class="dv">1</span>,], animal[<span class="dv">2</span>,])</a></code></pre></div>
<pre><code>## [1] 0.6</code></pre>
<div class="sourceCode" id="cb72"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb72-1" data-line-number="1">F_jaccard =<span class="st"> </span><span class="kw">outer</span>(<span class="dv">1</span><span class="op">:</span>n,<span class="dv">1</span><span class="op">:</span>n, <span class="kw">Vectorize</span>(<span class="cf">function</span>(i,j){<span class="kw">jaccard</span>(animal[i,], animal[j,])}))</a>
<a class="sourceLine" id="cb72-2" data-line-number="2"></a>
<a class="sourceLine" id="cb72-3" data-line-number="3">F_jaccard</a></code></pre></div>
<pre><code>##      [,1] [,2] [,3] [,4] [,5]
## [1,] 1.00  0.6  0.4  0.4 0.25
## [2,] 0.60  1.0  0.4  0.4 0.00
## [3,] 0.40  0.4  1.0  1.0 0.00
## [4,] 0.40  0.4  1.0  1.0 0.00
## [5,] 0.25  0.0  0.0  0.0 1.00</code></pre>
<div class="sourceCode" id="cb74"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb74-1" data-line-number="1"><span class="dv">1</span><span class="op">-</span><span class="kw">dist</span>(animal, <span class="dt">method=</span><span class="st">&quot;binary&quot;</span>)</a></code></pre></div>
<pre><code>##         Lion Giraffe  Cow Sheep
## Giraffe 0.60                   
## Cow     0.40    0.40           
## Sheep   0.40    0.40 1.00      
## Human   0.25    0.00 0.00  0.00</code></pre>
<div class="sourceCode" id="cb76"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb76-1" data-line-number="1">D &lt;-<span class="st"> </span><span class="kw">matrix</span>(<span class="dt">nr=</span><span class="dv">5</span>,<span class="dt">nc=</span><span class="dv">5</span>)</a>
<a class="sourceLine" id="cb76-2" data-line-number="2"><span class="cf">for</span>(ii <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span><span class="dv">5</span>){</a>
<a class="sourceLine" id="cb76-3" data-line-number="3">  <span class="cf">for</span>(jj <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span><span class="dv">5</span>){</a>
<a class="sourceLine" id="cb76-4" data-line-number="4">    D[ii,jj] &lt;-<span class="st"> </span><span class="kw">sqrt</span>(F_SMC[ii,ii]<span class="op">+</span>F_SMC[jj,jj]<span class="op">-</span><span class="dv">2</span><span class="op">*</span>F_SMC[ii,jj])</a>
<a class="sourceLine" id="cb76-5" data-line-number="5">  }</a>
<a class="sourceLine" id="cb76-6" data-line-number="6">}</a></code></pre></div>
<div class="sourceCode" id="cb77"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb77-1" data-line-number="1"><span class="co">#install.packages(&quot;RFLPtools&quot;, repos=&quot;http://R-Forge.R-project.org&quot;)</span></a>
<a class="sourceLine" id="cb77-2" data-line-number="2"><span class="kw">library</span>(RFLPtools)</a></code></pre></div>
<pre><code>## Loading required package: RColorBrewer</code></pre>
<div class="sourceCode" id="cb79"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb79-1" data-line-number="1">sim2dist</a></code></pre></div>
<pre><code>## function (x, maxSim = 1) 
## {
##     stopifnot(is.matrix(x))
##     stopifnot(isSymmetric(x))
##     stopifnot(maxSim &gt;= max(x))
##     d &lt;- maxSim - x
##     return(as.dist(d))
## }
## &lt;bytecode: 0x7feb76d8f880&gt;
## &lt;environment: namespace:RFLPtools&gt;</code></pre>
</div>
<div id="example" class="section level3">
<h3><span class="header-section-number">3.4.2</span> Example</h3>
<p>Do football data. Use points totals as similarity?????????</p>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="3-3-non-classical-mds.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="3-5-non-metric-mds.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["MultivariateStatistics.pdf"],
"toc": {
"collapse": "section"
},
"pandoc_args": "--top-level-division=[chapter|part]"
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
