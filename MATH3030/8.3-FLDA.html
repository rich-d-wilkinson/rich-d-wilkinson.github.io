<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>8.3 Fisher’s linear discriminant rule | Multivariate Statistics</title>
  <meta name="description" content="The lecture notes for MATH3030/4068: Multivariate Analysis / Applied Multivariate Statistics" />
  <meta name="generator" content="bookdown 0.36 and GitBook 2.6.7" />

  <meta property="og:title" content="8.3 Fisher’s linear discriminant rule | Multivariate Statistics" />
  <meta property="og:type" content="book" />
  
  <meta property="og:description" content="The lecture notes for MATH3030/4068: Multivariate Analysis / Applied Multivariate Statistics" />
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="8.3 Fisher’s linear discriminant rule | Multivariate Statistics" />
  
  <meta name="twitter:description" content="The lecture notes for MATH3030/4068: Multivariate Analysis / Applied Multivariate Statistics" />
  

<meta name="author" content="Prof. Richard Wilkinson" />



  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="8.2-lda-Bayes.html"/>
<link rel="next" href="8.4-computer-tasks-4.html"/>
<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections-1.1.0/anchor-sections.css" rel="stylesheet" />
<link href="libs/anchor-sections-1.1.0/anchor-sections-hash.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.1.0/anchor-sections.js"></script>
<script src="libs/kePrint-0.0.1/kePrint.js"></script>
<link href="libs/lightable-0.0.1/lightable.css" rel="stylesheet" />


<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { display: inline-block; text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { color: #008000; } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { color: #008000; font-weight: bold; } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<style type="text/css">
  
  div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Applied multivariate statistics</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Introduction</a></li>
<li class="chapter" data-level="" data-path="part-i-prerequisites.html"><a href="part-i-prerequisites.html"><i class="fa fa-check"></i>PART I: Prerequisites</a></li>
<li class="chapter" data-level="1" data-path="1-stat-prelim.html"><a href="1-stat-prelim.html"><i class="fa fa-check"></i><b>1</b> Statistical Preliminaries</a>
<ul>
<li class="chapter" data-level="1.1" data-path="1.1-notation.html"><a href="1.1-notation.html"><i class="fa fa-check"></i><b>1.1</b> Notation</a>
<ul>
<li class="chapter" data-level="1.1.1" data-path="1.1-notation.html"><a href="1.1-notation.html#example-datasets"><i class="fa fa-check"></i><b>1.1.1</b> Example datasets</a></li>
<li class="chapter" data-level="1.1.2" data-path="1.1-notation.html"><a href="1.1-notation.html#aims-of-multivariate-data-analysis"><i class="fa fa-check"></i><b>1.1.2</b> Aims of multivariate data analysis</a></li>
</ul></li>
<li class="chapter" data-level="1.2" data-path="1.2-exploratory-data-analysis-eda.html"><a href="1.2-exploratory-data-analysis-eda.html"><i class="fa fa-check"></i><b>1.2</b> Exploratory data analysis (EDA)</a>
<ul>
<li class="chapter" data-level="1.2.1" data-path="1.2-exploratory-data-analysis-eda.html"><a href="1.2-exploratory-data-analysis-eda.html#data-visualization"><i class="fa fa-check"></i><b>1.2.1</b> Data visualization</a></li>
<li class="chapter" data-level="1.2.2" data-path="1.2-exploratory-data-analysis-eda.html"><a href="1.2-exploratory-data-analysis-eda.html#summary-statistics"><i class="fa fa-check"></i><b>1.2.2</b> Summary statistics</a></li>
</ul></li>
<li class="chapter" data-level="1.3" data-path="1.3-randvec.html"><a href="1.3-randvec.html"><i class="fa fa-check"></i><b>1.3</b> Random vectors and matrices</a>
<ul>
<li class="chapter" data-level="1.3.1" data-path="1.3-randvec.html"><a href="1.3-randvec.html#estimators"><i class="fa fa-check"></i><b>1.3.1</b> Estimators</a></li>
</ul></li>
<li class="chapter" data-level="1.4" data-path="1.4-computer-tasks.html"><a href="1.4-computer-tasks.html"><i class="fa fa-check"></i><b>1.4</b> Computer tasks</a></li>
<li class="chapter" data-level="1.5" data-path="1.5-exercises.html"><a href="1.5-exercises.html"><i class="fa fa-check"></i><b>1.5</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="2-linalg-prelim.html"><a href="2-linalg-prelim.html"><i class="fa fa-check"></i><b>2</b> Review of linear algebra</a>
<ul>
<li class="chapter" data-level="2.1" data-path="2.1-linalg-basics.html"><a href="2.1-linalg-basics.html"><i class="fa fa-check"></i><b>2.1</b> Basics</a>
<ul>
<li class="chapter" data-level="2.1.1" data-path="2.1-linalg-basics.html"><a href="2.1-linalg-basics.html#notation-1"><i class="fa fa-check"></i><b>2.1.1</b> Notation</a></li>
<li class="chapter" data-level="2.1.2" data-path="2.1-linalg-basics.html"><a href="2.1-linalg-basics.html#elementary-matrix-operations"><i class="fa fa-check"></i><b>2.1.2</b> Elementary matrix operations</a></li>
<li class="chapter" data-level="2.1.3" data-path="2.1-linalg-basics.html"><a href="2.1-linalg-basics.html#special-matrices"><i class="fa fa-check"></i><b>2.1.3</b> Special matrices</a></li>
<li class="chapter" data-level="2.1.4" data-path="2.1-linalg-basics.html"><a href="2.1-linalg-basics.html#vectordiff"><i class="fa fa-check"></i><b>2.1.4</b> Vector Differentiation</a></li>
</ul></li>
<li class="chapter" data-level="2.2" data-path="2.2-linalg-vecspaces.html"><a href="2.2-linalg-vecspaces.html"><i class="fa fa-check"></i><b>2.2</b> Vector spaces</a>
<ul>
<li class="chapter" data-level="2.2.1" data-path="2.2-linalg-vecspaces.html"><a href="2.2-linalg-vecspaces.html#linear-independence"><i class="fa fa-check"></i><b>2.2.1</b> Linear independence</a></li>
<li class="chapter" data-level="2.2.2" data-path="2.2-linalg-vecspaces.html"><a href="2.2-linalg-vecspaces.html#colsspace"><i class="fa fa-check"></i><b>2.2.2</b> Row and column spaces</a></li>
<li class="chapter" data-level="2.2.3" data-path="2.2-linalg-vecspaces.html"><a href="2.2-linalg-vecspaces.html#linear-transformations"><i class="fa fa-check"></i><b>2.2.3</b> Linear transformations</a></li>
</ul></li>
<li class="chapter" data-level="2.3" data-path="2.3-linalg-innerprod.html"><a href="2.3-linalg-innerprod.html"><i class="fa fa-check"></i><b>2.3</b> Inner product spaces</a>
<ul>
<li class="chapter" data-level="2.3.1" data-path="2.3-linalg-innerprod.html"><a href="2.3-linalg-innerprod.html#normed"><i class="fa fa-check"></i><b>2.3.1</b> Distances, and angles</a></li>
<li class="chapter" data-level="2.3.2" data-path="2.3-linalg-innerprod.html"><a href="2.3-linalg-innerprod.html#orthogonal-matrices"><i class="fa fa-check"></i><b>2.3.2</b> Orthogonal matrices</a></li>
<li class="chapter" data-level="2.3.3" data-path="2.3-linalg-innerprod.html"><a href="2.3-linalg-innerprod.html#projection-matrix"><i class="fa fa-check"></i><b>2.3.3</b> Projections</a></li>
</ul></li>
<li class="chapter" data-level="2.4" data-path="2.4-centering-matrix.html"><a href="2.4-centering-matrix.html"><i class="fa fa-check"></i><b>2.4</b> The Centering Matrix</a></li>
<li class="chapter" data-level="2.5" data-path="2.5-tasks-ch2.html"><a href="2.5-tasks-ch2.html"><i class="fa fa-check"></i><b>2.5</b> Computer tasks</a></li>
<li class="chapter" data-level="2.6" data-path="2.6-exercises-ch2.html"><a href="2.6-exercises-ch2.html"><i class="fa fa-check"></i><b>2.6</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="3-linalg-decomp.html"><a href="3-linalg-decomp.html"><i class="fa fa-check"></i><b>3</b> Matrix decompositions</a>
<ul>
<li class="chapter" data-level="3.1" data-path="3.1-matrix-matrix.html"><a href="3.1-matrix-matrix.html"><i class="fa fa-check"></i><b>3.1</b> Matrix-matrix products</a></li>
<li class="chapter" data-level="3.2" data-path="3.2-spectraleigen-decomposition.html"><a href="3.2-spectraleigen-decomposition.html"><i class="fa fa-check"></i><b>3.2</b> Spectral/eigen decomposition</a>
<ul>
<li class="chapter" data-level="3.2.1" data-path="3.2-spectraleigen-decomposition.html"><a href="3.2-spectraleigen-decomposition.html#eigenvalues-and-eigenvectors"><i class="fa fa-check"></i><b>3.2.1</b> Eigenvalues and eigenvectors</a></li>
<li class="chapter" data-level="3.2.2" data-path="3.2-spectraleigen-decomposition.html"><a href="3.2-spectraleigen-decomposition.html#spectral-decomposition"><i class="fa fa-check"></i><b>3.2.2</b> Spectral decomposition</a></li>
<li class="chapter" data-level="3.2.3" data-path="3.2-spectraleigen-decomposition.html"><a href="3.2-spectraleigen-decomposition.html#matrixroots"><i class="fa fa-check"></i><b>3.2.3</b> Matrix square roots</a></li>
</ul></li>
<li class="chapter" data-level="3.3" data-path="3.3-linalg-SVD.html"><a href="3.3-linalg-SVD.html"><i class="fa fa-check"></i><b>3.3</b> Singular Value Decomposition (SVD)</a>
<ul>
<li class="chapter" data-level="3.3.1" data-path="3.3-linalg-SVD.html"><a href="3.3-linalg-SVD.html#examples"><i class="fa fa-check"></i><b>3.3.1</b> Examples</a></li>
</ul></li>
<li class="chapter" data-level="3.4" data-path="3.4-svdopt.html"><a href="3.4-svdopt.html"><i class="fa fa-check"></i><b>3.4</b> SVD optimization results</a></li>
<li class="chapter" data-level="3.5" data-path="3.5-lowrank.html"><a href="3.5-lowrank.html"><i class="fa fa-check"></i><b>3.5</b> Low-rank approximation</a>
<ul>
<li class="chapter" data-level="3.5.1" data-path="3.5-lowrank.html"><a href="3.5-lowrank.html#matrix-norms"><i class="fa fa-check"></i><b>3.5.1</b> Matrix norms</a></li>
<li class="chapter" data-level="3.5.2" data-path="3.5-lowrank.html"><a href="3.5-lowrank.html#eckart-young-mirsky-theorem"><i class="fa fa-check"></i><b>3.5.2</b> Eckart-Young-Mirsky Theorem</a></li>
<li class="chapter" data-level="3.5.3" data-path="3.5-lowrank.html"><a href="3.5-lowrank.html#example-image-compression"><i class="fa fa-check"></i><b>3.5.3</b> Example: image compression</a></li>
</ul></li>
<li class="chapter" data-level="3.6" data-path="3.6-tasks-ch3.html"><a href="3.6-tasks-ch3.html"><i class="fa fa-check"></i><b>3.6</b> Computer tasks</a></li>
<li class="chapter" data-level="3.7" data-path="3.7-exercises-ch3.html"><a href="3.7-exercises-ch3.html"><i class="fa fa-check"></i><b>3.7</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="part-ii-dimension-reduction-methods.html"><a href="part-ii-dimension-reduction-methods.html"><i class="fa fa-check"></i>PART II: Dimension reduction methods</a>
<ul>
<li class="chapter" data-level="" data-path="part-ii-dimension-reduction-methods.html"><a href="part-ii-dimension-reduction-methods.html#a-warning"><i class="fa fa-check"></i>A warning</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="4-pca.html"><a href="4-pca.html"><i class="fa fa-check"></i><b>4</b> Principal Component Analysis (PCA)</a>
<ul>
<li class="chapter" data-level="4.1" data-path="4.1-pca-an-informal-introduction.html"><a href="4.1-pca-an-informal-introduction.html"><i class="fa fa-check"></i><b>4.1</b> PCA: an informal introduction</a>
<ul>
<li class="chapter" data-level="4.1.1" data-path="4.1-pca-an-informal-introduction.html"><a href="4.1-pca-an-informal-introduction.html#notation-recap"><i class="fa fa-check"></i><b>4.1.1</b> Notation recap</a></li>
<li class="chapter" data-level="4.1.2" data-path="4.1-pca-an-informal-introduction.html"><a href="4.1-pca-an-informal-introduction.html#first-principal-component"><i class="fa fa-check"></i><b>4.1.2</b> First principal component</a></li>
<li class="chapter" data-level="4.1.3" data-path="4.1-pca-an-informal-introduction.html"><a href="4.1-pca-an-informal-introduction.html#second-principal-component"><i class="fa fa-check"></i><b>4.1.3</b> Second principal component</a></li>
<li class="chapter" data-level="4.1.4" data-path="4.1-pca-an-informal-introduction.html"><a href="4.1-pca-an-informal-introduction.html#geometric-interpretation-1"><i class="fa fa-check"></i><b>4.1.4</b> Geometric interpretation</a></li>
<li class="chapter" data-level="4.1.5" data-path="4.1-pca-an-informal-introduction.html"><a href="4.1-pca-an-informal-introduction.html#example"><i class="fa fa-check"></i><b>4.1.5</b> Example</a></li>
<li class="chapter" data-level="4.1.6" data-path="4.1-pca-an-informal-introduction.html"><a href="4.1-pca-an-informal-introduction.html#example-iris"><i class="fa fa-check"></i><b>4.1.6</b> Example: Iris</a></li>
</ul></li>
<li class="chapter" data-level="4.2" data-path="4.2-pca-a-formal-description-with-proofs.html"><a href="4.2-pca-a-formal-description-with-proofs.html"><i class="fa fa-check"></i><b>4.2</b> PCA: a formal description with proofs</a>
<ul>
<li class="chapter" data-level="4.2.1" data-path="4.2-pca-a-formal-description-with-proofs.html"><a href="4.2-pca-a-formal-description-with-proofs.html#properties-of-principal-components"><i class="fa fa-check"></i><b>4.2.1</b> Properties of principal components</a></li>
<li class="chapter" data-level="4.2.2" data-path="4.2-pca-a-formal-description-with-proofs.html"><a href="4.2-pca-a-formal-description-with-proofs.html#pca:football"><i class="fa fa-check"></i><b>4.2.2</b> Example: Football</a></li>
<li class="chapter" data-level="4.2.3" data-path="4.2-pca-a-formal-description-with-proofs.html"><a href="4.2-pca-a-formal-description-with-proofs.html#pcawithR"><i class="fa fa-check"></i><b>4.2.3</b> PCA based on <span class="math inline">\(\mathbf R\)</span> versus PCA based on <span class="math inline">\(\mathbf S\)</span></a></li>
<li class="chapter" data-level="4.2.4" data-path="4.2-pca-a-formal-description-with-proofs.html"><a href="4.2-pca-a-formal-description-with-proofs.html#population-pca"><i class="fa fa-check"></i><b>4.2.4</b> Population PCA</a></li>
<li class="chapter" data-level="4.2.5" data-path="4.2-pca-a-formal-description-with-proofs.html"><a href="4.2-pca-a-formal-description-with-proofs.html#pca-under-transformations-of-variables"><i class="fa fa-check"></i><b>4.2.5</b> PCA under transformations of variables</a></li>
</ul></li>
<li class="chapter" data-level="4.3" data-path="4.3-an-alternative-view-of-pca.html"><a href="4.3-an-alternative-view-of-pca.html"><i class="fa fa-check"></i><b>4.3</b> An alternative view of PCA</a>
<ul>
<li class="chapter" data-level="4.3.1" data-path="4.3-an-alternative-view-of-pca.html"><a href="4.3-an-alternative-view-of-pca.html#pca-mnist"><i class="fa fa-check"></i><b>4.3.1</b> Example: MNIST handwritten digits</a></li>
</ul></li>
<li class="chapter" data-level="4.4" data-path="4.4-pca-comptask.html"><a href="4.4-pca-comptask.html"><i class="fa fa-check"></i><b>4.4</b> Computer tasks</a></li>
<li class="chapter" data-level="4.5" data-path="4.5-exercises-1.html"><a href="4.5-exercises-1.html"><i class="fa fa-check"></i><b>4.5</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="5-cca.html"><a href="5-cca.html"><i class="fa fa-check"></i><b>5</b> Canonical Correlation Analysis (CCA)</a>
<ul>
<li class="chapter" data-level="5.1" data-path="5.1-cca1.html"><a href="5.1-cca1.html"><i class="fa fa-check"></i><b>5.1</b> The first pair of canonical variables</a>
<ul>
<li class="chapter" data-level="5.1.1" data-path="5.1-cca1.html"><a href="5.1-cca1.html#the-first-canonical-components"><i class="fa fa-check"></i><b>5.1.1</b> The first canonical components</a></li>
<li class="chapter" data-level="5.1.2" data-path="5.1-cca1.html"><a href="5.1-cca1.html#premcca"><i class="fa fa-check"></i><b>5.1.2</b> Example: Premier league football</a></li>
</ul></li>
<li class="chapter" data-level="5.2" data-path="5.2-the-full-set-of-canonical-correlations.html"><a href="5.2-the-full-set-of-canonical-correlations.html"><i class="fa fa-check"></i><b>5.2</b> The full set of canonical correlations</a>
<ul>
<li class="chapter" data-level="5.2.1" data-path="5.2-the-full-set-of-canonical-correlations.html"><a href="5.2-the-full-set-of-canonical-correlations.html#example-continued"><i class="fa fa-check"></i><b>5.2.1</b> Example continued</a></li>
</ul></li>
<li class="chapter" data-level="5.3" data-path="5.3-properties.html"><a href="5.3-properties.html"><i class="fa fa-check"></i><b>5.3</b> Properties</a>
<ul>
<li class="chapter" data-level="5.3.1" data-path="5.3-properties.html"><a href="5.3-properties.html#connection-with-linear-regression-when-q1"><i class="fa fa-check"></i><b>5.3.1</b> Connection with linear regression when <span class="math inline">\(q=1\)</span></a></li>
<li class="chapter" data-level="5.3.2" data-path="5.3-properties.html"><a href="5.3-properties.html#invarianceequivariance-properties-of-cca"><i class="fa fa-check"></i><b>5.3.2</b> Invariance/equivariance properties of CCA</a></li>
</ul></li>
<li class="chapter" data-level="5.4" data-path="5.4-computer-tasks-1.html"><a href="5.4-computer-tasks-1.html"><i class="fa fa-check"></i><b>5.4</b> Computer tasks</a></li>
<li class="chapter" data-level="5.5" data-path="5.5-exercises-2.html"><a href="5.5-exercises-2.html"><i class="fa fa-check"></i><b>5.5</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="6-mds.html"><a href="6-mds.html"><i class="fa fa-check"></i><b>6</b> Multidimensional Scaling (MDS)</a>
<ul>
<li class="chapter" data-level="6.1" data-path="6.1-classical-mds.html"><a href="6.1-classical-mds.html"><i class="fa fa-check"></i><b>6.1</b> Classical MDS</a>
<ul>
<li class="chapter" data-level="6.1.1" data-path="6.1-classical-mds.html"><a href="6.1-classical-mds.html#non-euclidean-distance-matrices"><i class="fa fa-check"></i><b>6.1.1</b> Non-Euclidean distance matrices</a></li>
<li class="chapter" data-level="6.1.2" data-path="6.1-classical-mds.html"><a href="6.1-classical-mds.html#principal-coordinate-analysis"><i class="fa fa-check"></i><b>6.1.2</b> Principal Coordinate Analysis</a></li>
</ul></li>
<li class="chapter" data-level="6.2" data-path="6.2-similarity.html"><a href="6.2-similarity.html"><i class="fa fa-check"></i><b>6.2</b> Similarity measures</a>
<ul>
<li class="chapter" data-level="6.2.1" data-path="6.2-similarity.html"><a href="6.2-similarity.html#binary-attributes"><i class="fa fa-check"></i><b>6.2.1</b> Binary attributes</a></li>
<li class="chapter" data-level="6.2.2" data-path="6.2-similarity.html"><a href="6.2-similarity.html#example-classical-mds-with-the-mnist-data"><i class="fa fa-check"></i><b>6.2.2</b> Example: Classical MDS with the MNIST data</a></li>
</ul></li>
<li class="chapter" data-level="6.3" data-path="6.3-non-metric-mds.html"><a href="6.3-non-metric-mds.html"><i class="fa fa-check"></i><b>6.3</b> Non-metric MDS</a></li>
<li class="chapter" data-level="6.4" data-path="6.4-exercises-3.html"><a href="6.4-exercises-3.html"><i class="fa fa-check"></i><b>6.4</b> Exercises</a></li>
<li class="chapter" data-level="6.5" data-path="6.5-computer-tasks-2.html"><a href="6.5-computer-tasks-2.html"><i class="fa fa-check"></i><b>6.5</b> Computer Tasks</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="part-iii-inference-using-the-multivariate-normal-distribution-mvn.html"><a href="part-iii-inference-using-the-multivariate-normal-distribution-mvn.html"><i class="fa fa-check"></i>Part III: Inference using the Multivariate Normal Distribution (MVN)</a></li>
<li class="chapter" data-level="7" data-path="7-multinormal.html"><a href="7-multinormal.html"><i class="fa fa-check"></i><b>7</b> The Multivariate Normal Distribution</a>
<ul>
<li class="chapter" data-level="7.1" data-path="7.1-definition-and-properties-of-the-mvn.html"><a href="7.1-definition-and-properties-of-the-mvn.html"><i class="fa fa-check"></i><b>7.1</b> Definition and Properties of the MVN</a>
<ul>
<li class="chapter" data-level="7.1.1" data-path="7.1-definition-and-properties-of-the-mvn.html"><a href="7.1-definition-and-properties-of-the-mvn.html#basics"><i class="fa fa-check"></i><b>7.1.1</b> Basics</a></li>
<li class="chapter" data-level="7.1.2" data-path="7.1-definition-and-properties-of-the-mvn.html"><a href="7.1-definition-and-properties-of-the-mvn.html#transformations"><i class="fa fa-check"></i><b>7.1.2</b> Transformations</a></li>
<li class="chapter" data-level="7.1.3" data-path="7.1-definition-and-properties-of-the-mvn.html"><a href="7.1-definition-and-properties-of-the-mvn.html#independence"><i class="fa fa-check"></i><b>7.1.3</b> Independence</a></li>
<li class="chapter" data-level="7.1.4" data-path="7.1-definition-and-properties-of-the-mvn.html"><a href="7.1-definition-and-properties-of-the-mvn.html#confidence-ellipses"><i class="fa fa-check"></i><b>7.1.4</b> Confidence ellipses</a></li>
<li class="chapter" data-level="7.1.5" data-path="7.1-definition-and-properties-of-the-mvn.html"><a href="7.1-definition-and-properties-of-the-mvn.html#sampling-results-for-the-mvn"><i class="fa fa-check"></i><b>7.1.5</b> Sampling results for the MVN</a></li>
</ul></li>
<li class="chapter" data-level="7.2" data-path="7.2-the-wishart-distribution.html"><a href="7.2-the-wishart-distribution.html"><i class="fa fa-check"></i><b>7.2</b> The Wishart distribution</a>
<ul>
<li class="chapter" data-level="7.2.1" data-path="7.2-the-wishart-distribution.html"><a href="7.2-the-wishart-distribution.html#properties-1"><i class="fa fa-check"></i><b>7.2.1</b> Properties</a></li>
<li class="chapter" data-level="7.2.2" data-path="7.2-the-wishart-distribution.html"><a href="7.2-the-wishart-distribution.html#cochrans-theorem"><i class="fa fa-check"></i><b>7.2.2</b> Cochran’s theorem</a></li>
</ul></li>
<li class="chapter" data-level="7.3" data-path="7.3-hotellings-t2-distribution.html"><a href="7.3-hotellings-t2-distribution.html"><i class="fa fa-check"></i><b>7.3</b> Hotelling’s <span class="math inline">\(T^2\)</span> distribution</a></li>
<li class="chapter" data-level="7.4" data-path="7.4-inference-based-on-the-mvn.html"><a href="7.4-inference-based-on-the-mvn.html"><i class="fa fa-check"></i><b>7.4</b> Inference based on the MVN</a>
<ul>
<li class="chapter" data-level="7.4.1" data-path="7.4-inference-based-on-the-mvn.html"><a href="7.4-inference-based-on-the-mvn.html#onesampleSigma"><i class="fa fa-check"></i><b>7.4.1</b> <span class="math inline">\(\boldsymbol{\Sigma}\)</span> known</a></li>
<li class="chapter" data-level="7.4.2" data-path="7.4-inference-based-on-the-mvn.html"><a href="7.4-inference-based-on-the-mvn.html#onesample"><i class="fa fa-check"></i><b>7.4.2</b> <span class="math inline">\(\boldsymbol{\Sigma}\)</span> unknown: 1 sample</a></li>
<li class="chapter" data-level="7.4.3" data-path="7.4-inference-based-on-the-mvn.html"><a href="7.4-inference-based-on-the-mvn.html#boldsymbolsigma-unknown-2-samples"><i class="fa fa-check"></i><b>7.4.3</b> <span class="math inline">\(\boldsymbol{\Sigma}\)</span> unknown: 2 samples</a></li>
</ul></li>
<li class="chapter" data-level="7.5" data-path="7.5-exercises-4.html"><a href="7.5-exercises-4.html"><i class="fa fa-check"></i><b>7.5</b> Exercises</a></li>
<li class="chapter" data-level="7.6" data-path="7.6-computer-tasks-3.html"><a href="7.6-computer-tasks-3.html"><i class="fa fa-check"></i><b>7.6</b> Computer tasks</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="part-iv-classification-and-clustering.html"><a href="part-iv-classification-and-clustering.html"><i class="fa fa-check"></i>Part IV: Classification and Clustering</a></li>
<li class="chapter" data-level="8" data-path="8-lda.html"><a href="8-lda.html"><i class="fa fa-check"></i><b>8</b> Discriminant analysis</a>
<ul>
<li class="chapter" data-level="" data-path="8-lda.html"><a href="8-lda.html#linear-discriminant-analysis"><i class="fa fa-check"></i>Linear discriminant analysis</a></li>
<li class="chapter" data-level="8.1" data-path="8.1-lda-ML.html"><a href="8.1-lda-ML.html"><i class="fa fa-check"></i><b>8.1</b> Maximum likelihood (ML) discriminant rule</a>
<ul>
<li class="chapter" data-level="8.1.1" data-path="8.1-lda-ML.html"><a href="8.1-lda-ML.html#multivariate-normal-populations"><i class="fa fa-check"></i><b>8.1.1</b> Multivariate normal populations</a></li>
<li class="chapter" data-level="8.1.2" data-path="8.1-lda-ML.html"><a href="8.1-lda-ML.html#sample-lda"><i class="fa fa-check"></i><b>8.1.2</b> The sample ML discriminant rule</a></li>
<li class="chapter" data-level="8.1.3" data-path="8.1-lda-ML.html"><a href="8.1-lda-ML.html#two-populations"><i class="fa fa-check"></i><b>8.1.3</b> Two populations</a></li>
<li class="chapter" data-level="8.1.4" data-path="8.1-lda-ML.html"><a href="8.1-lda-ML.html#more-than-two-populations"><i class="fa fa-check"></i><b>8.1.4</b> More than two populations</a></li>
</ul></li>
<li class="chapter" data-level="8.2" data-path="8.2-lda-Bayes.html"><a href="8.2-lda-Bayes.html"><i class="fa fa-check"></i><b>8.2</b> Bayes discriminant rule</a>
<ul>
<li class="chapter" data-level="8.2.1" data-path="8.2-lda-Bayes.html"><a href="8.2-lda-Bayes.html#example-lda-using-the-iris-data"><i class="fa fa-check"></i><b>8.2.1</b> Example: LDA using the Iris data</a></li>
<li class="chapter" data-level="8.2.2" data-path="8.2-lda-Bayes.html"><a href="8.2-lda-Bayes.html#quadratic-discriminant-analysis-qda"><i class="fa fa-check"></i><b>8.2.2</b> Quadratic Discriminant Analysis (QDA)</a></li>
<li class="chapter" data-level="8.2.3" data-path="8.2-lda-Bayes.html"><a href="8.2-lda-Bayes.html#prediction-accuracy"><i class="fa fa-check"></i><b>8.2.3</b> Prediction accuracy</a></li>
</ul></li>
<li class="chapter" data-level="8.3" data-path="8.3-FLDA.html"><a href="8.3-FLDA.html"><i class="fa fa-check"></i><b>8.3</b> Fisher’s linear discriminant rule</a>
<ul>
<li class="chapter" data-level="8.3.1" data-path="8.3-FLDA.html"><a href="8.3-FLDA.html#iris-example-continued-1"><i class="fa fa-check"></i><b>8.3.1</b> Iris example continued</a></li>
<li class="chapter" data-level="8.3.2" data-path="8.3-FLDA.html"><a href="8.3-FLDA.html#links-between-methods"><i class="fa fa-check"></i><b>8.3.2</b> Links between methods</a></li>
</ul></li>
<li class="chapter" data-level="8.4" data-path="8.4-computer-tasks-4.html"><a href="8.4-computer-tasks-4.html"><i class="fa fa-check"></i><b>8.4</b> Computer tasks</a></li>
<li class="chapter" data-level="8.5" data-path="8.5-exercises-5.html"><a href="8.5-exercises-5.html"><i class="fa fa-check"></i><b>8.5</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="9" data-path="9-cluster.html"><a href="9-cluster.html"><i class="fa fa-check"></i><b>9</b> Cluster Analysis</a>
<ul>
<li class="chapter" data-level="9.1" data-path="9.1-k-means-clustering.html"><a href="9.1-k-means-clustering.html"><i class="fa fa-check"></i><b>9.1</b> K-means clustering</a>
<ul>
<li class="chapter" data-level="9.1.1" data-path="9.1-k-means-clustering.html"><a href="9.1-k-means-clustering.html#estimating-boldsymbol-delta"><i class="fa fa-check"></i><b>9.1.1</b> Estimating <span class="math inline">\(\boldsymbol \delta\)</span></a></li>
<li class="chapter" data-level="9.1.2" data-path="9.1-k-means-clustering.html"><a href="9.1-k-means-clustering.html#k-means"><i class="fa fa-check"></i><b>9.1.2</b> K-means</a></li>
<li class="chapter" data-level="9.1.3" data-path="9.1-k-means-clustering.html"><a href="9.1-k-means-clustering.html#example-iris-data"><i class="fa fa-check"></i><b>9.1.3</b> Example: Iris data</a></li>
<li class="chapter" data-level="9.1.4" data-path="9.1-k-means-clustering.html"><a href="9.1-k-means-clustering.html#choosing-k"><i class="fa fa-check"></i><b>9.1.4</b> Choosing <span class="math inline">\(K\)</span></a></li>
</ul></li>
<li class="chapter" data-level="9.2" data-path="9.2-model-based-clustering.html"><a href="9.2-model-based-clustering.html"><i class="fa fa-check"></i><b>9.2</b> Model-based clustering</a>
<ul>
<li class="chapter" data-level="9.2.1" data-path="9.2-model-based-clustering.html"><a href="9.2-model-based-clustering.html#maximum-likelihood-estimation"><i class="fa fa-check"></i><b>9.2.1</b> Maximum-likelihood estimation</a></li>
<li class="chapter" data-level="9.2.2" data-path="9.2-model-based-clustering.html"><a href="9.2-model-based-clustering.html#multivariate-gaussian-clusters"><i class="fa fa-check"></i><b>9.2.2</b> Multivariate Gaussian clusters</a></li>
<li class="chapter" data-level="9.2.3" data-path="9.2-model-based-clustering.html"><a href="9.2-model-based-clustering.html#example-iris-1"><i class="fa fa-check"></i><b>9.2.3</b> Example: Iris</a></li>
</ul></li>
<li class="chapter" data-level="9.3" data-path="9.3-hierarchical-clustering-methods.html"><a href="9.3-hierarchical-clustering-methods.html"><i class="fa fa-check"></i><b>9.3</b> Hierarchical clustering methods</a>
<ul>
<li class="chapter" data-level="9.3.1" data-path="9.3-hierarchical-clustering-methods.html"><a href="9.3-hierarchical-clustering-methods.html#distance-measures"><i class="fa fa-check"></i><b>9.3.1</b> Distance measures</a></li>
<li class="chapter" data-level="9.3.2" data-path="9.3-hierarchical-clustering-methods.html"><a href="9.3-hierarchical-clustering-methods.html#toy-example"><i class="fa fa-check"></i><b>9.3.2</b> Toy Example</a></li>
<li class="chapter" data-level="9.3.3" data-path="9.3-hierarchical-clustering-methods.html"><a href="9.3-hierarchical-clustering-methods.html#comparison-of-methods"><i class="fa fa-check"></i><b>9.3.3</b> Comparison of methods</a></li>
</ul></li>
<li class="chapter" data-level="9.4" data-path="9.4-summary.html"><a href="9.4-summary.html"><i class="fa fa-check"></i><b>9.4</b> Summary</a></li>
<li class="chapter" data-level="9.5" data-path="9.5-computer-tasks-5.html"><a href="9.5-computer-tasks-5.html"><i class="fa fa-check"></i><b>9.5</b> Computer tasks</a></li>
<li class="chapter" data-level="9.6" data-path="9.6-exercises-6.html"><a href="9.6-exercises-6.html"><i class="fa fa-check"></i><b>9.6</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="10" data-path="10-lm.html"><a href="10-lm.html"><i class="fa fa-check"></i><b>10</b> Linear Models</a>
<ul>
<li class="chapter" data-level="" data-path="10-lm.html"><a href="10-lm.html#notation-3"><i class="fa fa-check"></i>Notation</a></li>
<li class="chapter" data-level="10.1" data-path="10.1-ordinary-least-squares-ols.html"><a href="10.1-ordinary-least-squares-ols.html"><i class="fa fa-check"></i><b>10.1</b> Ordinary least squares (OLS)</a>
<ul>
<li class="chapter" data-level="10.1.1" data-path="10.1-ordinary-least-squares-ols.html"><a href="10.1-ordinary-least-squares-ols.html#geometry"><i class="fa fa-check"></i><b>10.1.1</b> Geometry</a></li>
<li class="chapter" data-level="10.1.2" data-path="10.1-ordinary-least-squares-ols.html"><a href="10.1-ordinary-least-squares-ols.html#normal-linear-model"><i class="fa fa-check"></i><b>10.1.2</b> Normal linear model</a></li>
<li class="chapter" data-level="10.1.3" data-path="10.1-ordinary-least-squares-ols.html"><a href="10.1-ordinary-least-squares-ols.html#linear-models-in-r"><i class="fa fa-check"></i><b>10.1.3</b> Linear models in R</a></li>
<li class="chapter" data-level="10.1.4" data-path="10.1-ordinary-least-squares-ols.html"><a href="10.1-ordinary-least-squares-ols.html#problems-with-ols"><i class="fa fa-check"></i><b>10.1.4</b> Problems with OLS</a></li>
</ul></li>
<li class="chapter" data-level="10.2" data-path="10.2-principal-component-regression-pcr.html"><a href="10.2-principal-component-regression-pcr.html"><i class="fa fa-check"></i><b>10.2</b> Principal component regression (PCR)</a>
<ul>
<li class="chapter" data-level="10.2.1" data-path="10.2-principal-component-regression-pcr.html"><a href="10.2-principal-component-regression-pcr.html#pcr-in-r"><i class="fa fa-check"></i><b>10.2.1</b> PCR in R</a></li>
</ul></li>
<li class="chapter" data-level="10.3" data-path="10.3-shrinkage-methods.html"><a href="10.3-shrinkage-methods.html"><i class="fa fa-check"></i><b>10.3</b> Shrinkage methods</a>
<ul>
<li class="chapter" data-level="10.3.1" data-path="10.3-shrinkage-methods.html"><a href="10.3-shrinkage-methods.html#ridge-regression-in-r"><i class="fa fa-check"></i><b>10.3.1</b> Ridge regression in R</a></li>
</ul></li>
<li class="chapter" data-level="10.4" data-path="10.4-multi-output-linear-model.html"><a href="10.4-multi-output-linear-model.html"><i class="fa fa-check"></i><b>10.4</b> Multi-output Linear Model</a></li>
<li class="chapter" data-level="10.5" data-path="10.5-computer-tasks-6.html"><a href="10.5-computer-tasks-6.html"><i class="fa fa-check"></i><b>10.5</b> Computer tasks</a></li>
<li class="chapter" data-level="10.6" data-path="10.6-exercises-7.html"><a href="10.6-exercises-7.html"><i class="fa fa-check"></i><b>10.6</b> Exercises</a></li>
</ul></li>
<li class="divider"></li>
<li> <a href="https://rich-d-wilkinson.github.io/teaching.html" target="blank">University of Nottingham</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Multivariate Statistics</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="FLDA" class="section level2 hasAnchor" number="8.3">
<h2><span class="header-section-number">8.3</span> Fisher’s linear discriminant rule<a href="8.3-FLDA.html#FLDA" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Thus far we have assumed that observations from population <span class="math inline">\(\Pi_j\)</span> have a <span class="math inline">\(N_p ( \boldsymbol{\mu}_j, {\mathbf \Sigma})\)</span> distribution, and then used the MVN log-likelihood to derive the discriminant functions <span class="math inline">\(\delta_j(\mathbf x)\)</span>. The famous statistician R. A. Fisher took an alternative approach and looked for a linear discriminant functions without assuming any particular distribution for each population <span class="math inline">\(\Pi_j\)</span>.</p>
<p>This way of thinking leads to a form of dimension reduction. We find a projection of the data into a lower dimensional space that is optimal for classifying the data into the different populations.</p>
<div id="variance-decomposition" class="section level4 unnumbered hasAnchor">
<h4>Variance decomposition<a href="8.3-FLDA.html#variance-decomposition" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>Suppose we have a training sample <span class="math inline">\(\mathbf x_{1,j}, \ldots, \mathbf x_{n_j,j}\)</span> from <span class="math inline">\(\Pi_j\)</span> for <span class="math inline">\(j=1,\ldots,g\)</span>.
Fisher’s approach starts by splitting the total covariance matrix of the data (i.e. ignoring class labels) into two parts.</p>
<p><span class="math display">\[\begin{align*}
n\mathbf S=\mathbf X^\top\mathbf H\mathbf X&amp;= \sum_{j=1}^g\sum_{i=1}^{n_j} (\mathbf x_{i,j} - \bar{\mathbf x})(\mathbf x_{i,j} - \bar{\mathbf x})^\top\\
&amp;=\sum_{j=1}^g\sum_{i=1}^{n_j} (\mathbf x_{i,j} - \hat{{\boldsymbol{\mu}}}_j+\hat{{\boldsymbol{\mu}}}_j-\bar{\mathbf x})(\mathbf x_{i,j} - \hat{{\boldsymbol{\mu}}}_j+\hat{{\boldsymbol{\mu}}}_j-\bar{\mathbf x})^\top\\
&amp;= \sum_{j=1}^g\sum_{i=1}^{n_j} (\mathbf x_{i,j} - \hat{{\boldsymbol{\mu}}}_j)(\mathbf x_{i,j} - \hat{{\boldsymbol{\mu}}}_j)^\top+
\sum_{j=1}^g n_j (\hat{{\boldsymbol{\mu}}}_j-\bar{\mathbf x})(\hat{{\boldsymbol{\mu}}}_j-\bar{\mathbf x})^\top\\
&amp;=n\mathbf W+n\mathbf B
\end{align*}\]</span>
where <span class="math inline">\(\hat{{\boldsymbol{\mu}}}_j=\frac{1}{n_j} \sum \mathbf x_{i,j} = \bar{\mathbf x}_{+,j}\)</span> is the sample mean of the <span class="math inline">\(j\)</span>th group, <span class="math inline">\(\bar{\mathbf x} = \frac{1}{n} \sum_{j=1}^g \sum_{i=1}^{n_j}
\mathbf x_{ij}\)</span> is the overall mean, and <span class="math inline">\(n=\sum_{j=1}^g n_j\)</span>.</p>
<p>This has split the total covariance matrix into a <strong>within-class</strong> covariance matrix
<span class="math display">\[ \mathbf W= \frac{1}{n}\sum_{j=1}^g \sum_{i=1}^{n_j} (\mathbf x_{ij} - \hat{{\boldsymbol{\mu}}}_j) (\mathbf x_{ij} - \hat{{\boldsymbol{\mu}}}_j)^\top  = \frac{1}{n}\sum_{j=1}^g n_j \mathbf S_j \]</span>
and a <strong>between-class</strong> covariance matrix
<span class="math display">\[ \mathbf B= \frac{1}{n}\sum_{j=1}^g n_j (\hat{{\boldsymbol{\mu}}}_j - \bar{\mathbf x}) (\hat{{\boldsymbol{\mu}}}_j - \bar{\mathbf x})^\top\]</span>
i.e.
<span class="math display">\[\mathbf S= \mathbf W+ \mathbf B.\]</span></p>
<ol style="list-style-type: decimal">
<li><p><span class="math inline">\(\mathbf W\)</span> is an estimator of <span class="math inline">\(\boldsymbol{\Sigma}\)</span>, the shared covariance matrix in the MVN distributions for each population (c.f. Equation.
<a href="8.1-lda-ML.html#eq:ldawithin">(8.4)</a>).</p></li>
<li><p>If <span class="math inline">\(\mathbf M\)</span> is a <span class="math inline">\(n \times p\)</span> matrix of estimated class centroids for each observation
<span class="math display">\[\mathbf M= \begin{pmatrix} -&amp; \hat{{\boldsymbol{\mu}}}_1 &amp;-\\
&amp;\vdots &amp;\\
-&amp; \hat{{\boldsymbol{\mu}}}_1 &amp;-\\
-&amp; \hat{{\boldsymbol{\mu}}}_2 &amp;-\\
&amp;\vdots&amp;\\
-&amp; \hat{{\boldsymbol{\mu}}}_g &amp;-\\
&amp;\vdots &amp;\\
-&amp; \hat{{\boldsymbol{\mu}}}_g &amp;-\end{pmatrix}\]</span>
then <span class="math inline">\(\mathbf B=\frac{1}{n}\mathbf M^\top\mathbf H\mathbf M\)</span> is the covariance matrix of <span class="math inline">\(\mathbf M\)</span>.</p></li>
</ol>
</div>
<div id="fishers-criterion" class="section level4 unnumbered hasAnchor">
<h4>Fisher’s criterion<a href="8.3-FLDA.html#fishers-criterion" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>Fisher’s approach was to find a projection of the data <span class="math inline">\(z_i = \mathbf a^\top \mathbf x_i\)</span> or
<span class="math display">\[\mathbf z= \mathbf X\mathbf a\]</span>
in vector form,
that maximizes the between-class variance relative to the within-class variance.</p>
<p>Using the variance decomposition from above, we can see that the total variance of <span class="math inline">\(\mathbf z\)</span> is
<span class="math display">\[\begin{align*}
{\mathbb{V}\operatorname{ar}}(z_i)&amp;= {\mathbb{V}\operatorname{ar}}(\mathbf a^\top \mathbf x_i)\\
&amp;=\mathbf a^\top {\mathbb{V}\operatorname{ar}}(\mathbf x_i) \mathbf a\\
&amp;=\mathbf a^\top \mathbf S\mathbf a\\
&amp;= \mathbf a^\top\mathbf W\mathbf a+ \mathbf a^\top\mathbf B\mathbf a\\
\end{align*}\]</span>
which we have decomposed into the within-class variance of <span class="math inline">\(\mathbf z\)</span> and the between-class variance <span class="math inline">\(\mathbf z\)</span>.</p>
<p>Fisher’s criterion is to choose a vector, <span class="math inline">\(\mathbf a\)</span>, to maximise the ratio of the <strong>between-class</strong> variance relative to the <strong>within-class</strong> variance of <span class="math inline">\(\mathbf z=\mathbf X\mathbf a\)</span>, i.e., to solve
<span class="math display" id="eq:ldaFisheropt">\[\begin{equation}
\max_{\mathbf a}\frac{\mathbf a^\top \mathbf B\mathbf a}{\mathbf a^\top \mathbf W\mathbf a}, \tag{8.5}
\end{equation}\]</span></p>
<p>The idea is that this choice of <span class="math inline">\(\mathbf a\)</span> will make the classes most easily separable.</p>
</div>
<div id="solving-the-optimization-problem" class="section level4 unnumbered hasAnchor">
<h4>Solving the optimization problem<a href="8.3-FLDA.html#solving-the-optimization-problem" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>How do we solve the optimization problem <a href="8.3-FLDA.html#eq:ldaFisheropt">(8.5)</a> and find the optimal choice of <span class="math inline">\(\mathbf a\)</span>?</p>
<div class="proposition">
<p><span id="prp:nine2" class="proposition"><strong>Proposition 8.4  </strong></span>A vector <span class="math inline">\(\mathbf a\)</span> that solves <span class="math display">\[\max_{\mathbf a}\frac{\mathbf a^\top \mathbf B\mathbf a}{\mathbf a^\top \mathbf W\mathbf a}\]</span> is an eigenvector of <span class="math inline">\(\mathbf W^{-1}\mathbf B\)</span> corresponding to the largest eigenvalue of <span class="math inline">\(\mathbf W^{-1}\mathbf B\)</span>.</p>
</div>
<div class="proof">
<p><span id="unlabeled-div-45" class="proof"><em>Proof</em>. </span>Firstly, note that an equivalent optimization problem is</p>
<p><span class="math display">\[\begin{align*}
\mbox{Maximize } &amp;\mathbf a^\top \mathbf B\mathbf a\\
\mbox{ subject to } &amp;\mathbf a^\top \mathbf W\mathbf a=1
\end{align*}\]</span></p>
<p>as we can rescale <span class="math inline">\(\mathbf a\)</span> without changing the objective <a href="8.3-FLDA.html#eq:ldaFisheropt">(8.5)</a>. This looks a lot like the optimization problems we saw in the chapters on PCA and CCA.</p>
<p>To solve this, note that if we write <span class="math inline">\(\mathbf b=\mathbf W^{\frac{1}{2}}\mathbf a\)</span> then the optimization problem becomes</p>
<p><span class="math display">\[\begin{align*}
\mbox{Maximize } &amp;\mathbf b^\top \mathbf W^{-\frac{1}{2}}\mathbf B\mathbf W^{-\frac{1}{2}}\mathbf b\\
\mbox{ subject to } &amp;\mathbf b^\top \mathbf b=1.
\end{align*}\]</span>
Proposition <a href="3.4-svdopt.html#prp:two8">3.7</a> tells us that the maximum is obtained when <span class="math inline">\(\mathbf b=\mathbf v_1\)</span>, where <span class="math inline">\(\mathbf v_1\)</span> is the eigenvector of <span class="math inline">\(\mathbf W^{-\frac{1}{2}}\mathbf B\mathbf W^{-\frac{1}{2}}\)</span> corresponding to the largest eigenvalue of <span class="math inline">\(\mathbf W^{-\frac{1}{2}}\mathbf B\mathbf W^{-\frac{1}{2}}\)</span>, <span class="math inline">\(\lambda_1\)</span> say.</p>
<p>Converting back to <span class="math inline">\(\mathbf a\)</span> gives the solution to the original optimization problem <a href="8.3-FLDA.html#eq:ldaFisheropt">(8.5)</a> to be
<span class="math display">\[\mathbf a= \mathbf W^{-\frac{1}{2}}\mathbf v_1\]</span></p>
<p>Note that this is an eigenvalue of <span class="math inline">\(\mathbf W^{-1}\mathbf B\)</span></p>
<p><span class="math display">\[\begin{align*}
\mathbf W^{-1}\mathbf B\mathbf a&amp;= \mathbf W^{-1}\mathbf B\mathbf W^{-\frac{1}{2}}\mathbf v_1 \\
&amp;= \mathbf W^{-\frac{1}{2}}\mathbf W^{-\frac{1}{2}}\mathbf B\mathbf W^{-\frac{1}{2}}\mathbf v_1\\
&amp;= \lambda_1\mathbf W^{-\frac{1}{2}}\mathbf v_1\\
&amp;= \lambda_1 \mathbf a
\end{align*}\]</span></p>
<p>Finally, to complete the proof we should check that <span class="math inline">\(\mathbf W^{-1}\mathbf B\)</span> doesn’t have any larger eigenvalues, but we can do this by showing that its eigenvalues are the same as the eigenvalues of <span class="math inline">\(\mathbf W^{-\frac{1}{2}}\mathbf B\mathbf W^{-\frac{1}{2}}\)</span>. This is left as an exercise (note we’ve already done one direction - all you need to do is show that if <span class="math inline">\(\mathbf W^{-1}\mathbf B\)</span> has eigenvalue <span class="math inline">\(\lambda\)</span> then so does <span class="math inline">\(\mathbf W^{-\frac{1}{2}}\mathbf B\mathbf W^{-\frac{1}{2}}\)</span>).</p>
</div>
<!--Assume $\bW$ is positive definite and note that $\bW$ is symmetric, so we can use the spectral decomposition theorem and write $\bW = \bQ \ba \bQ^\top$.

Define $\bgamma = \bW^{1/2} \ba$.  Then $\ba = \bW^{-1/2} \bgamma$ where
$\bW^{-1/2}=\bQ \ba^{-1/2} \bQ^\top$ and
\begin{eqnarray*}
\max_{\ba \colon \ba^\top \ba=1} \left\{ \frac{\ba^\top \bB \ba}{\ba^\top \bW \ba} \right\}
&=& \max_{\bgamma \colon \bgamma \neq \bzero} \left\{ \frac{\bgamma^\top \bW^{-1/2} \bB \bW^{-1/2} \bgamma} {\bgamma^\top \bW^{-1/2} \bW \bW^{-1/2} \bgamma} \right\} \\
&=& \max_{\bgamma \colon \bgamma \neq \bzero} \left\{ \frac{ \bgamma^\top \bW^{-1/2} \bB \bW^{-1/2} \bgamma}{\bgamma^\top \bI_p \bgamma} \right\} \\
&=& \max_{\bgamma \colon \bgamma^\top \bgamma =1} \left\{ \bgamma^\top \bW^{-1/2} \bB \bW^{-1/2} \bgamma \right\}
\end{eqnarray*}

This is similar to the PCA situation in \S 3.2 where we chose $\bu$ to be the eigenvector corresponding to the largest eigenvalue of $\bS$ to maximise $\bu^\top \bS \bu$.  Hence, we choose $\bgamma$ to be the eigenvector corresponding to the largest eigenvalue of $\bW^{-1/2} \bB \bW^{-1/2}$.

If $\bgamma$ is an eigenvector of $\bW^{-1/2} \bB \bW^{-1/2}$ then, by definition,
$$\bW^{-1/2} \bB \bW^{-1/2} \bgamma = \rho \bgamma$$
 where $\rho$ is the corresponding eigenvalue.  Pre-multiplying both sides by $\bW^{-1/2}$ gives
\begin{eqnarray*}
\bW^{-1} \bB (\bW^{-1/2} \bgamma) &=& \rho \bW^{-1/2} \bgamma \\
\bW^{-1} \bB \ba &=& \rho \ba.
\end{eqnarray*}

So, the $\ba$ we require is the unit eigenvector corresponding to the largest
eigenvalue of $\bW^{-1} \bB$. -->
<p><br> </br></p>
</div>
<div id="fishers-discriminant-rule" class="section level4 unnumbered hasAnchor">
<h4>Fisher’s discriminant rule<a href="8.3-FLDA.html#fishers-discriminant-rule" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>The function <span class="math inline">\(L(\mathbf x)=\mathbf a^\top \mathbf x\)</span> is called Fisher’s linear discriminant function. Once <span class="math inline">\(L(\mathbf x)\)</span> has been obtained, we allocate <span class="math inline">\(\mathbf x\)</span> to the population <span class="math inline">\(\Pi_k\)</span> whose discriminant score <span class="math inline">\(L(\hat{{\boldsymbol{\mu}}}_k)\)</span> is closest to <span class="math inline">\(L(\mathbf x)\)</span>, that is, we use the discriminant rule <span class="math display">\[ d^{Fisher}(\mathbf x) = \arg \min_k |L(\mathbf x) - L({\boldsymbol{\mu}}_k)| = \arg \min_k | \mathbf a^\top \mathbf x- \mathbf a^\top \hat{{\boldsymbol{\mu}}}_k |. \]</span></p>
<p>This is of the same form as saw in the earlier sections. If we let <span class="math inline">\(\delta_k(\mathbf x)= -|\mathbf a^\top (\mathbf x-\hat{{\boldsymbol{\mu}}}_k)|\)</span> then we see that
<span class="math inline">\(d^{Fisher}(\mathbf x) = \arg \max_k \delta_k(\mathbf x)\)</span> as before.</p>
<p>If there are only two populations (and suppose <span class="math inline">\(L({\boldsymbol{\mu}}_1)&gt; L({\boldsymbol{\mu}}_2)\)</span>), this is equivalent to classifying to population 1 if <span class="math inline">\(L(\mathbf x)&gt;t\)</span> and to population 2 otherwise, where <span class="math inline">\(t = \frac{1}{2}(L({\boldsymbol{\mu}}_1)+L({\boldsymbol{\mu}}_2))\)</span>.</p>
<p>This is visualized in Figure <a href="8.3-FLDA.html#fig:flda">8.6</a> for the iris data. The black points are the setosa data, and the green the viriginica. The diagonal black line is in the direction <span class="math inline">\(\mathbf a\)</span>. Along that line we have plotted the projection of the data points onto the line (<span class="math inline">\(\mathbf x^\top \mathbf a\)</span>), with the mean of each population and their projections marked with <span class="math inline">\(+\)</span>. The red line is perpendicular to <span class="math inline">\(\mathbf a\)</span>, and joins the midpoint of the two population means, <span class="math inline">\(\mathbf h= \frac{\hat{{\boldsymbol{\mu}}}_s+\hat{{\boldsymbol{\mu}}}_v}{2}\)</span>, with the projection of that point onto <span class="math inline">\(\mathbf a\)</span>. The red diamond marks the decision boundary for the projected points, i.e., if the point is to the left of this we classify as setosa, otherwise viriginica. It is half way between the projection of the two population means.</p>
<p>In Figure <a href="8.3-FLDA.html#fig:flda">8.6</a> <span class="math inline">\(\mathbf a\)</span> is chosen to be Fisher’s optimal vector (the first eigenvector of <span class="math inline">\(\mathbf W^{-1}\mathbf B\)</span>). This is the projection that is optimal for maximizing the ratio of the between-group to within-group variance, i.e., it optimally separates the two populations in the projection. In Figure <a href="8.3-FLDA.html#fig:fldapca">8.7</a> <span class="math inline">\(\mathbf a\)</span> is instead chosen to be the first principal component, i.e., the first eigenvector of the covariance matrix. This is the projection that maximizes the variance of the projected points (as done in PCA). Note that this is different to the LDA projection, and does not separate the two populations as cleanly as the LDA projection did.</p>
<div class="figure"><span style="display:block;" id="fig:flda"></span>
<img src="09-lda_files/figure-html/flda-1.png" alt="Visualization of Fishers discriminant analysis." width="960" />
<p class="caption">
Figure 8.6: Visualization of Fishers discriminant analysis.
</p>
</div>
<div class="figure"><span style="display:block;" id="fig:fldapca"></span>
<img src="09-lda_files/figure-html/fldapca-1.png" alt="Projection onto the first principal component." width="960" />
<p class="caption">
Figure 8.7: Projection onto the first principal component.
</p>
</div>
</div>
<div id="multiple-projections" class="section level4 unnumbered hasAnchor">
<h4>Multiple projections<a href="8.3-FLDA.html#multiple-projections" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>To summarize what we’ve found so far, we have seen that the vector <span class="math inline">\(\mathbf a\)</span> which maximizes
<span class="math display">\[\frac{\mathbf a^\top \mathbf B\mathbf a}{\mathbf a^\top \mathbf W\mathbf a} \]</span>
is the first eigenvector of <span class="math inline">\(\mathbf W^{-1}\mathbf B\)</span>. As we did with PCA and CCA, we can continue and find a second, third, etc projection of the data. We’re not going to go through the details of the derivation, but we can consider the projection matrix
<span class="math display">\[\mathbf A= \begin{pmatrix}|&amp; \ldots &amp;|\\
\mathbf a_1 &amp; \ldots &amp; \mathbf a_r\\
|&amp; \ldots &amp;| \end{pmatrix}\]</span>
which has columns equal to the first <span class="math inline">\(r\)</span> eigenvectors of <span class="math inline">\(\mathbf W^{-1}\mathbf B\)</span>.</p>
<p>Recall that <span class="math display">\[\mathbf B= \frac{1}{n}\sum_{j=1}^g n_j (\hat{{\boldsymbol{\mu}}}_j - \bar{\mathbf x}) (\hat{{\boldsymbol{\mu}}}_j - \bar{\mathbf x})^\top\]</span> is the variance of the <span class="math inline">\(g\)</span> population means. The points <span class="math inline">\(\hat{{\boldsymbol{\mu}}}_j - \bar{\mathbf x}\)</span> lie in a <span class="math inline">\(g-1\)</span> dimensional subspace of <span class="math inline">\(\mathbb{R}^p\)</span> (they must sum to <span class="math inline">\({\boldsymbol 0}\)</span>), and so <span class="math inline">\(\mathbf B\)</span> and also <span class="math inline">\(\mathbf W^{-1}\mathbf B\)</span> has rank at most <span class="math inline">\(\min(g-1,p)\)</span>. Thus we can find at most <span class="math inline">\(\min(g-1,p)\)</span> projections of the data for maximizing the separation between the classes.</p>
<p>To classify points using <span class="math inline">\(r\)</span> different projections, we use the vector discriminant function</p>
<p><span class="math display">\[\mathbf L(\mathbf x) = \mathbf A^\top \mathbf x\]</span>
and use the discriminant rule
<span class="math display">\[d^{Fisher}(\mathbf x) = \arg \min_k ||\mathbf L(\mathbf x) - \mathbf L({\boldsymbol{\mu}}_k)||_2,\]</span>
i.e., we compute the <span class="math inline">\(r\)</span> dimensional projection of the data and then find the which (projected) population mean it is closest to.</p>
<p>Note the dimension reduction here. We have gone from an observation <span class="math inline">\(\mathbf x\in \mathcal{R}^p\)</span> to a projected point <span class="math inline">\(\mathbf A^\top\mathbf x\in \mathcal{F}^t\)</span>. We are free to choose <span class="math inline">\(r\)</span>, which can result in useful ways of visualizing the data.</p>
</div>
<div id="iris-example-continued-1" class="section level3 hasAnchor" number="8.3.1">
<h3><span class="header-section-number">8.3.1</span> Iris example continued<a href="8.3-FLDA.html#iris-example-continued-1" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Let’s continue the iris example we had before, using the full dataset of 150 observations on 3 species, with 4 measurements on each flower.</p>
<div class="sourceCode" id="cb250"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb250-1"><a href="8.3-FLDA.html#cb250-1" tabindex="-1"></a><span class="fu">library</span>(vcvComp)</span>
<span id="cb250-2"><a href="8.3-FLDA.html#cb250-2" tabindex="-1"></a>B<span class="ot">=</span><span class="fu">cov.B</span>(iris[,<span class="dv">1</span><span class="sc">:</span><span class="dv">4</span>], iris[,<span class="dv">5</span>])</span>
<span id="cb250-3"><a href="8.3-FLDA.html#cb250-3" tabindex="-1"></a>W<span class="ot">=</span><span class="fu">cov.W</span>(iris[,<span class="dv">1</span><span class="sc">:</span><span class="dv">4</span>], iris[,<span class="dv">5</span>]) </span>
<span id="cb250-4"><a href="8.3-FLDA.html#cb250-4" tabindex="-1"></a>iris.eig <span class="ot">&lt;-</span> <span class="fu">eigen</span>(<span class="fu">solve</span>(W)<span class="sc">%*%</span> B)</span>
<span id="cb250-5"><a href="8.3-FLDA.html#cb250-5" tabindex="-1"></a>iris.eig<span class="sc">$</span>values</span></code></pre></div>
<pre><code>## [1]  4.732214e+01  4.195248e-01  7.749285e-15 -4.752967e-15</code></pre>
<p>We can see that <span class="math inline">\(\mathbf W^{-1}\mathbf B\)</span> only has two positive eigenvalues, and so is rank 2. We expected this, as we said the rank of <span class="math inline">\(\mathbf W^{-1}\mathbf B\)</span> must be less than <span class="math inline">\(\min(g-1,p)= \min(2,4)=2\)</span>. We can plot these projections and look at the separation achieved.</p>
<div class="sourceCode" id="cb252"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb252-1"><a href="8.3-FLDA.html#cb252-1" tabindex="-1"></a>V <span class="ot">&lt;-</span> iris.eig<span class="sc">$</span>vectors[,<span class="dv">1</span><span class="sc">:</span><span class="dv">2</span>]</span>
<span id="cb252-2"><a href="8.3-FLDA.html#cb252-2" tabindex="-1"></a>Z <span class="ot">&lt;-</span> <span class="fu">as.matrix</span>(iris[,<span class="dv">1</span><span class="sc">:</span><span class="dv">4</span>])<span class="sc">%*%</span> V<span class="sc">|&gt;</span><span class="fu">data.frame</span>()<span class="sc">|&gt;</span> <span class="fu">mutate</span>()</span>
<span id="cb252-3"><a href="8.3-FLDA.html#cb252-3" tabindex="-1"></a></span>
<span id="cb252-4"><a href="8.3-FLDA.html#cb252-4" tabindex="-1"></a>Z <span class="sc">|&gt;</span> <span class="fu">ggplot</span>(<span class="fu">aes</span>(<span class="at">x=</span>X1, <span class="at">y=</span>X2, <span class="at">colour=</span>iris<span class="sc">$</span>Species))<span class="sc">+</span><span class="fu">geom_point</span>()<span class="sc">+</span><span class="fu">xlab</span>(<span class="st">&#39;LD1&#39;</span>)<span class="sc">+</span><span class="fu">ylab</span>(<span class="st">&#39;LD2&#39;</span>)</span></code></pre></div>
<div class="figure"><span style="display:block;" id="fig:ldairis2d"></span>
<img src="09-lda_files/figure-html/ldairis2d-1.png" alt="2-dimensional LDA projection of the iris data" width="672" />
<p class="caption">
Figure 8.8: 2-dimensional LDA projection of the iris data
</p>
</div>
<p>We can see that the first projected coordinate is doing nearly all of the useful work in separating the three populations. We can see this in the eigenvalues (c.f. the scree plots used in PCA) which can be used to understand the importance of each projected coordinate in separating the populations (this is the <code>Proportion of trace</code> reported in the <code>lda</code> output).</p>
<div class="sourceCode" id="cb253"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb253-1"><a href="8.3-FLDA.html#cb253-1" tabindex="-1"></a>iris.eig<span class="sc">$</span>values[<span class="dv">1</span><span class="sc">:</span><span class="dv">2</span>]<span class="sc">/</span><span class="fu">sum</span>(iris.eig<span class="sc">$</span>values)</span></code></pre></div>
<pre><code>## [1] 0.991212605 0.008787395</code></pre>
<p>R will automatically plot this projection for us:</p>
<div class="sourceCode" id="cb255"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb255-1"><a href="8.3-FLDA.html#cb255-1" tabindex="-1"></a><span class="fu">par</span>(<span class="at">pty=</span><span class="st">&quot;s&quot;</span>)</span>
<span id="cb255-2"><a href="8.3-FLDA.html#cb255-2" tabindex="-1"></a>iris.lda <span class="ot">&lt;-</span> <span class="fu">lda</span>(Species <span class="sc">~</span>., iris)</span>
<span id="cb255-3"><a href="8.3-FLDA.html#cb255-3" tabindex="-1"></a><span class="fu">plot</span>(iris.lda,<span class="at">col=</span><span class="fu">as.integer</span>(iris<span class="sc">$</span>Species)<span class="sc">+</span><span class="dv">1</span>, <span class="at">abbrev=</span><span class="dv">1</span>)</span></code></pre></div>
<div class="figure"><span style="display:block;" id="fig:ldairis2db"></span>
<img src="09-lda_files/figure-html/ldairis2db-1.png" alt="2-dimensional LDA projection of the iris data using the built-in R plotting function" width="672" />
<p class="caption">
Figure 8.9: 2-dimensional LDA projection of the iris data using the built-in R plotting function
</p>
</div>
<p>Although this looks different to Figure <a href="8.3-FLDA.html#fig:ldairis2d">8.8</a>, the projection is essentially the same it has just flipped the <span class="math inline">\(y\)</span> axis.</p>
<!--```{r}
qplot(Z[,1], -Z[,2], colour=iris$Species, main='LDA for the iris data', ylim=c(0,4))
```
-->
<p>Using the option <code>dimen = 1</code> will plot histograms of the first LDA projection for each group, which all we need in this example to separate the populations.</p>
<div class="sourceCode" id="cb256"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb256-1"><a href="8.3-FLDA.html#cb256-1" tabindex="-1"></a><span class="fu">plot</span>(lda.iris, <span class="at">dimen =</span> <span class="dv">1</span>, <span class="at">type =</span> <span class="st">&quot;b&quot;</span>)</span></code></pre></div>
<p><img src="09-lda_files/figure-html/unnamed-chunk-22-1.png" width="672" /></p>
<div id="comparison-with-pca" class="section level5 unnumbered hasAnchor">
<h5>Comparison with PCA<a href="8.3-FLDA.html#comparison-with-pca" class="anchor-section" aria-label="Anchor link to header"></a></h5>
<p>We can compare the LDA projection to the 2-dimensional projection found by PCA:</p>
<div class="sourceCode" id="cb257"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb257-1"><a href="8.3-FLDA.html#cb257-1" tabindex="-1"></a>iris.pca <span class="ot">=</span> <span class="fu">prcomp</span>(iris[,<span class="dv">1</span><span class="sc">:</span><span class="dv">4</span>])</span>
<span id="cb257-2"><a href="8.3-FLDA.html#cb257-2" tabindex="-1"></a></span>
<span id="cb257-3"><a href="8.3-FLDA.html#cb257-3" tabindex="-1"></a>iris<span class="sc">$</span>PC1<span class="ot">=</span>iris.pca<span class="sc">$</span>x[,<span class="dv">1</span>]</span>
<span id="cb257-4"><a href="8.3-FLDA.html#cb257-4" tabindex="-1"></a>iris<span class="sc">$</span>PC2<span class="ot">=</span>iris.pca<span class="sc">$</span>x[,<span class="dv">2</span>]</span>
<span id="cb257-5"><a href="8.3-FLDA.html#cb257-5" tabindex="-1"></a><span class="fu">ggplot</span>(<span class="fu">aes</span>(<span class="at">x=</span>PC1, <span class="at">y=</span>PC2, <span class="at">colour=</span>Species), <span class="at">data=</span>iris)<span class="sc">+</span><span class="fu">geom_point</span>()</span></code></pre></div>
<div class="figure"><span style="display:block;" id="fig:ldapcairis"></span>
<img src="09-lda_files/figure-html/ldapcairis-1.png" alt="2-dimensional PCA projection of the iris data" width="672" />
<p class="caption">
Figure 8.10: 2-dimensional PCA projection of the iris data
</p>
</div>
<p>PCA hasn’t separated the 3 populations as cleanly as LDA did, but that is not a surprise as that is not its aim (it aim is to maximize the variance). PCA doesn’t use the population labels, and so there is no reason why it should separate the populations. But this does illustrate the importance of choosing the right multivariate statistical methods.</p>
<!--
We can also plot the classification regions

```r
library(pracma)
xplt <- meshgrid(seq(-3,2,0.02), seq(-3,0,0.02))
Xplt <- cbind(c(xplt$X), c(xplt$Y))
Yplt <- apply(Xplt, 1, function(x){
  which.min(as.matrix(dist(rbind(MV, x), upper=TRUE, diag=TRUE))[4,1:3])
})
plot(Z[,1], Z[,2], col=iris$Species)
points(MV[,1], MV[,2], pch=3,cex=2, col=1:3)

points(Xplt[,1], Xplt[,2], col=Yplt, cex=0.1)
```
-->
<!--



## Probability of misclassification

NOT INCLUDING THIS AS MORE MODERN TREATMENT WOULD ESTIMATE CLASSIFICATION ERROS USING Cross validation


Let $p_{jk}$ denote the probability of allocating an observation to population $\Pi_j$, when in fact it comes from $\Pi_k$.  Therefore $p_{kk}$ is the probability of correctly classifying this observation and $1-p_{kk}$ is the probability of misclassification.

One way of estimating $p_{jk}$ is to consider the number of observations from the training data that are misclassified.  For example, if $n_k$ observations come from population $k$ and $n_{jk}$ is the number of observations from population $k$ classified as from population $j$, then
$$ \hat{p}_{jk} = \frac{n_{jk}}{n_k} $$
is an estimate of $p_{jk}$.

When $g=2$, $\Pi_j$ is $N_p(\bmu_j, \bSigma)$ and we use the ML rule, we obtain an explicit expression for $p_{12}$ and $p_{21}$ as follows.

Recall that we allocate $\bz$ to $\Pi_1$ if and only if  $U = \ba^\top (\bz-\bh)>0$ where $\ba = \bSigma^{-1} (\bmu_1 - \bmu_2)$ and $\bh = \frac{1}{2} (\bmu_1 + \bmu_2)$.

Suppose $\bz$ is from $\Pi_2$. Then $\bz \sim N_p(\bmu_2,\bSigma)$, so
\begin{eqnarray*}
E[U] &=& E[\ba^\top (\bz-\bh)] = \ba^\top (E[\bz] -\bh) = \ba^\top (\bmu_2-\bh); \\
\text{Var}(U) &=& \text{Var}(\ba^\top \bz - \ba^\top \bh) = \text{Var}(\ba^\top \bz) = \ba^\top \bSigma \ba.
\end{eqnarray*}
Hence, when $\bz$ is from $\Pi_2$, $U \sim N(\ba^\top (\bmu_2 - \bh), \ba^\top \bSigma \ba)$.

Define $\Delta^2 = (\bmu_1-\bmu_2)^\top \bSigma^{-1} (\bmu_1-\bmu_2)$, Then
\begin{eqnarray*}
\ba^\top (\bmu_2-\bh) &=& \ba^\top \lb \bmu_2 - \frac{1}{2}\bmu_1 - \frac{1}{2}\bmu_2 \rb = \frac{1}{2} \ba^\top (\bmu_2 - \bmu_1) \\
&=& \frac{1}{2} (\bmu_1 - \bmu_2)^\top \bSigma^{-1} (\bmu_2 - \bmu_1) \\
&=& -\frac{1}{2} (\bmu_1 - \bmu_2)^\top \bSigma^{-1} (\bmu_1 - \bmu_2) = -\frac{1}{2}\Delta^2.
\end{eqnarray*}
and
$$ \ba^\top \bSigma \ba = (\bmu_1 - \bmu_2)^\top \bSigma^{-1} \bSigma \bSigma^{-1} (\bmu_1 - \bmu_2) = \Delta ^2. $$
Hence  $U \sim N( -\frac{1}{2}\Delta^2, \Delta^2)$ when $\bz$ is from $\Pi_2$.


Now $\bz$ is allocated to $\Pi_1$ if $U>0$.  The probability of this event when $\bz$ is in fact from $\Pi_2$ is
\begin{eqnarray*}
p_{12} = P(U>0) &=& P \lb \frac{ U - (-\Delta^2/2) }{\Delta} > \frac{0 - (-\Delta^2/2) }{\Delta} \rb \\
&=& P \lb Z > \frac{\Delta^2}{2\Delta} = \frac{\Delta}{2}  \bigg| Z \sim N(0,1) \rb \\
&=& P \lb Z < -\frac{\Delta}{2} \rb
\end{eqnarray*}
which can be found from statistical tables.  A similar argument shows that $p_{21}=p_{12}$.

If we are using the sample ML rule then we can replace $\Delta^2$ with
$$D^2 = (\bar{\bx}_1 - \bar{\bx}_2)^\top \widehat{\bSigma}^{-1} (\bar{\bx}_1 - \bar{\bx}_2)$$
and $\hat{p}_{12} = P(Z<-D/2)$.

-->
</div>
</div>
<div id="links-between-methods" class="section level3 hasAnchor" number="8.3.2">
<h3><span class="header-section-number">8.3.2</span> Links between methods<a href="8.3-FLDA.html#links-between-methods" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>When <span class="math inline">\(g=2\)</span>, Fisher’s rule and the sample ML rule with <span class="math inline">\(\boldsymbol{\Sigma}_1=\boldsymbol{\Sigma}_2=\boldsymbol{\Sigma}\)</span> turn out to be the same. Note that in
the sample ML rule we assumed that the two groups are from <span class="math inline">\(N_p({\boldsymbol{\mu}}_i, \boldsymbol{\Sigma})\)</span>
populations, but Fisher’s rule makes no such assumption.</p>
<div class="proposition">
<p><span id="prp:nine3" class="proposition"><strong>Proposition 8.5  </strong></span>If <span class="math inline">\(g=2\)</span> then Fisher’s rule and the sample ML rule described in Section <a href="8.1-lda-ML.html#sample-lda">8.1.2</a> are equivalent.</p>
</div>
<div class="proof">
<p><span id="unlabeled-div-46" class="proof"><em>Proof</em>. </span>Beyond the scope of the module.</p>
</div>
<p><br></br></p>
<!--
::: {.proof}
First, note that
\begin{eqnarray*}
\hat{\bmu}_1 - \bar{\bx} &=& \hat{\bmu}_1 - \lb \frac{n_1 \hat{\bmu}_1 + n_2 \hat{\bmu}_2}{n_1+n_2} \rb \\
&=& \frac{ n_2 (\hat{\bmu}_1 - \hat{\bmu}_2) }{n_1 + n_2} = \frac{n_2 \bd}{n_1+n_2}
\end{eqnarray*}
where $\bd = \hat{\bmu}_1 - \hat{\bmu}_2$.  Similarly $\hat{\bmu}_2 - \bar{\bx} = -\frac{n_1 \bd}{n_1+n_2}$.  Therefore,
\begin{eqnarray*}
\bB &=& n_1 (\hat{\bmu}_1 - \bar{\bx})(\hat{\bmu}_1 - \bar{\bx})^\top + n_2 (\hat{\bmu}_2 - \bar{\bx})(\hat{\bmu}_2 - \bar{\bx})^\top \\
&=& \frac{n_1 n_2^2}{(n_1+n_2)^2} \bd\bd^\top + \frac{n_2 n_1^2}{(n_1+n_2)^2} (-\bd)(-\bd)^\top \\
&=&  \frac{n_1 n_2}{n_1+n_2} \bd\bd^\top.
\end{eqnarray*}
Let $c = \frac{n_1 n_2}{n_1+n_2}$.  

In Fisher's discriminant analysis we choose $\ba$ to be an eigenvector of $$\bW^{-1} \bB = c \bW^{-1} \bd \bd^\top.$$
Note that the non-zero eigenvalues of $c \bW^{-1} \bd \bd^\top$ are the same as the non-zero eigenvalues of $c \bd^\top \bW^{-1} \bd$, which is scalar and so itself is the only non-zero eigenvalue.  The eigenvector, $\ba$, must then satisfy
$$ c \bW^{-1} \bd \bd^\top \ba = c \bd^\top \bW^{-1} \bd \ba. $$
If we choose $\ba = \bW^{-1} \bd$ then the equation is satisfied.  Hence $\ba = \bW^{-1} (\hat{\bmu}_1 - \hat{\bmu}_2)$.

Let $r = \ba^\top \bz$, $s = \ba^\top \hat{\bmu}_1$ and $t = \ba^\top \hat{\bmu}_2$, then Fisher's rule allocates $\bz$ to $\Pi_1$ if and only if
\begin{eqnarray*}
&&| r-s | < | r-t | \\
&\iff & (r-s)^2 < (r-t)^2 \\
&\iff & r^2 - 2rs + s^2 < r^2 - 2rt + t^2 \\
&\iff & 0 < 2r(s-t) + t^2 - s^2 \\
& \iff & 0 < 2r(s-t) + (t-s)(t+s) \\
& \iff & 0 < (s-t)(2r-t-s)
\end{eqnarray*}
Now $s-t = \ba^\top (\hat{\bmu}_1 - \hat{\bmu}_2) = \bd^\top \bW^{-1} \bd$ which is a quadratic form and must therefore be positive, because $\bW$ is assumed to be positive definite.  Hence Fisher's rule allocates $\bz$ to $\Pi_1$ if
\begin{eqnarray*}
&& (2r-s-t) > 0\\
&\iff & r - \frac{1}{2}(s+t) > 0 \\
&\iff & \ba^\top \lb \bz - \frac{1}{2}(\hat{\bmu}_1 + \hat{\bmu}_2) \rb > 0 \\
&\iff & (\hat{\bmu}_1 - \hat{\bmu}_2)^\top \bW^{-1} \lb \bz - \frac{1}{2}(\hat{\bmu}_1 + \hat{\bmu}_2) \rb > 0 \\
&\iff & (\hat{\bmu}_1 - \hat{\bmu}_2)^\top \widehat{\bSigma}^{-1} \lb \bz - \frac{1}{2}(\hat{\bmu}_1 + \hat{\bmu}_2) \rb > 0
\end{eqnarray*}
where the last line follows since $\bW = (n_1 + n_2 - 2)\widehat{\bSigma}$.  This is equivalent to the sample ML rule for $g=2$.
:::
-->
<p>For <span class="math inline">\(g &gt; 2\)</span>, the sample ML rule and Fisher’s linear rule will not, in general, be the same. Fisher’s rule is linear when <span class="math inline">\(g&gt;2\)</span> and is easier to implement than ML rules when there are several populations. It is often reasonable to use Fisher’s rule for non-normal populations. In particular, Fisher’s rule requires fewer assumptions than ML rules. However, the ML rule is ‘optimal’ in some sense when its assumptions are valid.</p>
<p>There is also a link between LDA and linear regression when there are only two groups. If we let</p>
<p><span class="math display">\[
y_i = \begin{cases} 1 &amp;\mbox{ if }i \mbox{ is from } \Pi_1\\
-1 &amp;\mbox{ otherwise}
\end{cases}
\]</span></p>
<p>then if we fit the model
<span class="math display">\[\mathbf y= \mathbf X\boldsymbol \beta+ \mathbf e\]</span>
we find <span class="math inline">\(\boldsymbol \beta\)</span> is in the same direction as the first eigenvector of <span class="math inline">\(\mathbf W^{-1}\mathbf B\)</span>.</p>
<div class="sourceCode" id="cb258"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb258-1"><a href="8.3-FLDA.html#cb258-1" tabindex="-1"></a><span class="fu">library</span>(dplyr)</span>
<span id="cb258-2"><a href="8.3-FLDA.html#cb258-2" tabindex="-1"></a>iris3 <span class="ot">&lt;-</span> iris <span class="sc">%&gt;%</span> <span class="fu">filter</span>(Species <span class="sc">!=</span> <span class="st">&quot;versicolor&quot;</span>) <span class="sc">%&gt;%</span></span>
<span id="cb258-3"><a href="8.3-FLDA.html#cb258-3" tabindex="-1"></a>  dplyr<span class="sc">::</span><span class="fu">select</span>(Sepal.Length, Sepal.Width, Species)</span>
<span id="cb258-4"><a href="8.3-FLDA.html#cb258-4" tabindex="-1"></a>iris3<span class="sc">$</span>pm1 <span class="ot">&lt;-</span> <span class="fu">as.integer</span>(iris3<span class="sc">$</span>Species)<span class="sc">-</span><span class="dv">2</span> </span>
<span id="cb258-5"><a href="8.3-FLDA.html#cb258-5" tabindex="-1"></a><span class="co"># convert to +1 and -1</span></span>
<span id="cb258-6"><a href="8.3-FLDA.html#cb258-6" tabindex="-1"></a>iris3.lm <span class="ot">&lt;-</span> <span class="fu">lm</span>(pm1<span class="sc">~</span>Sepal.Width<span class="sc">+</span>Sepal.Length, iris3)</span>
<span id="cb258-7"><a href="8.3-FLDA.html#cb258-7" tabindex="-1"></a><span class="fu">coef</span>(iris3.lm)[<span class="dv">2</span>]<span class="sc">/</span><span class="fu">coef</span>(iris3.lm)[<span class="dv">3</span>]</span></code></pre></div>
<pre><code>## Sepal.Width 
##   -1.137257</code></pre>
<div class="sourceCode" id="cb260"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb260-1"><a href="8.3-FLDA.html#cb260-1" tabindex="-1"></a>iris.lda <span class="ot">&lt;-</span> <span class="fu">lda</span>(Species<span class="sc">~</span>Sepal.Width<span class="sc">+</span>Sepal.Length, iris3)</span>
<span id="cb260-2"><a href="8.3-FLDA.html#cb260-2" tabindex="-1"></a><span class="fu">coef</span>(iris.lda)[<span class="dv">1</span>]<span class="sc">/</span><span class="fu">coef</span>(iris.lda)[<span class="dv">2</span>]</span></code></pre></div>
<pre><code>## [1] -1.137257</code></pre>
<!-- also a link to CCA 
https://sites.stat.washington.edu/wxs/Stat592-w2011/Slides/cancorr-notes.pdf

-->
<!--## MNIST

Try penalized LDA?
https://cran.r-project.org/web/packages/penalizedLDA/penalizedLDA.pdf
-->
<!--##### Example: Student marks

Consider the G11PRB and G11STA module marks for $n_1 = 98$ students on G100 and $n_2 = 46$ students on G103.  The sample means and variances for each group are given by
\begin{eqnarray*}
\bar{\bx}_1 = \begin{pmatrix} 60.582 \\ 62.786 \end{pmatrix} &\qquad& \bar{\bx}_2 = \begin{pmatrix} 64.761 \\ 60.457 \end{pmatrix} \\
\bS_1 = \begin{pmatrix} 201.04 & 129.56 \\ 129.56 & 316.21 \end{pmatrix} &\qquad& \bS_2 = \begin{pmatrix} 229.88 & 177.02 \\ 177.02 & 354.16 \end{pmatrix}
\end{eqnarray*}
Hence,
\begin{eqnarray*}
\bS_u &=& \frac{1}{98+46-2} \lb 98 \bS_1 + 46 \bS_2 \rb = \begin{pmatrix} 213.21 & 146.76 \\ 146.76 & 332.96 \end{pmatrix}, \\
\bar{\bx}_1 - \bar{\bx}_2 &=& \begin{pmatrix} -4.179 \\ 2.329 \end{pmatrix}, \\
\hat{\bh} &=& \frac{1}{2} (\bar{\bx}_1 + \bar{\bx}_2) = \begin{pmatrix} 62.671 \\ 61.621 \end{pmatrix},
\end{eqnarray*}
and
$$\hat{\ba} = \bS_u^{-1} (\bar{\bx}_1 - \bar{\bx}_2) = \begin{pmatrix} 0.0067 & -0.0030 \\ -0.0030 & 0.0043 \end{pmatrix} \begin{pmatrix} -4.179 \\ 2.329 \end{pmatrix} = \begin{pmatrix} -0.035 \\ 0.022 \end{pmatrix}.$$

The sample ML discriminant rule allocates a new observation\\
 $\bx = (z_1, z_2)^\top$ to $\Pi_1$ if and only if
$$ \hat{\ba}^\top (\bx - \hat{\bh}) = \begin{pmatrix} -0.035 & 0.022 \end{pmatrix} \begin{pmatrix} z_1 - 62.671 \\ z_2 - 61.621 \end{pmatrix} > 0.$$

For example, if a student on this year's course scores 80 on G11PRB and 60 on G11STA then
$$ \hat{\ba}^\top (\bx - \hat{\bh}) = \begin{pmatrix} -0.035 & 0.022 \end{pmatrix} \begin{pmatrix} 80 - 62.671 \\ 60 - 61.621 \end{pmatrix} = -0.644 < 0,$$
and so we would allocate this student to G103.  The boundary, where $\hat{\ba}^\top (\bx - \hat{\bh}) = 0$, is shown below.

FIX
<!--
\begin{center}
\includegraphics[width=12cm,angle=0]{sample_mldr1.pdf}
\end{center}
-->
<!--Note that the boundary line passes half-way between the two sample means.  In this example it is difficult to discriminate accurately between G100 and G103 because there is a large overlap between the two populations.

We could extend the example to include, say, students on GL11.  Here the boundary between the three populations is piece-wise linear and they meet at a common point.

FIX-->
<!--
\begin{center}
\includegraphics[width=12cm,angle=0]{sample_mldr2.pdf}
\end{center}
-->

</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="8.2-lda-Bayes.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="8.4-computer-tasks-4.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["MultivariateStatistics.pdf", "MultivariateStatistics.epub"],
"search": {
"engine": "fuse",
"options": null
},
"toc": {
"collapse": "subsection",
"download": "pdf",
"toc_float": {
"collapsed": true,
"smooth_scroll": false
}
},
"pandoc_args": "--top-level-division=[chapter]"
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/latest.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
