<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>4.3 An alternative view of PCA | Multivariate Statistics</title>
  <meta name="description" content="The lecture notes for MATH3030/4068: Multivariate Analysis / Applied Multivariate Statistics" />
  <meta name="generator" content="bookdown 0.21 and GitBook 2.6.7" />

  <meta property="og:title" content="4.3 An alternative view of PCA | Multivariate Statistics" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="The lecture notes for MATH3030/4068: Multivariate Analysis / Applied Multivariate Statistics" />
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="4.3 An alternative view of PCA | Multivariate Statistics" />
  
  <meta name="twitter:description" content="The lecture notes for MATH3030/4068: Multivariate Analysis / Applied Multivariate Statistics" />
  

<meta name="author" content="Prof. Richard Wilkinson" />



  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="4-2-pca-a-formal-description-with-proofs.html"/>
<link rel="next" href="4-4-pca-comptask.html"/>
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />









<script src="libs/kePrint-0.0.1/kePrint.js"></script>
<link href="libs/lightable-0.0.1/lightable.css" rel="stylesheet" />


<style type="text/css">
a.sourceLine { display: inline-block; line-height: 1.25; }
a.sourceLine { pointer-events: none; color: inherit; text-decoration: inherit; }
a.sourceLine:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode { white-space: pre; position: relative; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
code.sourceCode { white-space: pre-wrap; }
a.sourceLine { text-indent: -1em; padding-left: 1em; }
}
pre.numberSource a.sourceLine
  { position: relative; left: -4em; }
pre.numberSource a.sourceLine::before
  { content: attr(data-line-number);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; pointer-events: all; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {  }
@media screen {
a.sourceLine::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Applied multivariate statistics</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Introduction</a></li>
<li class="chapter" data-level="" data-path="part-i-prerequisites.html"><a href="part-i-prerequisites.html"><i class="fa fa-check"></i>PART I: Prerequisites</a></li>
<li class="chapter" data-level="1" data-path="1-stat-prelim.html"><a href="1-stat-prelim.html"><i class="fa fa-check"></i><b>1</b> Statistical Preliminaries</a><ul>
<li class="chapter" data-level="1.1" data-path="1-1-notation.html"><a href="1-1-notation.html"><i class="fa fa-check"></i><b>1.1</b> Notation</a><ul>
<li class="chapter" data-level="1.1.1" data-path="1-1-notation.html"><a href="1-1-notation.html#example-datasets"><i class="fa fa-check"></i><b>1.1.1</b> Example datasets</a></li>
<li class="chapter" data-level="1.1.2" data-path="1-1-notation.html"><a href="1-1-notation.html#aims-of-multivariate-data-analysis"><i class="fa fa-check"></i><b>1.1.2</b> Aims of multivariate data analysis</a></li>
</ul></li>
<li class="chapter" data-level="1.2" data-path="1-2-exploratory-data-analysis-eda.html"><a href="1-2-exploratory-data-analysis-eda.html"><i class="fa fa-check"></i><b>1.2</b> Exploratory data analysis (EDA)</a><ul>
<li class="chapter" data-level="1.2.1" data-path="1-2-exploratory-data-analysis-eda.html"><a href="1-2-exploratory-data-analysis-eda.html#data-visualization"><i class="fa fa-check"></i><b>1.2.1</b> Data visualization</a></li>
<li class="chapter" data-level="1.2.2" data-path="1-2-exploratory-data-analysis-eda.html"><a href="1-2-exploratory-data-analysis-eda.html#summary-statistics"><i class="fa fa-check"></i><b>1.2.2</b> Summary statistics</a></li>
</ul></li>
<li class="chapter" data-level="1.3" data-path="1-3-randvec.html"><a href="1-3-randvec.html"><i class="fa fa-check"></i><b>1.3</b> Random vectors and matrices</a><ul>
<li class="chapter" data-level="1.3.1" data-path="1-3-randvec.html"><a href="1-3-randvec.html#estimators"><i class="fa fa-check"></i><b>1.3.1</b> Estimators</a></li>
</ul></li>
<li class="chapter" data-level="1.4" data-path="1-4-computer-tasks.html"><a href="1-4-computer-tasks.html"><i class="fa fa-check"></i><b>1.4</b> Computer tasks</a></li>
<li class="chapter" data-level="1.5" data-path="1-5-exercises.html"><a href="1-5-exercises.html"><i class="fa fa-check"></i><b>1.5</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="2-linalg-prelim.html"><a href="2-linalg-prelim.html"><i class="fa fa-check"></i><b>2</b> Review of linear algebra</a><ul>
<li class="chapter" data-level="2.1" data-path="2-1-linalg-basics.html"><a href="2-1-linalg-basics.html"><i class="fa fa-check"></i><b>2.1</b> Basics</a><ul>
<li class="chapter" data-level="2.1.1" data-path="2-1-linalg-basics.html"><a href="2-1-linalg-basics.html#notation-1"><i class="fa fa-check"></i><b>2.1.1</b> Notation</a></li>
<li class="chapter" data-level="2.1.2" data-path="2-1-linalg-basics.html"><a href="2-1-linalg-basics.html#elementary-matrix-operations"><i class="fa fa-check"></i><b>2.1.2</b> Elementary matrix operations</a></li>
<li class="chapter" data-level="2.1.3" data-path="2-1-linalg-basics.html"><a href="2-1-linalg-basics.html#special-matrices"><i class="fa fa-check"></i><b>2.1.3</b> Special matrices</a></li>
<li class="chapter" data-level="2.1.4" data-path="2-1-linalg-basics.html"><a href="2-1-linalg-basics.html#vectordiff"><i class="fa fa-check"></i><b>2.1.4</b> Vector Differentiation</a></li>
</ul></li>
<li class="chapter" data-level="2.2" data-path="2-2-linalg-vecspaces.html"><a href="2-2-linalg-vecspaces.html"><i class="fa fa-check"></i><b>2.2</b> Vector spaces</a><ul>
<li class="chapter" data-level="2.2.1" data-path="2-2-linalg-vecspaces.html"><a href="2-2-linalg-vecspaces.html#linear-independence"><i class="fa fa-check"></i><b>2.2.1</b> Linear independence</a></li>
<li class="chapter" data-level="2.2.2" data-path="2-2-linalg-vecspaces.html"><a href="2-2-linalg-vecspaces.html#colsspace"><i class="fa fa-check"></i><b>2.2.2</b> Row and column spaces</a></li>
<li class="chapter" data-level="2.2.3" data-path="2-2-linalg-vecspaces.html"><a href="2-2-linalg-vecspaces.html#linear-transformations"><i class="fa fa-check"></i><b>2.2.3</b> Linear transformations</a></li>
</ul></li>
<li class="chapter" data-level="2.3" data-path="2-3-linalg-innerprod.html"><a href="2-3-linalg-innerprod.html"><i class="fa fa-check"></i><b>2.3</b> Inner product spaces</a><ul>
<li class="chapter" data-level="2.3.1" data-path="2-3-linalg-innerprod.html"><a href="2-3-linalg-innerprod.html#normed"><i class="fa fa-check"></i><b>2.3.1</b> Distances, and angles</a></li>
<li class="chapter" data-level="2.3.2" data-path="2-3-linalg-innerprod.html"><a href="2-3-linalg-innerprod.html#orthogonal-matrices"><i class="fa fa-check"></i><b>2.3.2</b> Orthogonal matrices</a></li>
<li class="chapter" data-level="2.3.3" data-path="2-3-linalg-innerprod.html"><a href="2-3-linalg-innerprod.html#projection-matrix"><i class="fa fa-check"></i><b>2.3.3</b> Projections</a></li>
</ul></li>
<li class="chapter" data-level="2.4" data-path="2-4-centering-matrix.html"><a href="2-4-centering-matrix.html"><i class="fa fa-check"></i><b>2.4</b> The Centering Matrix</a></li>
<li class="chapter" data-level="2.5" data-path="2-5-tasks-ch2.html"><a href="2-5-tasks-ch2.html"><i class="fa fa-check"></i><b>2.5</b> Computer tasks</a></li>
<li class="chapter" data-level="2.6" data-path="2-6-exercises-ch2.html"><a href="2-6-exercises-ch2.html"><i class="fa fa-check"></i><b>2.6</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="3-linalg-decomp.html"><a href="3-linalg-decomp.html"><i class="fa fa-check"></i><b>3</b> Matrix decompositions</a><ul>
<li class="chapter" data-level="3.1" data-path="3-1-matrix-matrix.html"><a href="3-1-matrix-matrix.html"><i class="fa fa-check"></i><b>3.1</b> Matrix-matrix products</a></li>
<li class="chapter" data-level="3.2" data-path="3-2-spectraleigen-decomposition.html"><a href="3-2-spectraleigen-decomposition.html"><i class="fa fa-check"></i><b>3.2</b> Spectral/eigen decomposition</a><ul>
<li class="chapter" data-level="3.2.1" data-path="3-2-spectraleigen-decomposition.html"><a href="3-2-spectraleigen-decomposition.html#eigenvalues-and-eigenvectors"><i class="fa fa-check"></i><b>3.2.1</b> Eigenvalues and eigenvectors</a></li>
<li class="chapter" data-level="3.2.2" data-path="3-2-spectraleigen-decomposition.html"><a href="3-2-spectraleigen-decomposition.html#spectral-decomposition"><i class="fa fa-check"></i><b>3.2.2</b> Spectral decomposition</a></li>
<li class="chapter" data-level="3.2.3" data-path="3-2-spectraleigen-decomposition.html"><a href="3-2-spectraleigen-decomposition.html#matrixroots"><i class="fa fa-check"></i><b>3.2.3</b> Matrix square roots</a></li>
</ul></li>
<li class="chapter" data-level="3.3" data-path="3-3-linalg-SVD.html"><a href="3-3-linalg-SVD.html"><i class="fa fa-check"></i><b>3.3</b> Singular Value Decomposition (SVD)</a><ul>
<li class="chapter" data-level="3.3.1" data-path="3-3-linalg-SVD.html"><a href="3-3-linalg-SVD.html#examples"><i class="fa fa-check"></i><b>3.3.1</b> Examples</a></li>
</ul></li>
<li class="chapter" data-level="3.4" data-path="3-4-svdopt.html"><a href="3-4-svdopt.html"><i class="fa fa-check"></i><b>3.4</b> SVD optimization results</a></li>
<li class="chapter" data-level="3.5" data-path="3-5-low-rank-approximation.html"><a href="3-5-low-rank-approximation.html"><i class="fa fa-check"></i><b>3.5</b> Low-rank approximation</a><ul>
<li class="chapter" data-level="3.5.1" data-path="3-5-low-rank-approximation.html"><a href="3-5-low-rank-approximation.html#matrix-norms"><i class="fa fa-check"></i><b>3.5.1</b> Matrix norms</a></li>
<li class="chapter" data-level="3.5.2" data-path="3-5-low-rank-approximation.html"><a href="3-5-low-rank-approximation.html#eckart-young-mirsky-theorem"><i class="fa fa-check"></i><b>3.5.2</b> Eckart-Young-Mirsky Theorem</a></li>
<li class="chapter" data-level="3.5.3" data-path="3-5-low-rank-approximation.html"><a href="3-5-low-rank-approximation.html#example-image-compression"><i class="fa fa-check"></i><b>3.5.3</b> Example: image compression</a></li>
</ul></li>
<li class="chapter" data-level="3.6" data-path="3-6-tasks-ch3.html"><a href="3-6-tasks-ch3.html"><i class="fa fa-check"></i><b>3.6</b> Computer tasks</a></li>
<li class="chapter" data-level="3.7" data-path="3-7-exercises-ch3.html"><a href="3-7-exercises-ch3.html"><i class="fa fa-check"></i><b>3.7</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="part-ii-dimension-reduction-methods.html"><a href="part-ii-dimension-reduction-methods.html"><i class="fa fa-check"></i>PART II: Dimension reduction methods</a><ul>
<li class="chapter" data-level="" data-path="part-ii-dimension-reduction-methods.html"><a href="part-ii-dimension-reduction-methods.html#a-warning"><i class="fa fa-check"></i>A warning</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="4-pca.html"><a href="4-pca.html"><i class="fa fa-check"></i><b>4</b> Principal Component Analysis (PCA)</a><ul>
<li class="chapter" data-level="4.1" data-path="4-1-pca-an-informal-introduction.html"><a href="4-1-pca-an-informal-introduction.html"><i class="fa fa-check"></i><b>4.1</b> PCA: an informal introduction</a><ul>
<li class="chapter" data-level="4.1.1" data-path="4-1-pca-an-informal-introduction.html"><a href="4-1-pca-an-informal-introduction.html#notation-recap"><i class="fa fa-check"></i><b>4.1.1</b> Notation recap</a></li>
<li class="chapter" data-level="4.1.2" data-path="4-1-pca-an-informal-introduction.html"><a href="4-1-pca-an-informal-introduction.html#first-principal-component"><i class="fa fa-check"></i><b>4.1.2</b> First principal component</a></li>
<li class="chapter" data-level="4.1.3" data-path="4-1-pca-an-informal-introduction.html"><a href="4-1-pca-an-informal-introduction.html#second-principal-component"><i class="fa fa-check"></i><b>4.1.3</b> Second principal component</a></li>
<li class="chapter" data-level="4.1.4" data-path="4-1-pca-an-informal-introduction.html"><a href="4-1-pca-an-informal-introduction.html#geometric-interpretation-1"><i class="fa fa-check"></i><b>4.1.4</b> Geometric interpretation</a></li>
<li class="chapter" data-level="4.1.5" data-path="4-1-pca-an-informal-introduction.html"><a href="4-1-pca-an-informal-introduction.html#example"><i class="fa fa-check"></i><b>4.1.5</b> Example</a></li>
<li class="chapter" data-level="4.1.6" data-path="4-1-pca-an-informal-introduction.html"><a href="4-1-pca-an-informal-introduction.html#example-iris"><i class="fa fa-check"></i><b>4.1.6</b> Example: Iris</a></li>
</ul></li>
<li class="chapter" data-level="4.2" data-path="4-2-pca-a-formal-description-with-proofs.html"><a href="4-2-pca-a-formal-description-with-proofs.html"><i class="fa fa-check"></i><b>4.2</b> PCA: a formal description with proofs</a><ul>
<li class="chapter" data-level="4.2.1" data-path="4-2-pca-a-formal-description-with-proofs.html"><a href="4-2-pca-a-formal-description-with-proofs.html#properties-of-principal-components"><i class="fa fa-check"></i><b>4.2.1</b> Properties of principal components</a></li>
<li class="chapter" data-level="4.2.2" data-path="4-2-pca-a-formal-description-with-proofs.html"><a href="4-2-pca-a-formal-description-with-proofs.html#pca:football"><i class="fa fa-check"></i><b>4.2.2</b> Example: Football</a></li>
<li class="chapter" data-level="4.2.3" data-path="4-2-pca-a-formal-description-with-proofs.html"><a href="4-2-pca-a-formal-description-with-proofs.html#pcawithR"><i class="fa fa-check"></i><b>4.2.3</b> PCA based on <span class="math inline">\(\mathbf R\)</span> versus PCA based on <span class="math inline">\(\mathbf S\)</span></a></li>
<li class="chapter" data-level="4.2.4" data-path="4-2-pca-a-formal-description-with-proofs.html"><a href="4-2-pca-a-formal-description-with-proofs.html#population-pca"><i class="fa fa-check"></i><b>4.2.4</b> Population PCA</a></li>
<li class="chapter" data-level="4.2.5" data-path="4-2-pca-a-formal-description-with-proofs.html"><a href="4-2-pca-a-formal-description-with-proofs.html#pca-under-transformations-of-variables"><i class="fa fa-check"></i><b>4.2.5</b> PCA under transformations of variables</a></li>
</ul></li>
<li class="chapter" data-level="4.3" data-path="4-3-an-alternative-view-of-pca.html"><a href="4-3-an-alternative-view-of-pca.html"><i class="fa fa-check"></i><b>4.3</b> An alternative view of PCA</a><ul>
<li class="chapter" data-level="4.3.1" data-path="4-3-an-alternative-view-of-pca.html"><a href="4-3-an-alternative-view-of-pca.html#pca-mnist"><i class="fa fa-check"></i><b>4.3.1</b> Example: MNIST handwritten digits</a></li>
</ul></li>
<li class="chapter" data-level="4.4" data-path="4-4-pca-comptask.html"><a href="4-4-pca-comptask.html"><i class="fa fa-check"></i><b>4.4</b> Computer tasks</a></li>
<li class="chapter" data-level="4.5" data-path="4-5-exercises-1.html"><a href="4-5-exercises-1.html"><i class="fa fa-check"></i><b>4.5</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="5-cca.html"><a href="5-cca.html"><i class="fa fa-check"></i><b>5</b> Canonical Correlation Analysis (CCA)</a><ul>
<li class="chapter" data-level="5.1" data-path="5-1-cca1.html"><a href="5-1-cca1.html"><i class="fa fa-check"></i><b>5.1</b> The first pair of canonical variables</a><ul>
<li class="chapter" data-level="5.1.1" data-path="5-1-cca1.html"><a href="5-1-cca1.html#the-first-canonical-components"><i class="fa fa-check"></i><b>5.1.1</b> The first canonical components</a></li>
<li class="chapter" data-level="5.1.2" data-path="5-1-cca1.html"><a href="5-1-cca1.html#premcca"><i class="fa fa-check"></i><b>5.1.2</b> Example: Premier league football</a></li>
</ul></li>
<li class="chapter" data-level="5.2" data-path="5-2-the-full-set-of-canonical-correlations.html"><a href="5-2-the-full-set-of-canonical-correlations.html"><i class="fa fa-check"></i><b>5.2</b> The full set of canonical correlations</a><ul>
<li class="chapter" data-level="5.2.1" data-path="5-2-the-full-set-of-canonical-correlations.html"><a href="5-2-the-full-set-of-canonical-correlations.html#example-continued"><i class="fa fa-check"></i><b>5.2.1</b> Example continued</a></li>
</ul></li>
<li class="chapter" data-level="5.3" data-path="5-3-properties.html"><a href="5-3-properties.html"><i class="fa fa-check"></i><b>5.3</b> Properties</a><ul>
<li class="chapter" data-level="5.3.1" data-path="5-3-properties.html"><a href="5-3-properties.html#connection-with-linear-regression-when-q1"><i class="fa fa-check"></i><b>5.3.1</b> Connection with linear regression when <span class="math inline">\(q=1\)</span></a></li>
<li class="chapter" data-level="5.3.2" data-path="5-3-properties.html"><a href="5-3-properties.html#invarianceequivariance-properties-of-cca"><i class="fa fa-check"></i><b>5.3.2</b> Invariance/equivariance properties of CCA</a></li>
</ul></li>
<li class="chapter" data-level="5.4" data-path="5-4-computer-tasks-1.html"><a href="5-4-computer-tasks-1.html"><i class="fa fa-check"></i><b>5.4</b> Computer tasks</a></li>
<li class="chapter" data-level="5.5" data-path="5-5-exercises-2.html"><a href="5-5-exercises-2.html"><i class="fa fa-check"></i><b>5.5</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="6-mds.html"><a href="6-mds.html"><i class="fa fa-check"></i><b>6</b> Multidimensional Scaling (MDS)</a><ul>
<li class="chapter" data-level="6.1" data-path="6-1-classical-mds.html"><a href="6-1-classical-mds.html"><i class="fa fa-check"></i><b>6.1</b> Classical MDS</a><ul>
<li class="chapter" data-level="6.1.1" data-path="6-1-classical-mds.html"><a href="6-1-classical-mds.html#non-euclidean-distance-matrices"><i class="fa fa-check"></i><b>6.1.1</b> Non-Euclidean distance matrices</a></li>
<li class="chapter" data-level="6.1.2" data-path="6-1-classical-mds.html"><a href="6-1-classical-mds.html#principal-coordinate-analysis"><i class="fa fa-check"></i><b>6.1.2</b> Principal Coordinate Analysis</a></li>
</ul></li>
<li class="chapter" data-level="6.2" data-path="6-2-similarity.html"><a href="6-2-similarity.html"><i class="fa fa-check"></i><b>6.2</b> Similarity measures</a><ul>
<li class="chapter" data-level="6.2.1" data-path="6-2-similarity.html"><a href="6-2-similarity.html#binary-attributes"><i class="fa fa-check"></i><b>6.2.1</b> Binary attributes</a></li>
<li class="chapter" data-level="6.2.2" data-path="6-2-similarity.html"><a href="6-2-similarity.html#example-classical-mds-with-the-mnist-data"><i class="fa fa-check"></i><b>6.2.2</b> Example: Classical MDS with the MNIST data</a></li>
</ul></li>
<li class="chapter" data-level="6.3" data-path="6-3-non-metric-mds.html"><a href="6-3-non-metric-mds.html"><i class="fa fa-check"></i><b>6.3</b> Non-metric MDS</a></li>
<li class="chapter" data-level="6.4" data-path="6-4-exercises-3.html"><a href="6-4-exercises-3.html"><i class="fa fa-check"></i><b>6.4</b> Exercises</a></li>
<li class="chapter" data-level="6.5" data-path="6-5-computer-tasks-2.html"><a href="6-5-computer-tasks-2.html"><i class="fa fa-check"></i><b>6.5</b> Computer Tasks</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="part-iii-inference-using-the-multivariate-normal-distribution-mvn.html"><a href="part-iii-inference-using-the-multivariate-normal-distribution-mvn.html"><i class="fa fa-check"></i>Part III: Inference using the Multivariate Normal Distribution (MVN)</a></li>
<li class="chapter" data-level="7" data-path="7-multinormal.html"><a href="7-multinormal.html"><i class="fa fa-check"></i><b>7</b> The Multivariate Normal Distribution</a><ul>
<li class="chapter" data-level="7.1" data-path="7-1-definition-and-properties-of-the-mvn.html"><a href="7-1-definition-and-properties-of-the-mvn.html"><i class="fa fa-check"></i><b>7.1</b> Definition and Properties of the MVN</a><ul>
<li class="chapter" data-level="7.1.1" data-path="7-1-definition-and-properties-of-the-mvn.html"><a href="7-1-definition-and-properties-of-the-mvn.html#basics"><i class="fa fa-check"></i><b>7.1.1</b> Basics</a></li>
<li class="chapter" data-level="7.1.2" data-path="7-1-definition-and-properties-of-the-mvn.html"><a href="7-1-definition-and-properties-of-the-mvn.html#transformations"><i class="fa fa-check"></i><b>7.1.2</b> Transformations</a></li>
<li class="chapter" data-level="7.1.3" data-path="7-1-definition-and-properties-of-the-mvn.html"><a href="7-1-definition-and-properties-of-the-mvn.html#independence"><i class="fa fa-check"></i><b>7.1.3</b> Independence</a></li>
<li class="chapter" data-level="7.1.4" data-path="7-1-definition-and-properties-of-the-mvn.html"><a href="7-1-definition-and-properties-of-the-mvn.html#confidence-ellipses"><i class="fa fa-check"></i><b>7.1.4</b> Confidence ellipses</a></li>
<li class="chapter" data-level="7.1.5" data-path="7-1-definition-and-properties-of-the-mvn.html"><a href="7-1-definition-and-properties-of-the-mvn.html#sampling-results-for-the-mvn"><i class="fa fa-check"></i><b>7.1.5</b> Sampling results for the MVN</a></li>
</ul></li>
<li class="chapter" data-level="7.2" data-path="7-2-the-wishart-distribution.html"><a href="7-2-the-wishart-distribution.html"><i class="fa fa-check"></i><b>7.2</b> The Wishart distribution</a><ul>
<li class="chapter" data-level="7.2.1" data-path="7-2-the-wishart-distribution.html"><a href="7-2-the-wishart-distribution.html#properties-1"><i class="fa fa-check"></i><b>7.2.1</b> Properties</a></li>
<li class="chapter" data-level="7.2.2" data-path="7-2-the-wishart-distribution.html"><a href="7-2-the-wishart-distribution.html#cochrans-theorem"><i class="fa fa-check"></i><b>7.2.2</b> Cochran’s theorem</a></li>
</ul></li>
<li class="chapter" data-level="7.3" data-path="7-3-hotellings-t2-distribution.html"><a href="7-3-hotellings-t2-distribution.html"><i class="fa fa-check"></i><b>7.3</b> Hotelling’s <span class="math inline">\(T^2\)</span> distribution</a></li>
<li class="chapter" data-level="7.4" data-path="7-4-inference-based-on-the-mvn.html"><a href="7-4-inference-based-on-the-mvn.html"><i class="fa fa-check"></i><b>7.4</b> Inference based on the MVN</a><ul>
<li class="chapter" data-level="7.4.1" data-path="7-4-inference-based-on-the-mvn.html"><a href="7-4-inference-based-on-the-mvn.html#onesampleSigma"><i class="fa fa-check"></i><b>7.4.1</b> <span class="math inline">\(\boldsymbol{\Sigma}\)</span> known</a></li>
<li class="chapter" data-level="7.4.2" data-path="7-4-inference-based-on-the-mvn.html"><a href="7-4-inference-based-on-the-mvn.html#onesample"><i class="fa fa-check"></i><b>7.4.2</b> <span class="math inline">\(\boldsymbol{\Sigma}\)</span> unknown: 1 sample</a></li>
<li class="chapter" data-level="7.4.3" data-path="7-4-inference-based-on-the-mvn.html"><a href="7-4-inference-based-on-the-mvn.html#boldsymbolsigma-unknown-2-samples"><i class="fa fa-check"></i><b>7.4.3</b> <span class="math inline">\(\boldsymbol{\Sigma}\)</span> unknown: 2 samples</a></li>
</ul></li>
<li class="chapter" data-level="7.5" data-path="7-5-exercises-4.html"><a href="7-5-exercises-4.html"><i class="fa fa-check"></i><b>7.5</b> Exercises</a></li>
<li class="chapter" data-level="7.6" data-path="7-6-computer-tasks-3.html"><a href="7-6-computer-tasks-3.html"><i class="fa fa-check"></i><b>7.6</b> Computer tasks</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="part-iv-classification-and-clustering.html"><a href="part-iv-classification-and-clustering.html"><i class="fa fa-check"></i>Part IV: Classification and Clustering</a></li>
<li class="chapter" data-level="8" data-path="8-lda.html"><a href="8-lda.html"><i class="fa fa-check"></i><b>8</b> Discriminant analysis</a><ul>
<li class="chapter" data-level="8.0.1" data-path="8-lda.html"><a href="8-lda.html#linear-discriminant-analysis"><i class="fa fa-check"></i><b>8.0.1</b> Linear discriminant analysis</a></li>
<li class="chapter" data-level="8.1" data-path="8-1-lda-ML.html"><a href="8-1-lda-ML.html"><i class="fa fa-check"></i><b>8.1</b> Maximum likelihood (ML) discriminant rule</a><ul>
<li class="chapter" data-level="8.1.1" data-path="8-1-lda-ML.html"><a href="8-1-lda-ML.html#multivariate-normal-populations"><i class="fa fa-check"></i><b>8.1.1</b> Multivariate normal populations</a></li>
<li class="chapter" data-level="8.1.2" data-path="8-1-lda-ML.html"><a href="8-1-lda-ML.html#sample-lda"><i class="fa fa-check"></i><b>8.1.2</b> The sample ML discriminant rule</a></li>
<li class="chapter" data-level="8.1.3" data-path="8-1-lda-ML.html"><a href="8-1-lda-ML.html#two-populations"><i class="fa fa-check"></i><b>8.1.3</b> Two populations</a></li>
<li class="chapter" data-level="8.1.4" data-path="8-1-lda-ML.html"><a href="8-1-lda-ML.html#more-than-two-populations"><i class="fa fa-check"></i><b>8.1.4</b> More than two populations</a></li>
</ul></li>
<li class="chapter" data-level="8.2" data-path="8-2-lda-Bayes.html"><a href="8-2-lda-Bayes.html"><i class="fa fa-check"></i><b>8.2</b> Bayes discriminant rule</a><ul>
<li class="chapter" data-level="8.2.1" data-path="8-2-lda-Bayes.html"><a href="8-2-lda-Bayes.html#example-lda-using-the-iris-data"><i class="fa fa-check"></i><b>8.2.1</b> Example: LDA using the Iris data</a></li>
<li class="chapter" data-level="8.2.2" data-path="8-2-lda-Bayes.html"><a href="8-2-lda-Bayes.html#quadratic-discriminant-analysis-qda"><i class="fa fa-check"></i><b>8.2.2</b> Quadratic Discriminant Analysis (QDA)</a></li>
<li class="chapter" data-level="8.2.3" data-path="8-2-lda-Bayes.html"><a href="8-2-lda-Bayes.html#prediction-accuracy"><i class="fa fa-check"></i><b>8.2.3</b> Prediction accuracy</a></li>
</ul></li>
<li class="chapter" data-level="8.3" data-path="8-3-FLDA.html"><a href="8-3-FLDA.html"><i class="fa fa-check"></i><b>8.3</b> Fisher’s linear discriminant rule</a><ul>
<li class="chapter" data-level="8.3.1" data-path="8-3-FLDA.html"><a href="8-3-FLDA.html#iris-example-continued-1"><i class="fa fa-check"></i><b>8.3.1</b> Iris example continued</a></li>
<li class="chapter" data-level="8.3.2" data-path="8-3-FLDA.html"><a href="8-3-FLDA.html#links-between-methods"><i class="fa fa-check"></i><b>8.3.2</b> Links between methods</a></li>
</ul></li>
<li class="chapter" data-level="8.4" data-path="8-4-computer-tasks-4.html"><a href="8-4-computer-tasks-4.html"><i class="fa fa-check"></i><b>8.4</b> Computer tasks</a></li>
<li class="chapter" data-level="8.5" data-path="8-5-exercises-5.html"><a href="8-5-exercises-5.html"><i class="fa fa-check"></i><b>8.5</b> Exercises</a></li>
</ul></li>
<li class="divider"></li>
<li> <a href="https://rich-d-wilkinson.github.io/teaching.html" target="blank">University of Nottingham</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Multivariate Statistics</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="an-alternative-view-of-pca" class="section level2">
<h2><span class="header-section-number">4.3</span> An alternative view of PCA</h2>
<p>In this section, we will again consider the situation in which the sample <span class="math inline">\(\mathbf x_1, \ldots , \mathbf x_n \in \mathbb{R}^p\)</span> have zero mean (replace <span class="math inline">\(\mathbf x_i\)</span> by <span class="math inline">\(\mathbf x_i-\bar{\mathbf x}\)</span> if the mean is not zero).</p>
<p>To recap, in PCA to find the <span class="math inline">\(r\)</span> leading principal components, we solve the optimization problem
<span class="math display">\[\begin{align*}
\mbox{For } k=1, \ldots, r &amp;\mbox{ maximize } \mathbf u_k^\top \mathbf S\mathbf u_k \\
 &amp;\mbox{ subject to } \mathbf u_k^\top \mathbf u_j = \begin{cases}
 1  &amp;\mbox{ if } j=k\\
 0 &amp; \mbox{ otherwise.}
 \end{cases}
 \end{align*}\]</span></p>
<p>We can write this in the form given in the introduction to this chapter (Equation <a href="4-pca.html#eq:dimredopt">(4.1)</a>) as
<span class="math display">\[\begin{align*}
&amp;\mbox{Maximize } \operatorname{tr}(\mathbf U^\top \mathbf S\mathbf U) \\
 &amp;\mbox{ subject to } \mathbf U^\top \mathbf U=\mathbf I_r,
 \end{align*}\]</span>
as <span class="math inline">\(\operatorname{tr}(\mathbf U^\top \mathbf S\mathbf U) = \sum_{k=1}^r \mathbf u_k^\top \mathbf S\mathbf u_k\)</span> if <span class="math inline">\(\mathbf U\)</span> has columns <span class="math inline">\(\mathbf u_1, \ldots, \mathbf u_r\)</span>.</p>
<div id="an-equivalent-problem" class="section level4 unnumbered">
<h4>An equivalent problem</h4>
<p>There is another optimization problem that we sometimes wish to solve, that turns out to be equivalent to the above, thus providing another reason why PCA is so widely used.</p>
<p>Suppose we want to find the best rank-<span class="math inline">\(r\)</span> linear approximation to the data matrix <span class="math inline">\(\mathbf X=\begin{pmatrix}\mathbf x_1&amp; \ldots &amp; \mathbf x_n\end{pmatrix}^\top\)</span> (remember that we’re assuming the data have been column centered, if not, replace <span class="math inline">\(\mathbf X\)</span> by <span class="math inline">\(\mathbf H\mathbf X\)</span>). One way to think about this is seek a <span class="math inline">\(p\times r\)</span> matrix <span class="math inline">\(\mathbf U\)</span> for which the rank <span class="math inline">\(r\)</span> linear model
<span class="math display">\[f(\mathbf y) = \mathbf U\mathbf y\]</span> can be used to represent the data.</p>
<p>Let’s choose <span class="math inline">\(\mathbf y_i\in \mathbb{R}^r\)</span> and <span class="math inline">\(\mathbf U\)</span> to minimize the sum of squared errors
<span class="math display">\[\sum_{i=1}^n ||\mathbf x_i - \mathbf U\mathbf y_i||^2_2.\]</span></p>
<p>If we write
<span class="math display">\[\mathbf Y^\top = \begin{pmatrix} 
| &amp;&amp;|\\
\mathbf y_1&amp; \ldots &amp; \mathbf y_n\\
| &amp;&amp;|
\end{pmatrix}\]</span>
then
<span class="math display">\[\begin{align*}
\sum_{i=1}^n ||\mathbf x_i - \mathbf U\mathbf y_i||^2_2 &amp;=\operatorname{tr}((\mathbf X^\top - \mathbf U\mathbf Y^\top)^\top (\mathbf X^\top - \mathbf U\mathbf Y^\top))\\
&amp;=||\mathbf X^\top - \mathbf U\mathbf Y^\top||_F^2
\end{align*}\]</span></p>
<p>i.e., we’re looking for the rank-<span class="math inline">\(r\)</span> matrix <span class="math inline">\(\mathbf X_r\)</span> that minimizes <span class="math inline">\(||\mathbf X- \mathbf X_r||_F=||\mathbf X^\top - \mathbf X_r^\top||_F\)</span>, noting that we can write an arbitrary rank-<span class="math inline">\(r\)</span> matrix as <span class="math inline">\(\mathbf X_r^\top = \mathbf U\mathbf Y^\top\)</span> for some <span class="math inline">\(p\times r\)</span> matrix <span class="math inline">\(\mathbf U\)</span> and a <span class="math inline">\(n \times r\)</span> matrix <span class="math inline">\(\mathbf Y\)</span>.</p>
<p>It makes sense to restrict the columns of <span class="math inline">\(\mathbf U\)</span> to be orthonormal so that <span class="math inline">\(\mathbf U^\top \mathbf U=\mathbf I_r\)</span> as non-orthonormal coordinates systems are confusing. We know that the <span class="math inline">\(\mathbf u\in \mathcal{C}(\mathbf U)\)</span> (where <span class="math inline">\(\mathcal{C}(\mathbf U)\)</span> is the column space of <span class="math inline">\(\mathbf U\)</span>) that minimizes
<span class="math display">\[||\mathbf x-\mathbf u||_2\]</span>
is the orthogonal projection of <span class="math inline">\(\mathbf x\)</span> onto <span class="math inline">\(\mathcal{C}(\mathbf U)\)</span>, which given the columns of <span class="math inline">\(\mathbf U\)</span> are orthonormal is <span class="math inline">\(\mathbf u= \mathbf U\mathbf U^\top \mathbf x\)</span> (see Section <a href="2-3-linalg-innerprod.html#orthogproj">2.3.3.1</a>). So we must have <span class="math inline">\(\mathbf X_r^\top = \mathbf U\mathbf U^\top \mathbf X^\top\)</span> and <span class="math inline">\(\mathbf Y^\top = \mathbf U^\top \mathbf X^\top\)</span>.</p>
<p>So it remains to find the optimal choice for <span class="math inline">\(\mathbf U\)</span> by minimizing
<span class="math display">\[\begin{align*}
||\mathbf X^\top - \mathbf U\mathbf U^\top \mathbf X^\top||_F^2 &amp;=||\mathbf X- \mathbf X\mathbf U\mathbf U^\top ||_F^2\\
&amp;= \operatorname{tr}((\mathbf X- \mathbf X\mathbf U\mathbf U^\top)^\top(\mathbf X- \mathbf X\mathbf U\mathbf U^\top))\\
&amp;= \operatorname{tr}(\mathbf X^\top \mathbf X) - 2 \operatorname{tr}(\mathbf U\mathbf U^\top \mathbf X^\top\mathbf X) +  \operatorname{tr}(\mathbf U\mathbf U^\top \mathbf X^\top\mathbf X\mathbf U\mathbf U^\top)\\
&amp;= \operatorname{tr}(\mathbf X^\top \mathbf X)  - \operatorname{tr}(\mathbf U^\top \mathbf X^\top \mathbf X\mathbf U) 
\end{align*}\]</span>
where we’ve used the fact <span class="math inline">\(\operatorname{tr}(\mathbf A\mathbf B) = \operatorname{tr}(\mathbf B\mathbf A)\)</span> and that <span class="math inline">\(\mathbf U^\top \mathbf U=\mathbf I_r\)</span>.</p>
<p>Minimizing the equation above with respect to <span class="math inline">\(\mathbf U\)</span> is equivalent to maximizing
<span class="math display">\[\operatorname{tr}(\mathbf U^\top \mathbf S\mathbf U) \]</span>
which is the maximum variance objective we used to introduce PCA.</p>
<p>So to summarize, the optimization problem
<span class="math display">\[\begin{align*}
&amp;\mbox{Minimize } ||\mathbf X^\top -\mathbf U\mathbf U^\top \mathbf X^\top||_F \\
 &amp;\mbox{ subject to } \mathbf U^\top \mathbf U=\mathbf I_r,
 \end{align*}\]</span>
is equivalent to (and has the same as) the PCA optimization problem.</p>
</div>
<div id="pca-mnist" class="section level3">
<h3><span class="header-section-number">4.3.1</span> Example: MNIST handwritten digits</h3>
<p>Let’s consider the MNIST dataset of handwritten digits discussed in Chapter <a href="1-stat-prelim.html#stat-prelim">1</a>. Recall this is a collection of 60,000 digits, each of which has been converted to a <span class="math inline">\(28\times 28\)</span> pixel greyscale image (so <span class="math inline">\(p=784\)</span>).
I’ve made a clean version of the dataset available on Moodle, so you can try this analysis for yourself. Let’s look at just the 3s. I’ve created a plotting function <code>plot.mnist</code>, which is in the code file on Moodle.</p>
<div class="sourceCode" id="cb113"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb113-1" data-line-number="1"><span class="kw">load</span>(<span class="dt">file=</span><span class="st">&quot;mnist.rda&quot;</span>) </a>
<a class="sourceLine" id="cb113-2" data-line-number="2"><span class="kw">source</span>(<span class="st">&#39;mnisttools.R&#39;</span>)</a>
<a class="sourceLine" id="cb113-3" data-line-number="3">mnist3 =<span class="st"> </span>mnist<span class="op">$</span>train<span class="op">$</span>x[mnist<span class="op">$</span>train<span class="op">$</span>y<span class="op">==</span><span class="dv">3</span>,] <span class="co"># select just the 3s</span></a>
<a class="sourceLine" id="cb113-4" data-line-number="4"><span class="kw">plot.mnist</span>(mnist3[<span class="dv">1</span><span class="op">:</span><span class="dv">12</span>,]) <span class="co"># plot the first 12 images</span></a></code></pre></div>
<p><img src="04-pca_files/figure-html/unnamed-chunk-33-1.png" width="672" /></p>
<p>We can see there is quite a bit of variation between them.
Now lets look at <span class="math inline">\(\bar{\mathbf x}\)</span>, the average 3.</p>
<div class="sourceCode" id="cb114"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb114-1" data-line-number="1">xbar=<span class="kw">colMeans</span>(mnist3)</a>
<a class="sourceLine" id="cb114-2" data-line-number="2"><span class="kw">plot.mnist</span>(xbar)</a></code></pre></div>
<p><img src="04-pca_files/figure-html/unnamed-chunk-34-1.png" width="384" /></p>
<p>We can use the <code>prcomp</code> command to find the principal components. Note that we can’t use the <code>scale=TRUE</code> option as some of the columns are all 0, and so R throws an error as it cannot rescale these to have variance 1. Let’s plot the first few principal components/eigenvectors/loading vectors.</p>
<div class="sourceCode" id="cb115"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb115-1" data-line-number="1">mnist3.pca &lt;-<span class="st"> </span><span class="kw">prcomp</span>(mnist3)</a>
<a class="sourceLine" id="cb115-2" data-line-number="2"><span class="kw">plot.mnist</span>(mnist3.pca<span class="op">$</span>rotation[,<span class="dv">1</span><span class="op">:</span><span class="dv">12</span>]) </a></code></pre></div>
<p><img src="04-pca_files/figure-html/unnamed-chunk-35-1.png" width="672" /></p>
<p>These show the main mode of variability in the 3s. Focusing on the first PC, we can see that this is a form of rotation and causes the 3 to slant either forward or backward. If we wanted a rank-2 approximation to the data we would use
<span class="math display">\[f(\mathbf y) = \bar{\mathbf x} + y_1 \mathbf v_1 + y_2 \mathbf v_2\]</span></p>
<p>Let’s try reconstructing the data with <span class="math inline">\(r=2\)</span>.</p>
<div class="sourceCode" id="cb116"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb116-1" data-line-number="1">r=<span class="dv">2</span></a>
<a class="sourceLine" id="cb116-2" data-line-number="2">recon =<span class="st">  </span>mnist3.pca<span class="op">$</span>x[,<span class="dv">1</span><span class="op">:</span>r] <span class="op">%*%</span><span class="st"> </span><span class="kw">t</span>(mnist3.pca<span class="op">$</span>rotation[,<span class="dv">1</span><span class="op">:</span>r])</a>
<a class="sourceLine" id="cb116-3" data-line-number="3"><span class="kw">plot.mnist2</span>(<span class="kw">matrix</span>(<span class="kw">rep</span>(xbar,<span class="dv">12</span>), <span class="dt">byrow=</span>T, <span class="dt">nr=</span><span class="dv">12</span>)<span class="op">+</span>recon[<span class="dv">1</span><span class="op">:</span><span class="dv">12</span>,])</a></code></pre></div>
<p><img src="04-pca_files/figure-html/unnamed-chunk-36-1.png" width="672" /></p>
<p>We can see that all of these 3s still look a lot like the average 3, but that they vary in their slant, and the heaviness of the line.</p>
<p>The scree plot shows a sharp decrease in the eigenvalues until about the 100th component, at which point they level off.</p>
<div class="sourceCode" id="cb117"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb117-1" data-line-number="1"><span class="kw">plot</span>(mnist3.pca<span class="op">$</span>sdev) <span class="co"># scree plot</span></a></code></pre></div>
<p><img src="04-pca_files/figure-html/unnamed-chunk-37-1.png" width="672" /></p>
<p>It can also be useful to plot the cumulative sum of the total proportion of variance explained by a given number of principal components. I’ve drawn on horizontal lines at 90% and 95% of variance explained, to help identify when we cross these thresholds.
We need 80 components to explain 90% of the variance, and 138 components to explain 95% of the variance.</p>
<div class="sourceCode" id="cb118"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb118-1" data-line-number="1">cumvar =<span class="st"> </span><span class="dv">100</span><span class="op">*</span><span class="kw">cumsum</span>(mnist3.pca<span class="op">$</span>sdev<span class="op">^</span><span class="dv">2</span>) <span class="op">/</span><span class="st"> </span><span class="kw">sum</span>(mnist3.pca<span class="op">$</span>sdev<span class="op">^</span><span class="dv">2</span>)</a>
<a class="sourceLine" id="cb118-2" data-line-number="2"><span class="kw">plot</span>(cumvar, <span class="dt">ylab=</span><span class="st">&quot;Cumulative proportion of variance explained&quot;</span>, <span class="dt">xlab=</span><span class="st">&quot;Number of PCs used&quot;</span>, <span class="dt">ylim=</span><span class="kw">c</span>(<span class="dv">0</span>,<span class="dv">100</span>))</a>
<a class="sourceLine" id="cb118-3" data-line-number="3"><span class="kw">abline</span>(<span class="dt">h=</span><span class="dv">90</span>, <span class="dt">lty=</span><span class="dv">2</span>)</a>
<a class="sourceLine" id="cb118-4" data-line-number="4"><span class="kw">abline</span>(<span class="dt">v=</span><span class="kw">min</span>(<span class="kw">which</span>(cumvar<span class="op">&gt;</span><span class="dv">90</span>)), <span class="dt">lty=</span><span class="dv">2</span>)</a>
<a class="sourceLine" id="cb118-5" data-line-number="5"><span class="kw">abline</span>(<span class="dt">h=</span><span class="dv">95</span>, <span class="dt">lty=</span><span class="dv">2</span>)</a>
<a class="sourceLine" id="cb118-6" data-line-number="6"><span class="kw">abline</span>(<span class="dt">v=</span><span class="kw">min</span>(<span class="kw">which</span>(cumvar<span class="op">&gt;</span><span class="dv">95</span>)), <span class="dt">lty=</span><span class="dv">2</span>)</a></code></pre></div>
<p><img src="04-pca_files/figure-html/unnamed-chunk-39-1.png" width="672" /></p>
<p>Let’s now look at the reconstruction using <span class="math inline">\(r=10, \;50, \;100\)</span> and <span class="math inline">\(500\)</span> components to see how the accuracy changes.</p>
<div class="sourceCode" id="cb119"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb119-1" data-line-number="1">r=<span class="dv">10</span></a>
<a class="sourceLine" id="cb119-2" data-line-number="2">recon =<span class="st">  </span>mnist3.pca<span class="op">$</span>x[,<span class="dv">1</span><span class="op">:</span>r] <span class="op">%*%</span><span class="st"> </span><span class="kw">t</span>(mnist3.pca<span class="op">$</span>rotation[,<span class="dv">1</span><span class="op">:</span>r])</a>
<a class="sourceLine" id="cb119-3" data-line-number="3"><span class="kw">plot.mnist2</span>(<span class="kw">matrix</span>(<span class="kw">rep</span>(xbar,<span class="dv">12</span>), <span class="dt">byrow=</span>T, <span class="dt">nr=</span><span class="dv">12</span>)<span class="op">+</span>recon[<span class="dv">1</span><span class="op">:</span><span class="dv">12</span>,])</a></code></pre></div>
<p><img src="04-pca_files/figure-html/unnamed-chunk-40-1.png" width="672" /></p>
<div class="sourceCode" id="cb120"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb120-1" data-line-number="1">r=<span class="dv">50</span></a>
<a class="sourceLine" id="cb120-2" data-line-number="2">recon =<span class="st">  </span>mnist3.pca<span class="op">$</span>x[,<span class="dv">1</span><span class="op">:</span>r] <span class="op">%*%</span><span class="st"> </span><span class="kw">t</span>(mnist3.pca<span class="op">$</span>rotation[,<span class="dv">1</span><span class="op">:</span>r])</a>
<a class="sourceLine" id="cb120-3" data-line-number="3"><span class="kw">plot.mnist2</span>(<span class="kw">matrix</span>(<span class="kw">rep</span>(xbar,<span class="dv">12</span>), <span class="dt">byrow=</span>T, <span class="dt">nr=</span><span class="dv">12</span>)<span class="op">+</span>recon[<span class="dv">1</span><span class="op">:</span><span class="dv">12</span>,])</a></code></pre></div>
<p><img src="04-pca_files/figure-html/unnamed-chunk-41-1.png" width="672" /></p>
<div class="sourceCode" id="cb121"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb121-1" data-line-number="1">r=<span class="dv">100</span></a>
<a class="sourceLine" id="cb121-2" data-line-number="2">recon =<span class="st">  </span>mnist3.pca<span class="op">$</span>x[,<span class="dv">1</span><span class="op">:</span>r] <span class="op">%*%</span><span class="st"> </span><span class="kw">t</span>(mnist3.pca<span class="op">$</span>rotation[,<span class="dv">1</span><span class="op">:</span>r])</a>
<a class="sourceLine" id="cb121-3" data-line-number="3"><span class="kw">plot.mnist2</span>(<span class="kw">matrix</span>(<span class="kw">rep</span>(xbar,<span class="dv">12</span>), <span class="dt">byrow=</span>T, <span class="dt">nr=</span><span class="dv">12</span>)<span class="op">+</span>recon[<span class="dv">1</span><span class="op">:</span><span class="dv">12</span>,])</a></code></pre></div>
<p><img src="04-pca_files/figure-html/unnamed-chunk-42-1.png" width="672" /></p>
<div class="sourceCode" id="cb122"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb122-1" data-line-number="1">r=<span class="dv">500</span></a>
<a class="sourceLine" id="cb122-2" data-line-number="2">recon =<span class="st">  </span>mnist3.pca<span class="op">$</span>x[,<span class="dv">1</span><span class="op">:</span>r] <span class="op">%*%</span><span class="st"> </span><span class="kw">t</span>(mnist3.pca<span class="op">$</span>rotation[,<span class="dv">1</span><span class="op">:</span>r])</a>
<a class="sourceLine" id="cb122-3" data-line-number="3"><span class="kw">plot.mnist2</span>(<span class="kw">matrix</span>(<span class="kw">rep</span>(xbar,<span class="dv">12</span>), <span class="dt">byrow=</span>T, <span class="dt">nr=</span><span class="dv">12</span>)<span class="op">+</span>recon[<span class="dv">1</span><span class="op">:</span><span class="dv">12</span>,])</a></code></pre></div>
<p><img src="04-pca_files/figure-html/unnamed-chunk-43-1.png" width="672" /></p>
<p>We can see that as the number of components increases the reconstructions start to look more like the original 12 images.</p>
<p>We can visualise the range of 3s by looking at a scatter plot of the first two principal components.</p>
<div class="sourceCode" id="cb123"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb123-1" data-line-number="1"><span class="kw">library</span>(ggplot2)</a>
<a class="sourceLine" id="cb123-2" data-line-number="2"><span class="kw">qplot</span>(mnist3.pca<span class="op">$</span>x[,<span class="dv">1</span>], mnist3.pca<span class="op">$</span>x[,<span class="dv">2</span>])</a></code></pre></div>
<p><img src="04-pca_files/figure-html/unnamed-chunk-44-1.png" width="672" /></p>
<p>We can then finding images that differ according to these two PC scores. The first plot below is the 3 with the smallest PC1 score, and the second has the largest PC1 score. The third plot has the smallest PC2 score, and the fourth plot the largest PC2 score.
These four different 3s differ in more than just the first two principal components, but you can see the effect of the PC1 score is to slant the image forward or backward, whereas PC2 changes the thickness of the line.</p>
<div class="sourceCode" id="cb124"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb124-1" data-line-number="1">image_list &lt;-<span class="st"> </span><span class="kw">c</span>(<span class="kw">which.min</span>(mnist3.pca<span class="op">$</span>x[,<span class="dv">1</span>]), <span class="kw">which.max</span>(mnist3.pca<span class="op">$</span>x[,<span class="dv">1</span>]),</a>
<a class="sourceLine" id="cb124-2" data-line-number="2">                <span class="kw">which.min</span>(mnist3.pca<span class="op">$</span>x[,<span class="dv">2</span>]), <span class="kw">which.max</span>(mnist3.pca<span class="op">$</span>x[,<span class="dv">2</span>]))</a>
<a class="sourceLine" id="cb124-3" data-line-number="3"><span class="kw">plot.mnist</span>(mnist3[image_list,]) <span class="co"># plot the first 12 images</span></a></code></pre></div>
<p><img src="04-pca_files/figure-html/unnamed-chunk-45-1.png" width="672" /></p>
<p>Finally, let’s do PCA on a selection of the 60,000 images (not just the 3s). You can compute the SVD (which is what <code>prcomp</code> uses to do PCA) on a <span class="math inline">\(60,000 \times 784\)</span> matrix, but it takes a long time on most computers, so here I’ve just computed the first two components on a random selection of 5,000 images using the option <code>rank=2</code> which significantly speeds up the computation time.</p>
<div class="sourceCode" id="cb125"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb125-1" data-line-number="1"><span class="co"># Note this is slow to compute!</span></a>
<a class="sourceLine" id="cb125-2" data-line-number="2">image_index &lt;-<span class="st"> </span><span class="kw">sample</span>(<span class="dv">1</span><span class="op">:</span><span class="dv">60000</span>, <span class="dt">size=</span><span class="dv">5000</span>) <span class="co"># select a random sample of images</span></a>
<a class="sourceLine" id="cb125-3" data-line-number="3">mnist.pca &lt;-<span class="st"> </span><span class="kw">prcomp</span>(mnist<span class="op">$</span>train<span class="op">$</span>x[image_index,], <span class="dt">rank=</span><span class="dv">2</span>)</a>
<a class="sourceLine" id="cb125-4" data-line-number="4">Digit =<span class="st"> </span><span class="kw">as.factor</span>(mnist<span class="op">$</span>train<span class="op">$</span>y[image_index])</a>
<a class="sourceLine" id="cb125-5" data-line-number="5"><span class="kw">ggplot</span>(<span class="kw">as.data.frame</span>(mnist.pca<span class="op">$</span>x), <span class="kw">aes</span>(<span class="dt">x=</span>PC1, <span class="dt">y=</span>PC2, <span class="dt">colour=</span>Digit, <span class="dt">label=</span>Digit))<span class="op">+</span></a>
<a class="sourceLine" id="cb125-6" data-line-number="6"><span class="kw">geom_text</span>(<span class="kw">aes</span>(<span class="dt">label=</span>Digit))</a></code></pre></div>
<p><img src="04-pca_files/figure-html/unnamed-chunk-46-1.png" width="672" /></p>
<p>We can see from this scatter plot that the first two principal components do a surprisingly good job of separating and clustering the digits.</p>
<!--Extensions 

Principal component regression



 Total least squares
-->
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="4-2-pca-a-formal-description-with-proofs.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="4-4-pca-comptask.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["MultivariateStatistics.pdf"],
"toc": {
"collapse": "section"
},
"pandoc_args": "--top-level-division=[chapter|part]"
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
