<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>4.5 PCA under transformations of variables | Multivariate Statistics</title>
  <meta name="description" content="The lecture notes for MATH3030/4068: Multivariate Analysis / Applied Multivariate Statistics" />
  <meta name="generator" content="bookdown 0.20.6 and GitBook 2.6.7" />

  <meta property="og:title" content="4.5 PCA under transformations of variables | Multivariate Statistics" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="The lecture notes for MATH3030/4068: Multivariate Analysis / Applied Multivariate Statistics" />
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="4.5 PCA under transformations of variables | Multivariate Statistics" />
  
  <meta name="twitter:description" content="The lecture notes for MATH3030/4068: Multivariate Analysis / Applied Multivariate Statistics" />
  

<meta name="author" content="Prof.Â Richard Wilkinson" />



  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="4-4-an-alternative-derivation-of-pca.html"/>
<link rel="next" href="4-6-pca-based-on-boldsymbol-s-versus-pca-based-on-boldsymbol-r.html"/>
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />









<script src="libs/kePrint-0.0.1/kePrint.js"></script>
<link href="libs/lightable-0.0.1/lightable.css" rel="stylesheet" />


<style type="text/css">
a.sourceLine { display: inline-block; line-height: 1.25; }
a.sourceLine { pointer-events: none; color: inherit; text-decoration: inherit; }
a.sourceLine:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode { white-space: pre; position: relative; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
code.sourceCode { white-space: pre-wrap; }
a.sourceLine { text-indent: -1em; padding-left: 1em; }
}
pre.numberSource a.sourceLine
  { position: relative; left: -4em; }
pre.numberSource a.sourceLine::before
  { content: attr(data-line-number);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; pointer-events: all; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {  }
@media screen {
a.sourceLine::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Applied multivariate statistics</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Introduction</a></li>
<li class="chapter" data-level="" data-path="part-i-prerequisites.html"><a href="part-i-prerequisites.html"><i class="fa fa-check"></i>PART I: Prerequisites</a></li>
<li class="chapter" data-level="1" data-path="1-stat-prelim.html"><a href="1-stat-prelim.html"><i class="fa fa-check"></i><b>1</b> Statistical Preliminaries</a><ul>
<li class="chapter" data-level="1.1" data-path="1-1-notation.html"><a href="1-1-notation.html"><i class="fa fa-check"></i><b>1.1</b> Notation</a><ul>
<li class="chapter" data-level="1.1.1" data-path="1-1-notation.html"><a href="1-1-notation.html#example-datasets"><i class="fa fa-check"></i><b>1.1.1</b> Example datasets</a></li>
<li class="chapter" data-level="1.1.2" data-path="1-1-notation.html"><a href="1-1-notation.html#aims-of-nultivariate-data-analysis"><i class="fa fa-check"></i><b>1.1.2</b> Aims of nultivariate data analysis</a></li>
</ul></li>
<li class="chapter" data-level="1.2" data-path="1-2-exploratory-data-analysis-eda.html"><a href="1-2-exploratory-data-analysis-eda.html"><i class="fa fa-check"></i><b>1.2</b> Exploratory data analysis (EDA)</a><ul>
<li class="chapter" data-level="1.2.1" data-path="1-2-exploratory-data-analysis-eda.html"><a href="1-2-exploratory-data-analysis-eda.html#data-visualization"><i class="fa fa-check"></i><b>1.2.1</b> Data visualization</a></li>
<li class="chapter" data-level="1.2.2" data-path="1-2-exploratory-data-analysis-eda.html"><a href="1-2-exploratory-data-analysis-eda.html#summary-statistics"><i class="fa fa-check"></i><b>1.2.2</b> Summary statistics</a></li>
</ul></li>
<li class="chapter" data-level="1.3" data-path="1-3-random-vectors-and-matrices.html"><a href="1-3-random-vectors-and-matrices.html"><i class="fa fa-check"></i><b>1.3</b> Random vectors and matrices</a><ul>
<li class="chapter" data-level="1.3.1" data-path="1-3-random-vectors-and-matrices.html"><a href="1-3-random-vectors-and-matrices.html#estimators"><i class="fa fa-check"></i><b>1.3.1</b> Estimators</a></li>
</ul></li>
<li class="chapter" data-level="1.4" data-path="1-4-computer-tasks.html"><a href="1-4-computer-tasks.html"><i class="fa fa-check"></i><b>1.4</b> Computer tasks</a></li>
<li class="chapter" data-level="1.5" data-path="1-5-exercises.html"><a href="1-5-exercises.html"><i class="fa fa-check"></i><b>1.5</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="2-linalg-prelim.html"><a href="2-linalg-prelim.html"><i class="fa fa-check"></i><b>2</b> Review of linear algebra</a><ul>
<li class="chapter" data-level="2.1" data-path="2-1-linalg-basics.html"><a href="2-1-linalg-basics.html"><i class="fa fa-check"></i><b>2.1</b> Basics</a><ul>
<li class="chapter" data-level="2.1.1" data-path="2-1-linalg-basics.html"><a href="2-1-linalg-basics.html#notation-1"><i class="fa fa-check"></i><b>2.1.1</b> Notation</a></li>
<li class="chapter" data-level="2.1.2" data-path="2-1-linalg-basics.html"><a href="2-1-linalg-basics.html#elementary-matrix-operations"><i class="fa fa-check"></i><b>2.1.2</b> Elementary matrix operations</a></li>
<li class="chapter" data-level="2.1.3" data-path="2-1-linalg-basics.html"><a href="2-1-linalg-basics.html#special-matrices"><i class="fa fa-check"></i><b>2.1.3</b> Special matrices</a></li>
<li class="chapter" data-level="2.1.4" data-path="2-1-linalg-basics.html"><a href="2-1-linalg-basics.html#vector-differentiation"><i class="fa fa-check"></i><b>2.1.4</b> Vector Differentiation</a></li>
</ul></li>
<li class="chapter" data-level="2.2" data-path="2-2-linalg-vecspaces.html"><a href="2-2-linalg-vecspaces.html"><i class="fa fa-check"></i><b>2.2</b> Vector spaces</a><ul>
<li class="chapter" data-level="2.2.1" data-path="2-2-linalg-vecspaces.html"><a href="2-2-linalg-vecspaces.html#linear-independence"><i class="fa fa-check"></i><b>2.2.1</b> Linear independence</a></li>
<li class="chapter" data-level="2.2.2" data-path="2-2-linalg-vecspaces.html"><a href="2-2-linalg-vecspaces.html#colsspace"><i class="fa fa-check"></i><b>2.2.2</b> Row and column spaces</a></li>
<li class="chapter" data-level="2.2.3" data-path="2-2-linalg-vecspaces.html"><a href="2-2-linalg-vecspaces.html#linear-transformations"><i class="fa fa-check"></i><b>2.2.3</b> Linear transformations</a></li>
</ul></li>
<li class="chapter" data-level="2.3" data-path="2-3-linalg-innerprod.html"><a href="2-3-linalg-innerprod.html"><i class="fa fa-check"></i><b>2.3</b> Inner product spaces</a><ul>
<li class="chapter" data-level="2.3.1" data-path="2-3-linalg-innerprod.html"><a href="2-3-linalg-innerprod.html#normed"><i class="fa fa-check"></i><b>2.3.1</b> Distances, and angles</a></li>
<li class="chapter" data-level="2.3.2" data-path="2-3-linalg-innerprod.html"><a href="2-3-linalg-innerprod.html#orthogonal-matrices"><i class="fa fa-check"></i><b>2.3.2</b> Orthogonal matrices</a></li>
<li class="chapter" data-level="2.3.3" data-path="2-3-linalg-innerprod.html"><a href="2-3-linalg-innerprod.html#projections"><i class="fa fa-check"></i><b>2.3.3</b> Projections</a></li>
</ul></li>
<li class="chapter" data-level="2.4" data-path="2-4-linalg-misc.html"><a href="2-4-linalg-misc.html"><i class="fa fa-check"></i><b>2.4</b> Miscellaneous topics</a><ul>
<li class="chapter" data-level="2.4.1" data-path="2-4-linalg-misc.html"><a href="2-4-linalg-misc.html#the-centering-matrix"><i class="fa fa-check"></i><b>2.4.1</b> The Centering Matrix</a></li>
<li class="chapter" data-level="2.4.2" data-path="2-4-linalg-misc.html"><a href="2-4-linalg-misc.html#quadratic-forms-and-ellipses"><i class="fa fa-check"></i><b>2.4.2</b> Quadratic forms and ellipses</a></li>
<li class="chapter" data-level="2.4.3" data-path="2-4-linalg-misc.html"><a href="2-4-linalg-misc.html#lines-and-hyperplanes-in-mathbbrp"><i class="fa fa-check"></i><b>2.4.3</b> Lines and Hyperplanes in <span class="math inline">\(\mathbb{R}^p\)</span></a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="3" data-path="3-linalg-decomp.html"><a href="3-linalg-decomp.html"><i class="fa fa-check"></i><b>3</b> Matrix decompositions</a><ul>
<li class="chapter" data-level="3.1" data-path="3-1-matrix-matrix.html"><a href="3-1-matrix-matrix.html"><i class="fa fa-check"></i><b>3.1</b> Matrix-matrix products</a></li>
<li class="chapter" data-level="3.2" data-path="3-2-eigenvalues-and-eigenvectors.html"><a href="3-2-eigenvalues-and-eigenvectors.html"><i class="fa fa-check"></i><b>3.2</b> Eigenvalues and eigenvectors</a></li>
<li class="chapter" data-level="3.3" data-path="3-3-spectraleigen-decomposition.html"><a href="3-3-spectraleigen-decomposition.html"><i class="fa fa-check"></i><b>3.3</b> Spectral/eigen decomposition</a><ul>
<li class="chapter" data-level="3.3.1" data-path="3-3-spectraleigen-decomposition.html"><a href="3-3-spectraleigen-decomposition.html#matrix-square-roots"><i class="fa fa-check"></i><b>3.3.1</b> Matrix square roots</a></li>
</ul></li>
<li class="chapter" data-level="3.4" data-path="3-4-linalg-SVD.html"><a href="3-4-linalg-SVD.html"><i class="fa fa-check"></i><b>3.4</b> Singular Value Decomposition (SVD)</a><ul>
<li class="chapter" data-level="3.4.1" data-path="3-4-linalg-SVD.html"><a href="3-4-linalg-SVD.html#examples"><i class="fa fa-check"></i><b>3.4.1</b> Examples</a></li>
</ul></li>
<li class="chapter" data-level="3.5" data-path="3-5-optimization-results.html"><a href="3-5-optimization-results.html"><i class="fa fa-check"></i><b>3.5</b> Optimization results</a></li>
<li class="chapter" data-level="3.6" data-path="3-6-best-approximating-matrices.html"><a href="3-6-best-approximating-matrices.html"><i class="fa fa-check"></i><b>3.6</b> Best approximating matrices</a><ul>
<li class="chapter" data-level="3.6.1" data-path="3-6-best-approximating-matrices.html"><a href="3-6-best-approximating-matrices.html#matrix-norms"><i class="fa fa-check"></i><b>3.6.1</b> Matrix norms</a></li>
<li class="chapter" data-level="3.6.2" data-path="3-6-best-approximating-matrices.html"><a href="3-6-best-approximating-matrices.html#eckart-young-mirsky-theorem"><i class="fa fa-check"></i><b>3.6.2</b> Eckart-Young-Mirsky Theorem</a></li>
<li class="chapter" data-level="3.6.3" data-path="3-6-best-approximating-matrices.html"><a href="3-6-best-approximating-matrices.html#example-image-compression"><i class="fa fa-check"></i><b>3.6.3</b> Example: image compression</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="" data-path="part-ii-dimension-reduction-methods.html"><a href="part-ii-dimension-reduction-methods.html"><i class="fa fa-check"></i>PART II: Dimension reduction methods</a><ul>
<li class="chapter" data-level="" data-path="part-ii-dimension-reduction-methods.html"><a href="part-ii-dimension-reduction-methods.html#a-warning"><i class="fa fa-check"></i>A warning</a></li>
<li class="chapter" data-level="3.6.4" data-path="part-ii-dimension-reduction-methods.html"><a href="part-ii-dimension-reduction-methods.html#why-reduce-dimension"><i class="fa fa-check"></i><b>3.6.4</b> Why reduce dimension?</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="4-pca.html"><a href="4-pca.html"><i class="fa fa-check"></i><b>4</b> Principal component analysis</a><ul>
<li class="chapter" data-level="4.1" data-path="4-1-principal-component-vectors-and-scores.html"><a href="4-1-principal-component-vectors-and-scores.html"><i class="fa fa-check"></i><b>4.1</b> Principal component vectors and scores</a></li>
<li class="chapter" data-level="4.2" data-path="4-2-properties-of-principal-components.html"><a href="4-2-properties-of-principal-components.html"><i class="fa fa-check"></i><b>4.2</b> Properties of principal components</a></li>
<li class="chapter" data-level="4.3" data-path="4-3-population-pca.html"><a href="4-3-population-pca.html"><i class="fa fa-check"></i><b>4.3</b> Population PCA</a></li>
<li class="chapter" data-level="4.4" data-path="4-4-an-alternative-derivation-of-pca.html"><a href="4-4-an-alternative-derivation-of-pca.html"><i class="fa fa-check"></i><b>4.4</b> An Alternative Derivation of PCA</a></li>
<li class="chapter" data-level="4.5" data-path="4-5-pca-under-transformations-of-variables.html"><a href="4-5-pca-under-transformations-of-variables.html"><i class="fa fa-check"></i><b>4.5</b> PCA under transformations of variables</a></li>
<li class="chapter" data-level="4.6" data-path="4-6-pca-based-on-boldsymbol-s-versus-pca-based-on-boldsymbol-r.html"><a href="4-6-pca-based-on-boldsymbol-s-versus-pca-based-on-boldsymbol-r.html"><i class="fa fa-check"></i><b>4.6</b> PCA based on <span class="math inline">\(\boldsymbol S\)</span> versus PCA based on <span class="math inline">\(\boldsymbol R\)</span></a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="5-cca.html"><a href="5-cca.html"><i class="fa fa-check"></i><b>5</b> Canonical Correlation Analysis</a><ul>
<li class="chapter" data-level="5.1" data-path="5-1-canonical-correlation-analysis.html"><a href="5-1-canonical-correlation-analysis.html"><i class="fa fa-check"></i><b>5.1</b> Canonical Correlation Analysis</a></li>
<li class="chapter" data-level="5.2" data-path="5-2-the-full-set-of-canonical-correlations.html"><a href="5-2-the-full-set-of-canonical-correlations.html"><i class="fa fa-check"></i><b>5.2</b> The full set of canonical correlations</a></li>
<li class="chapter" data-level="5.3" data-path="5-3-connection-with-linear-regression-when-q1.html"><a href="5-3-connection-with-linear-regression-when-q1.html"><i class="fa fa-check"></i><b>5.3</b> Connection with linear regression when <span class="math inline">\(q=1\)</span></a></li>
<li class="chapter" data-level="5.4" data-path="5-4-population-cca.html"><a href="5-4-population-cca.html"><i class="fa fa-check"></i><b>5.4</b> Population CCA</a></li>
<li class="chapter" data-level="5.5" data-path="5-5-invarianceequivariance-properties-of-cca.html"><a href="5-5-invarianceequivariance-properties-of-cca.html"><i class="fa fa-check"></i><b>5.5</b> Invariance/equivariance properties of CCA</a></li>
<li class="chapter" data-level="5.6" data-path="5-6-testing-for-zero-canonical-correlation-coefficients.html"><a href="5-6-testing-for-zero-canonical-correlation-coefficients.html"><i class="fa fa-check"></i><b>5.6</b> Testing for zero canonical correlation coefficients</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="6-mds.html"><a href="6-mds.html"><i class="fa fa-check"></i><b>6</b> Multidimensional Scaling</a><ul>
<li class="chapter" data-level="6.1" data-path="6-1-multidimensional-scaling.html"><a href="6-1-multidimensional-scaling.html"><i class="fa fa-check"></i><b>6.1</b> Multidimensional Scaling</a></li>
<li class="chapter" data-level="6.2" data-path="6-2-principal-coordinates.html"><a href="6-2-principal-coordinates.html"><i class="fa fa-check"></i><b>6.2</b> Principal Coordinates</a></li>
<li class="chapter" data-level="6.3" data-path="6-3-similarity-measures.html"><a href="6-3-similarity-measures.html"><i class="fa fa-check"></i><b>6.3</b> Similarity measures</a></li>
</ul></li>
<li class="divider"></li>
<li> <a href="https://rich-d-wilkinson.github.io/teaching.html" target="blank">University of Nottingham</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Multivariate Statistics</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="pca-under-transformations-of-variables" class="section level2">
<h2><span class="header-section-number">4.5</span> PCA under transformations of variables</h2>
<p>Let us return to the example of <span class="math inline">\(n=10\)</span> students who studied G11PRB and G11STA. Earlier, we calculated the sample mean, sample variance matrix and the eigenvalues/vectors of <span class="math inline">\(\boldsymbol S\)</span>,
<span class="math display">\[\begin{eqnarray*}
\bar{\boldsymbol x} = \begin{pmatrix} 62.6 \\ 66.2 \end{pmatrix}, &amp;\quad&amp;
\boldsymbol S= \begin{pmatrix} 162.04 &amp; 135.38 \\ 135.38 &amp; 175.36 \end{pmatrix} \\
\boldsymbol \Lambda= \begin{pmatrix} 304.24 &amp; 0 \\ 0 &amp; 33.16 \end{pmatrix}, &amp;\quad&amp;
\boldsymbol Q= \begin{pmatrix} 0.690 &amp; -0.724 \\ 0.724 &amp; 0.690 \end{pmatrix}
\end{eqnarray*}\]</span>
with PC 1 scores
<span class="math display">\[y_i = \boldsymbol q_1^\top (\boldsymbol x_i - \bar{\boldsymbol x}) = 0.690 (x_{1i} - \bar{x}_1) + 0.724 (x_{2i} - \bar{x}_2).\]</span></p>
<p>We now consider what happens to the above quantities under various transformations of the <span class="math inline">\(\boldsymbol x_i\)</span>, the <span class="math inline">\(2 \times 1\)</span>
response vectors.</p>
<p><strong>Addition transformation</strong></p>
<p>Firstly, we consider the transformation of addition where, for example, the G11PRB lecturer decides to add 5 marks for all the students. We can write this transformation as <span class="math inline">\(\boldsymbol z_i = \boldsymbol x_i + \boldsymbol c\)</span>, where <span class="math inline">\(\boldsymbol c\)</span> is a fixed vector. Under this transformation the sample mean changes, <span class="math inline">\(\bar{\boldsymbol z} = \bar{\boldsymbol x} + \boldsymbol c\)</span>, but the sample variance remains <span class="math inline">\(\boldsymbol S\)</span>. Consequently, the eigenvalues and eigenvectors of <span class="math inline">\(\boldsymbol S\)</span> remain the same and, therefore, so does the PC 1 score,
<span class="math display">\[y_i = \boldsymbol q_1^\top (\boldsymbol z_i - \bar{\boldsymbol z}) = \boldsymbol q_1^\top(\boldsymbol x_i + \boldsymbol c- (\bar{\boldsymbol x} + \boldsymbol c)) = \boldsymbol q_1^\top (\boldsymbol x_i - \bar{\boldsymbol x}).\]</span>
We say that the principal components are <strong>invariant</strong> under the addition transformation. An important special case is to choose <span class="math inline">\(\boldsymbol c= -\bar{\boldsymbol x}\)</span> so that the PC 1 score is simply <span class="math inline">\(y_i = \boldsymbol q_1^\top \boldsymbol z_i\)</span>.</p>
<p><strong>Scale transformation</strong></p>
<p>Secondly, we consider the scale transformation where, for example, the G11PRB lecturer decides to double the marks for all students. A scale transformation occurs more naturally when we convert units of measurement from, say, metres to kilometres. We can write this transformation as <span class="math inline">\(\boldsymbol z_i = \boldsymbol D\boldsymbol x_i\)</span>, where <span class="math inline">\(\boldsymbol D\)</span> is a diagonal matrix with positive elements. Under this transformation the sample mean changes from <span class="math inline">\(\bar{\boldsymbol x}\)</span> to <span class="math inline">\(\bar{\boldsymbol z} = \boldsymbol D\bar{\boldsymbol x}\)</span>, and the sample covariance matrix changes from <span class="math inline">\(\boldsymbol S\)</span> to <span class="math inline">\(\boldsymbol D\boldsymbol S\boldsymbol D\)</span>. Consequently, the principal components also change.</p>
<p>This lack of scale-invariance is undesirable. One solution is to choose
<span class="math display">\[
\boldsymbol D= \text{diag}(s_{11}^{-1/2}, \ldots , s_{pp}^{-1/2}),
 \]</span>
where <span class="math inline">\(s_{ii}\)</span> is the <span class="math inline">\(i\)</span>th diagonal element of <span class="math inline">\(\boldsymbol S\)</span>. In effect, we have standardised all the new variables to have variance 1. In this case the sample covariance matrix of the <span class="math inline">\(\boldsymbol z_i\)</span>âs is simply the sample correlation matrix of the original variables, <span class="math inline">\(\boldsymbol x_i\)</span>. Therefore, we can carry out PCA on the sample correlation matrix, <span class="math inline">\(\boldsymbol R\)</span>, which is invariant to changes of scale.</p>
<p>In summary: <span class="math inline">\(\boldsymbol R\)</span> is scale-invariant while <span class="math inline">\(\boldsymbol S\)</span> is not.</p>

<div class="example">
<p><span id="exm:unnamed-chunk-14" class="example"><strong>Example 4.3  </strong></span>For the G11PRB/G11STA data, we choose
<span class="math display">\[
\boldsymbol D= \text{diag}(162.04,175.36)^{-1/2} = \text{diag}(0.079,0.076)
\]</span>
so that <span class="math inline">\(\boldsymbol z_i = \boldsymbol D\boldsymbol x_i\)</span>.
The sample correlation matrix is then
<span class="math display">\[\begin{align*}
\boldsymbol R&amp;= \boldsymbol D\boldsymbol S\boldsymbol D\\
 &amp;= \begin{pmatrix} 0.079 &amp; 0 \\ 0 &amp; 0.076 \end{pmatrix}
\begin{pmatrix} 162.04 &amp; 135.38 \\ 135.38 &amp; 175.36 \end{pmatrix}
\begin{pmatrix} 0.079 &amp; 0 \\ 0 &amp; 0.076 \end{pmatrix} \\
&amp;= \begin{pmatrix} 1.000 &amp; 0.803 \\ 0.803 &amp; 1.000 \end{pmatrix}.
\end{align*}\]</span>
The eigenvalues and eigenvectors of <span class="math inline">\(\boldsymbol R\)</span> are then
<span class="math display">\[\boldsymbol \Lambda= \begin{pmatrix} 1.803 &amp; 0 \\ 0 &amp; 0.197 \end{pmatrix}, \qquad
\boldsymbol Q= \begin{pmatrix} 0.707 &amp; 0.707 \\ 0.707 &amp; -0.707 \end{pmatrix},\]</span>
and the PC 1 score is
<span class="math display">\[\begin{eqnarray*}
y_i &amp;=&amp; \boldsymbol q_1^\top (\boldsymbol z_i - \bar{\boldsymbol z}) = \boldsymbol q_1^\top \boldsymbol D(\boldsymbol x_i - \bar{\boldsymbol x}) \\
&amp;=&amp; 0.707 \times 0.079 (x_{1i} - \bar{x}_1) + 0.707 \times 0.076 (x_{2i} - \bar{x}_2).
\end{eqnarray*}\]</span></p>
In the example above, there is little difference between using <span class="math inline">\(\boldsymbol S\)</span> and <span class="math inline">\(\boldsymbol R\)</span> for the PCA because the variances for G11PRB and G11STA are similar. In other cases, particularly when the variables are measured on wildly different scales, the difference will be notable. For example, in the football data the sample variances of <span class="math inline">\(F\)</span> and <span class="math inline">\(A\)</span> are much larger than the sample variances of <span class="math inline">\(W\)</span>, <span class="math inline">\(D\)</span> and <span class="math inline">\(L\)</span>.
</div>

<p><strong>Orthogonal transformation</strong></p>
<p>Thirdly, we consider a transformation by an orthogonal matrix, <span class="math inline">\(\stackrel{p \times p}{\boldsymbol A}\)</span>, such that <span class="math inline">\(\boldsymbol A\boldsymbol A^\top = \boldsymbol A^\top \boldsymbol A= \mathbf I_p\)</span>, and write <span class="math inline">\(\boldsymbol z_i = \boldsymbol A\boldsymbol x_i\)</span>. This is equivalent to rotating and/or reflecting the original data.</p>
<p>Let <span class="math inline">\(\boldsymbol S\)</span> be the sample covariance matrix of the <span class="math inline">\(\boldsymbol x_i\)</span> and let <span class="math inline">\(\boldsymbol T\)</span> be the sample covariance matrix of the <span class="math inline">\(\boldsymbol z_i\)</span>. Under this transformation the sample mean changes from <span class="math inline">\(\bar{\boldsymbol x}\)</span> to <span class="math inline">\(\bar{\boldsymbol z} = \boldsymbol A\bar{\boldsymbol x}\)</span>, and the sample covariance matrix <span class="math inline">\(\boldsymbol S\)</span> changes from <span class="math inline">\(\boldsymbol S\)</span> to <span class="math inline">\(\boldsymbol T= \boldsymbol A\boldsymbol S\boldsymbol A^\top\)</span>.</p>
<p>However, if we write <span class="math inline">\(\boldsymbol S\)</span> in terms of its spectral decomposition <span class="math inline">\(\boldsymbol S= \boldsymbol Q\boldsymbol \Lambda\boldsymbol Q^\top\)</span>, then <span class="math inline">\(\boldsymbol T= \boldsymbol A\boldsymbol Q\boldsymbol \Lambda\boldsymbol Q^\top \boldsymbol A^\top = \boldsymbol B\boldsymbol \Lambda\boldsymbol B^\top\)</span> where <span class="math inline">\(\boldsymbol B= \boldsymbol A\boldsymbol Q\)</span> is also orthogonal. It is therefore apparent that the eigenvalues of <span class="math inline">\(\boldsymbol T\)</span> are the same as those of <span class="math inline">\(\boldsymbol S\)</span>; and the eigenvectors of <span class="math inline">\(\boldsymbol T\)</span> are given by <span class="math inline">\(\boldsymbol b_j\)</span> where <span class="math inline">\(\boldsymbol b_j = \boldsymbol A\boldsymbol q_j\)</span>, <span class="math inline">\(j=1,\ldots,p\)</span>. The PC 1 scores of the transformed variables are
<span class="math display">\[ y_i = \boldsymbol b_1^\top (\boldsymbol z_i - \bar{\boldsymbol z}) = \boldsymbol q_1^\top \boldsymbol A^\top \boldsymbol A(\boldsymbol x_i - \bar{\boldsymbol x}) = \boldsymbol q_1^\top (\boldsymbol x_i - \bar{\boldsymbol x}),\]</span>
and so they are identical to the PC 1 scores of the original variables.</p>
<p>Therefore, under an orthogonal transformation the eigenvalues and PC scores are unchanged and the PCs are orthogonal transformations of the original PCs. We say that the principal components are <strong>equivariant</strong> with respect to orthogonal transformations.</p>

<div class="example">
<span id="exm:unnamed-chunk-15" class="example"><strong>Example 4.4  </strong></span>Suppose we rotate the G11PRB/G11STA data by the matrix <span class="math inline">\(\boldsymbol A= \begin{pmatrix} 0.866 &amp; -0.500 \\ 0.500 &amp; 0.866 \end{pmatrix}\)</span>. The sample covariance matrix of the rotated data is
<span class="math display">\[\begin{align*}
\boldsymbol T&amp;= \boldsymbol A\boldsymbol S\boldsymbol A^\top\\
&amp;= \begin{pmatrix} 0.866 &amp; -0.500 \\ 0.500 &amp; 0.866 \end{pmatrix}
\begin{pmatrix} 162.04 &amp; 135.38 \\ 135.38 &amp; 175.36 \end{pmatrix}
\begin{pmatrix} 0.866 &amp; 0.500 \\ -0.500 &amp; 0.866 \end{pmatrix} \\
&amp;= \begin{pmatrix} 48.13 &amp; 61.92 \\ 61.92 &amp; 289.27 \end{pmatrix}.
\end{align*}\]</span>
The eigenvalues of <span class="math inline">\(\boldsymbol T\)</span> are <span class="math inline">\(304.24\)</span> and <span class="math inline">\(33.16\)</span> (same as for <span class="math inline">\(\boldsymbol S\)</span>). The eigenvectors of <span class="math inline">\(\boldsymbol T\)</span> are then
<span class="math display">\[\begin{eqnarray*}
\boldsymbol B&amp;=&amp; \boldsymbol A\boldsymbol Q= \begin{pmatrix} 0.866 &amp; -0.500 \\ 0.500 &amp; 0.866 \end{pmatrix} \begin{pmatrix} 0.690 &amp; -0.724 \\ 0.724 &amp; 0.690 \end{pmatrix} \\
&amp;=&amp; \begin{pmatrix} 0.235 &amp; -0.972 \\ 0.972 &amp; 0.235 \end{pmatrix}
\end{eqnarray*}\]</span>
and the PC 1 scores are unchanged.
</div>

</div>
            </section>

          </div>
        </div>
      </div>
<a href="4-4-an-alternative-derivation-of-pca.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="4-6-pca-based-on-boldsymbol-s-versus-pca-based-on-boldsymbol-r.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["MultivariateStatistics.pdf"],
"toc": {
"collapse": "section"
},
"pandoc_args": "--top-level-division=[chapter|part]"
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
