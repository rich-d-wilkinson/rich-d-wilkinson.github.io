<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>1.2 The sample ML discriminant rule | Multivariate Statistics</title>
  <meta name="description" content="The lecture notes for MATH3030/4068: Multivariate Analysis / Applied Multivariate Statistics" />
  <meta name="generator" content="bookdown 0.21 and GitBook 2.6.7" />

  <meta property="og:title" content="1.2 The sample ML discriminant rule | Multivariate Statistics" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="The lecture notes for MATH3030/4068: Multivariate Analysis / Applied Multivariate Statistics" />
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="1.2 The sample ML discriminant rule | Multivariate Statistics" />
  
  <meta name="twitter:description" content="The lecture notes for MATH3030/4068: Multivariate Analysis / Applied Multivariate Statistics" />
  

<meta name="author" content="Prof. Richard Wilkinson" />



  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="1-1-lda-ML.html"/>
<link rel="next" href="1-3-FLDA.html"/>
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />











<style type="text/css">
a.sourceLine { display: inline-block; line-height: 1.25; }
a.sourceLine { pointer-events: none; color: inherit; text-decoration: inherit; }
a.sourceLine:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode { white-space: pre; position: relative; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
code.sourceCode { white-space: pre-wrap; }
a.sourceLine { text-indent: -1em; padding-left: 1em; }
}
pre.numberSource a.sourceLine
  { position: relative; left: -4em; }
pre.numberSource a.sourceLine::before
  { content: attr(data-line-number);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; pointer-events: all; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {  }
@media screen {
a.sourceLine::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Applied multivariate statistics</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Introduction</a></li>
<li class="chapter" data-level="" data-path="part-iv-classification-and-clustering.html"><a href="part-iv-classification-and-clustering.html"><i class="fa fa-check"></i>Part IV: Classification and Clustering</a></li>
<li class="chapter" data-level="1" data-path="1-lda.html"><a href="1-lda.html"><i class="fa fa-check"></i><b>1</b> Discriminant analysis</a><ul>
<li class="chapter" data-level="1.1" data-path="1-1-lda-ML.html"><a href="1-1-lda-ML.html"><i class="fa fa-check"></i><b>1.1</b> Maximum likelihood (ML) discriminant rule</a><ul>
<li class="chapter" data-level="1.1.1" data-path="1-1-lda-ML.html"><a href="1-1-lda-ML.html#multivariate-gaussian-populations"><i class="fa fa-check"></i><b>1.1.1</b> Multivariate Gaussian populations</a></li>
<li class="chapter" data-level="1.1.2" data-path="1-1-lda-ML.html"><a href="1-1-lda-ML.html#more-than-two-populations"><i class="fa fa-check"></i><b>1.1.2</b> More than two populations</a></li>
<li class="chapter" data-level="1.1.3" data-path="1-1-lda-ML.html"><a href="1-1-lda-ML.html#bayes-classifier"><i class="fa fa-check"></i><b>1.1.3</b> Bayes classifier</a></li>
</ul></li>
<li class="chapter" data-level="1.2" data-path="1-2-sample-lda.html"><a href="1-2-sample-lda.html"><i class="fa fa-check"></i><b>1.2</b> The sample ML discriminant rule</a></li>
<li class="chapter" data-level="1.3" data-path="1-3-FLDA.html"><a href="1-3-FLDA.html"><i class="fa fa-check"></i><b>1.3</b> Fisher’s linear discriminant rule</a></li>
<li class="chapter" data-level="1.4" data-path="1-4-probability-of-misclassification.html"><a href="1-4-probability-of-misclassification.html"><i class="fa fa-check"></i><b>1.4</b> Probability of misclassification</a></li>
</ul></li>
<li class="divider"></li>
<li> <a href="https://rich-d-wilkinson.github.io/teaching.html" target="blank">University of Nottingham</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Multivariate Statistics</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="sample-lda" class="section level2">
<h2><span class="header-section-number">1.2</span> The sample ML discriminant rule</h2>
<p>To use the ML discriminant rule, above, we need to know the model parameters for each group. In reality, we often do not know these parameters but we can estimate them from ``training’’ data. Training data typically consists of samples <span class="math inline">\(\mathbf x_{1,j}, \ldots, \mathbf x_{n_j,j}\)</span> known to be from population <span class="math inline">\(\Pi_j\)</span> (<span class="math inline">\(j=1,\ldots ,g\)</span>). Note that there are <span class="math inline">\(n_j\)</span> observations from population <span class="math inline">\(\Pi_j\)</span>.</p>
<p>For simplicity, we shall assume that the populations have multivariate normal distributions with different means <span class="math inline">\({\boldsymbol{\mu}}_j\)</span>, <span class="math inline">\(j=1,\ldots,g\)</span>, and the same covariance matrix, <span class="math inline">\(\boldsymbol{\Sigma}\)</span>. Let <span class="math inline">\(\bar{\mathbf x}_j\)</span> and <span class="math inline">\(\mathbf S_j\)</span> be the sample mean and sample covariance matrix for the <span class="math inline">\(j\)</span>th group. Then <span class="math inline">\(\bar{\mathbf x}_j\)</span> is an unbiased estimate of <span class="math inline">\({\boldsymbol{\mu}}_j\)</span> and
<span class="math display">\[\mathbf S_u = \frac{1}{n-g} \sum_{k=1}^g n_k \mathbf S_k\]</span>
is an unbiased estimate of <span class="math inline">\(\boldsymbol{\Sigma}\)</span> where <span class="math inline">\(n = n_1 + n_2 + \ldots + n_g\)</span>. Note that this is not the same as the covariance of all the <span class="math inline">\(\mathbf x_i\)</span>.</p>
<p>The sample ML discriminant rule is then defined by substituting these estimates into <a href="1-1-lda-ML.html#prp:nine1">1.1</a>.</p>

<div class="definition">
<span id="def:sampleML" class="definition"><strong>Definition 1.2  </strong></span>If cases in population <span class="math inline">\(\Pi_k\)</span> have a <span class="math inline">\(N_p({\boldsymbol{\mu}}_k,\boldsymbol{\Sigma})\)</span> distribution, then the <strong>sample ML discriminant rule</strong> is
<span class="math display">\[d(\mathbf z) = \arg \min_k (\mathbf z-\bar{\mathbf x}_k)^\top \mathbf S_u^{-1} (\mathbf z-\bar{\mathbf x}_k).\]</span>
</div>

<p>NEEDED?????? DEPENDS IF I DO MORE THAN 2 groupS??</p>
<p>In the case <span class="math inline">\(g=2\)</span>, the rule allocates <span class="math inline">\(\mathbf z\)</span> to <span class="math inline">\(\Pi_1\)</span> if and only if
<span class="math display">\[\hat{\mathbf a}^\top (\mathbf z- \hat{\mathbf h}) &gt; 0\]</span>
where <span class="math inline">\(\hat{\mathbf a} = \mathbf S_u^{-1} (\bar{\mathbf x}_1 - \bar{\mathbf x}_2)\)</span>, <span class="math inline">\(\hat{\mathbf h} = \frac{1}{2} (\bar{\mathbf x}_1 + \bar{\mathbf x}_2)\)</span> and <span class="math inline">\(\mathbf S_u\)</span>, the pooled estimate of <span class="math inline">\(\boldsymbol{\Sigma}\)</span>, is given by
<span class="math display">\[ \mathbf S_u =  \frac{1}{n_1 + n_2 -2} (n_1 \mathbf S_1 + n_2 \mathbf S_2 ).\]</span></p>
<p>This is analogous to the Corollary following Proposition <a href="1-1-lda-ML.html#prp:nine1">1.1</a>.</p>
<div id="lda-with-the-iris-data" class="section level4 unnumbered">
<h4>LDA with the Iris data</h4>
<p>Let’s consider doing LDA with the iris data. To begin with, lets use just the setosa and virginica species so that we only have <span class="math inline">\(g=2\)</span> populations. We will also just use the sepal measurements so that <span class="math inline">\(p=2\)</span>. If we plot the data, we can see that the two populations should be easy to classify using just these two measurements:</p>
<p><img src="09-lda_files/figure-html/unnamed-chunk-8-1.png" width="672" /></p>
<p>The sample means and variances for each group are</p>
<p><span class="math display">\[\begin{eqnarray*}
\bar{\mathbf x}_s = \begin{pmatrix}5.01 \\3.43 \\\end{pmatrix} &amp;\qquad&amp; \bar{\mathbf x}_v = \begin{pmatrix}6.59 \\2.97 \\\end{pmatrix} \\
\mathbf S_s = \begin{pmatrix}0.124&amp;0.0992 \\0.0992&amp;0.144 \\\end{pmatrix} &amp;\qquad&amp; \mathbf S_v =\begin{pmatrix}0.404&amp;0.0938 \\0.0938&amp;0.104 \\\end{pmatrix}
\end{eqnarray*}\]</span>
where the <span class="math inline">\(s\)</span> subscript gives the values for setosa, and <span class="math inline">\(v\)</span> for virginica.</p>
<p>We have data on <span class="math inline">\(n=50\)</span> flowers in each population. Hence,
<span class="math display">\[\begin{eqnarray*}
\mathbf S_u &amp;=&amp; \frac{1}{50+50-2} \left(50 \mathbf S_s + 50 \mathbf S_v \right)= \begin{pmatrix}0.27&amp;0.0985 \\0.0985&amp;0.126 \\\end{pmatrix}, \\
\bar{\mathbf x}_s - \bar{\mathbf x}_v &amp;=&amp; \begin{pmatrix}-1.58 \\0.454 \\\end{pmatrix}, \\
\hat{\mathbf h} &amp;=&amp; \frac{1}{2} (\bar{\mathbf x}_s + \bar{\mathbf x}_v) = \begin{pmatrix}5.797 \\3.201 \\\end{pmatrix},
\end{eqnarray*}\]</span>
and
<span class="math display">\[\hat{\mathbf a} = \mathbf S_u^{-1} (\bar{\mathbf x}_s - \bar{\mathbf x}_v) = \begin{pmatrix}5.18&amp;-4.04 \\-4.04&amp;11.1 \\\end{pmatrix} \begin{pmatrix}-1.58 \\0.454 \\\end{pmatrix} = \begin{pmatrix}-10.031 \\11.407 \\\end{pmatrix}.\]</span></p>
<p>The sample ML discriminant rule allocates a new observation\
<span class="math inline">\(\mathbf z= (z_1, z_2)^\top\)</span> to <span class="math inline">\(\Pi_{\mbox{setosa}}\)</span> if and only if
<span class="math display">\[ \hat{\mathbf a}^\top (\mathbf z- \hat{\mathbf h}) = \begin{pmatrix}-10.031&amp;11.407 \\\end{pmatrix} \begin{pmatrix} z_1 - 5.797 \\ z_2 - 3.201 \end{pmatrix} &gt; 0.\]</span></p>
<p>If we draw on the line defined by
<span class="math display">\[\mathbf a^\top (\mathbf z-\mathbf h)=0\]</span>
which can be written as
<span class="math display">\[z_2 = \frac{1}{a_2}(\mathbf a^\top\mathbf h-a_1z_1) = -1.8963517 - -0.8793085 z_1\]</span>
we can see this line clearly separates the two species of iris.</p>
<p><img src="09-lda_files/figure-html/unnamed-chunk-10-1.png" width="672" /></p>
<p>For example, if an iris had a sepal length of 5.8 and a sepal width of 2.5 then</p>
<p><span class="math display">\[ \hat{\mathbf a}^\top (\mathbf z- \hat{\mathbf h}) = \begin{pmatrix}-10.031&amp;11.407 \\\end{pmatrix} \begin{pmatrix} 5.8 - 5.797 \\ 2.5 - 3.201 \end{pmatrix} =-8.0267032 &lt; 0,\]</span>
and so we would allocate this iris to virginia.</p>
<p>As always, there is an R command to do this work for us. The command is called <code>lda</code> and it is in the <code>MASS</code> R package.</p>
<div class="sourceCode" id="cb1"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1-1" data-line-number="1"><span class="kw">library</span>(MASS)</a>
<a class="sourceLine" id="cb1-2" data-line-number="2"><span class="kw">library</span>(dplyr)</a>
<a class="sourceLine" id="cb1-3" data-line-number="3">iris2 &lt;-<span class="st"> </span>iris <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">filter</span>(Species <span class="op">!=</span><span class="st"> &quot;versicolor&quot;</span>) <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb1-4" data-line-number="4"><span class="st">  </span>dplyr<span class="op">::</span><span class="kw">select</span>(Sepal.Length, Sepal.Width, Species)</a>
<a class="sourceLine" id="cb1-5" data-line-number="5">iris.lda1 &lt;-<span class="st"> </span><span class="kw">lda</span>(Species <span class="op">~</span><span class="st"> </span>., iris2)</a>
<a class="sourceLine" id="cb1-6" data-line-number="6"></a>
<a class="sourceLine" id="cb1-7" data-line-number="7"><span class="co"># lda(Species ~ Sepal.Length+Sepal.Width, iris2) </span></a>
<a class="sourceLine" id="cb1-8" data-line-number="8">  <span class="co">#  does the same thing</span></a>
<a class="sourceLine" id="cb1-9" data-line-number="9">iris.lda1</a></code></pre></div>
<pre><code>## Call:
## lda(Species ~ ., data = iris2)
## 
## Prior probabilities of groups:
##    setosa virginica 
##       0.5       0.5 
## 
## Group means:
##           Sepal.Length Sepal.Width
## setosa           5.006       3.428
## virginica        6.588       2.974
## 
## Coefficients of linear discriminants:
##                    LD1
## Sepal.Length  2.208596
## Sepal.Width  -2.511742</code></pre>
<p>You can see that this has computed the group means, and the lda coefficients. The coefficients are different to the ones we compute for <span class="math inline">\(\mathbf a\)</span>, but have the same ratio, which is all that matters for us.</p>
<div class="sourceCode" id="cb3"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb3-1" data-line-number="1">iris.coef &lt;-<span class="st"> </span><span class="kw">coef</span>(iris.lda1)</a>
<a class="sourceLine" id="cb3-2" data-line-number="2">iris.coef[<span class="dv">1</span>]<span class="op">/</span>iris.coef[<span class="dv">2</span>]</a></code></pre></div>
<pre><code>## [1] -0.8793085</code></pre>
<div class="sourceCode" id="cb5"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb5-1" data-line-number="1">a[<span class="dv">1</span>]<span class="op">/</span>a[<span class="dv">2</span>]</a></code></pre></div>
<pre><code>## [1] -0.8793085</code></pre>
<p>The output also includes the estimate of the prior probabilities….?????????????????????????????????</p>
<p>We can make predictions about new points:</p>
<div class="sourceCode" id="cb7"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb7-1" data-line-number="1">z=<span class="kw">data.frame</span>(<span class="dt">Sepal.Length=</span><span class="fl">5.8</span>, <span class="dt">Sepal.Width=</span><span class="fl">2.5</span>)</a>
<a class="sourceLine" id="cb7-2" data-line-number="2"><span class="kw">predict</span>(iris.lda1, z)</a></code></pre></div>
<pre><code>## $class
## [1] virginica
## Levels: setosa versicolor virginica
## 
## $posterior
##         setosa virginica
## 1 0.0002771946 0.9997228
## 
## $x
##        LD1
## 1 1.767357</code></pre>
<p>and in this case it gives us the</p>
<div class="sourceCode" id="cb9"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb9-1" data-line-number="1"><span class="kw">library</span>(mvtnorm)</a></code></pre></div>
<pre><code>## Warning: package &#39;mvtnorm&#39; was built under R version 3.6.2</code></pre>
<pre><code>## 
## Attaching package: &#39;mvtnorm&#39;</code></pre>
<pre><code>## The following objects are masked from &#39;package:mixtools&#39;:
## 
##     dmvnorm, rmvnorm</code></pre>
<div class="sourceCode" id="cb13"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb13-1" data-line-number="1">l_set=mvtnorm<span class="op">::</span><span class="kw">dmvnorm</span>(z, <span class="dt">mean =</span> xbar_set, <span class="dt">sigma =</span> S_set)</a>
<a class="sourceLine" id="cb13-2" data-line-number="2">l_vir=mvtnorm<span class="op">::</span><span class="kw">dmvnorm</span>(z, <span class="dt">mean =</span> xbar_vir, <span class="dt">sigma =</span> S_vir)</a>
<a class="sourceLine" id="cb13-3" data-line-number="3"><span class="kw">c</span>(l_set,l_vir)<span class="op">/</span>(l_set<span class="op">+</span>l_vir)</a></code></pre></div>
<pre><code>##         1         1 
## 3.513e-09 1.000e+00</code></pre>
<p>Note that we could also</p>
<!--##### Example: Student marks

Consider the G11PRB and G11STA module marks for $n_1 = 98$ students on G100 and $n_2 = 46$ students on G103.  The sample means and variances for each group are given by
\begin{eqnarray*}
\bar{\bx}_1 = \begin{pmatrix} 60.582 \\ 62.786 \end{pmatrix} &\qquad& \bar{\bx}_2 = \begin{pmatrix} 64.761 \\ 60.457 \end{pmatrix} \\
\bS_1 = \begin{pmatrix} 201.04 & 129.56 \\ 129.56 & 316.21 \end{pmatrix} &\qquad& \bS_2 = \begin{pmatrix} 229.88 & 177.02 \\ 177.02 & 354.16 \end{pmatrix}
\end{eqnarray*}
Hence,
\begin{eqnarray*}
\bS_u &=& \frac{1}{98+46-2} \lb 98 \bS_1 + 46 \bS_2 \rb = \begin{pmatrix} 213.21 & 146.76 \\ 146.76 & 332.96 \end{pmatrix}, \\
\bar{\bx}_1 - \bar{\bx}_2 &=& \begin{pmatrix} -4.179 \\ 2.329 \end{pmatrix}, \\
\hat{\bh} &=& \frac{1}{2} (\bar{\bx}_1 + \bar{\bx}_2) = \begin{pmatrix} 62.671 \\ 61.621 \end{pmatrix},
\end{eqnarray*}
and
$$\hat{\ba} = \bS_u^{-1} (\bar{\bx}_1 - \bar{\bx}_2) = \begin{pmatrix} 0.0067 & -0.0030 \\ -0.0030 & 0.0043 \end{pmatrix} \begin{pmatrix} -4.179 \\ 2.329 \end{pmatrix} = \begin{pmatrix} -0.035 \\ 0.022 \end{pmatrix}.$$

The sample ML discriminant rule allocates a new observation\\
 $\bz = (z_1, z_2)^\top$ to $\Pi_1$ if and only if
$$ \hat{\ba}^\top (\bz - \hat{\bh}) = \begin{pmatrix} -0.035 & 0.022 \end{pmatrix} \begin{pmatrix} z_1 - 62.671 \\ z_2 - 61.621 \end{pmatrix} > 0.$$

For example, if a student on this year's course scores 80 on G11PRB and 60 on G11STA then
$$ \hat{\ba}^\top (\bz - \hat{\bh}) = \begin{pmatrix} -0.035 & 0.022 \end{pmatrix} \begin{pmatrix} 80 - 62.671 \\ 60 - 61.621 \end{pmatrix} = -0.644 < 0,$$
and so we would allocate this student to G103.  The boundary, where $\hat{\ba}^\top (\bz - \hat{\bh}) = 0$, is shown below.

FIX
<!--
\begin{center}
\includegraphics[width=12cm,angle=0]{sample_mldr1.pdf}
\end{center}
-->
<!--Note that the boundary line passes half-way between the two sample means.  In this example it is difficult to discriminate accurately between G100 and G103 because there is a large overlap between the two populations.

We could extend the example to include, say, students on GL11.  Here the boundary between the three populations is piece-wise linear and they meet at a common point.

FIX-->
<!--
\begin{center}
\includegraphics[width=12cm,angle=0]{sample_mldr2.pdf}
\end{center}
-->
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="1-1-lda-ML.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="1-3-FLDA.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["MultivariateStatistics.pdf"],
"toc": {
"collapse": "section"
},
"pandoc_args": "--top-level-division=[chapter|part]"
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
