<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>7.1 Definition and Properties of the MVN | Multivariate Statistics</title>
  <meta name="description" content="The lecture notes for MATH3030/4068: Multivariate Analysis / Applied Multivariate Statistics" />
  <meta name="generator" content="bookdown 0.20.6 and GitBook 2.6.7" />

  <meta property="og:title" content="7.1 Definition and Properties of the MVN | Multivariate Statistics" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="The lecture notes for MATH3030/4068: Multivariate Analysis / Applied Multivariate Statistics" />
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="7.1 Definition and Properties of the MVN | Multivariate Statistics" />
  
  <meta name="twitter:description" content="The lecture notes for MATH3030/4068: Multivariate Analysis / Applied Multivariate Statistics" />
  

<meta name="author" content="Prof.Â Richard Wilkinson" />



  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="7-multinormal.html"/>
<link rel="next" href="7-2-the-wishart-distribution.html"/>
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />









<script src="libs/kePrint-0.0.1/kePrint.js"></script>
<link href="libs/lightable-0.0.1/lightable.css" rel="stylesheet" />


<style type="text/css">
a.sourceLine { display: inline-block; line-height: 1.25; }
a.sourceLine { pointer-events: none; color: inherit; text-decoration: inherit; }
a.sourceLine:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode { white-space: pre; position: relative; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
code.sourceCode { white-space: pre-wrap; }
a.sourceLine { text-indent: -1em; padding-left: 1em; }
}
pre.numberSource a.sourceLine
  { position: relative; left: -4em; }
pre.numberSource a.sourceLine::before
  { content: attr(data-line-number);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; pointer-events: all; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {  }
@media screen {
a.sourceLine::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Applied multivariate statistics</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Introduction</a></li>
<li class="chapter" data-level="" data-path="part-i-prerequisites.html"><a href="part-i-prerequisites.html"><i class="fa fa-check"></i>PART I: Prerequisites</a></li>
<li class="chapter" data-level="1" data-path="1-stat-prelim.html"><a href="1-stat-prelim.html"><i class="fa fa-check"></i><b>1</b> Statistical Preliminaries</a><ul>
<li class="chapter" data-level="1.1" data-path="1-1-notation.html"><a href="1-1-notation.html"><i class="fa fa-check"></i><b>1.1</b> Notation</a><ul>
<li class="chapter" data-level="1.1.1" data-path="1-1-notation.html"><a href="1-1-notation.html#example-datasets"><i class="fa fa-check"></i><b>1.1.1</b> Example datasets</a></li>
<li class="chapter" data-level="1.1.2" data-path="1-1-notation.html"><a href="1-1-notation.html#aims-of-multivariate-data-analysis"><i class="fa fa-check"></i><b>1.1.2</b> Aims of multivariate data analysis</a></li>
</ul></li>
<li class="chapter" data-level="1.2" data-path="1-2-exploratory-data-analysis-eda.html"><a href="1-2-exploratory-data-analysis-eda.html"><i class="fa fa-check"></i><b>1.2</b> Exploratory data analysis (EDA)</a><ul>
<li class="chapter" data-level="1.2.1" data-path="1-2-exploratory-data-analysis-eda.html"><a href="1-2-exploratory-data-analysis-eda.html#data-visualization"><i class="fa fa-check"></i><b>1.2.1</b> Data visualization</a></li>
<li class="chapter" data-level="1.2.2" data-path="1-2-exploratory-data-analysis-eda.html"><a href="1-2-exploratory-data-analysis-eda.html#summary-statistics"><i class="fa fa-check"></i><b>1.2.2</b> Summary statistics</a></li>
</ul></li>
<li class="chapter" data-level="1.3" data-path="1-3-random-vectors-and-matrices.html"><a href="1-3-random-vectors-and-matrices.html"><i class="fa fa-check"></i><b>1.3</b> Random vectors and matrices</a><ul>
<li class="chapter" data-level="1.3.1" data-path="1-3-random-vectors-and-matrices.html"><a href="1-3-random-vectors-and-matrices.html#estimators"><i class="fa fa-check"></i><b>1.3.1</b> Estimators</a></li>
</ul></li>
<li class="chapter" data-level="1.4" data-path="1-4-computer-tasks.html"><a href="1-4-computer-tasks.html"><i class="fa fa-check"></i><b>1.4</b> Computer tasks</a></li>
<li class="chapter" data-level="1.5" data-path="1-5-exercises.html"><a href="1-5-exercises.html"><i class="fa fa-check"></i><b>1.5</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="2-linalg-prelim.html"><a href="2-linalg-prelim.html"><i class="fa fa-check"></i><b>2</b> Review of linear algebra</a><ul>
<li class="chapter" data-level="2.1" data-path="2-1-linalg-basics.html"><a href="2-1-linalg-basics.html"><i class="fa fa-check"></i><b>2.1</b> Basics</a><ul>
<li class="chapter" data-level="2.1.1" data-path="2-1-linalg-basics.html"><a href="2-1-linalg-basics.html#notation-1"><i class="fa fa-check"></i><b>2.1.1</b> Notation</a></li>
<li class="chapter" data-level="2.1.2" data-path="2-1-linalg-basics.html"><a href="2-1-linalg-basics.html#elementary-matrix-operations"><i class="fa fa-check"></i><b>2.1.2</b> Elementary matrix operations</a></li>
<li class="chapter" data-level="2.1.3" data-path="2-1-linalg-basics.html"><a href="2-1-linalg-basics.html#special-matrices"><i class="fa fa-check"></i><b>2.1.3</b> Special matrices</a></li>
<li class="chapter" data-level="2.1.4" data-path="2-1-linalg-basics.html"><a href="2-1-linalg-basics.html#vectordiff"><i class="fa fa-check"></i><b>2.1.4</b> Vector Differentiation</a></li>
</ul></li>
<li class="chapter" data-level="2.2" data-path="2-2-linalg-vecspaces.html"><a href="2-2-linalg-vecspaces.html"><i class="fa fa-check"></i><b>2.2</b> Vector spaces</a><ul>
<li class="chapter" data-level="2.2.1" data-path="2-2-linalg-vecspaces.html"><a href="2-2-linalg-vecspaces.html#linear-independence"><i class="fa fa-check"></i><b>2.2.1</b> Linear independence</a></li>
<li class="chapter" data-level="2.2.2" data-path="2-2-linalg-vecspaces.html"><a href="2-2-linalg-vecspaces.html#colsspace"><i class="fa fa-check"></i><b>2.2.2</b> Row and column spaces</a></li>
<li class="chapter" data-level="2.2.3" data-path="2-2-linalg-vecspaces.html"><a href="2-2-linalg-vecspaces.html#linear-transformations"><i class="fa fa-check"></i><b>2.2.3</b> Linear transformations</a></li>
</ul></li>
<li class="chapter" data-level="2.3" data-path="2-3-linalg-innerprod.html"><a href="2-3-linalg-innerprod.html"><i class="fa fa-check"></i><b>2.3</b> Inner product spaces</a><ul>
<li class="chapter" data-level="2.3.1" data-path="2-3-linalg-innerprod.html"><a href="2-3-linalg-innerprod.html#normed"><i class="fa fa-check"></i><b>2.3.1</b> Distances, and angles</a></li>
<li class="chapter" data-level="2.3.2" data-path="2-3-linalg-innerprod.html"><a href="2-3-linalg-innerprod.html#orthogonal-matrices"><i class="fa fa-check"></i><b>2.3.2</b> Orthogonal matrices</a></li>
<li class="chapter" data-level="2.3.3" data-path="2-3-linalg-innerprod.html"><a href="2-3-linalg-innerprod.html#projection-matrix"><i class="fa fa-check"></i><b>2.3.3</b> Projections</a></li>
</ul></li>
<li class="chapter" data-level="2.4" data-path="2-4-centering-matrix.html"><a href="2-4-centering-matrix.html"><i class="fa fa-check"></i><b>2.4</b> The Centering Matrix</a></li>
<li class="chapter" data-level="2.5" data-path="2-5-tasks-ch2.html"><a href="2-5-tasks-ch2.html"><i class="fa fa-check"></i><b>2.5</b> Computer tasks</a></li>
<li class="chapter" data-level="2.6" data-path="2-6-exercises-ch2.html"><a href="2-6-exercises-ch2.html"><i class="fa fa-check"></i><b>2.6</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="3-linalg-decomp.html"><a href="3-linalg-decomp.html"><i class="fa fa-check"></i><b>3</b> Matrix decompositions</a><ul>
<li class="chapter" data-level="3.1" data-path="3-1-matrix-matrix.html"><a href="3-1-matrix-matrix.html"><i class="fa fa-check"></i><b>3.1</b> Matrix-matrix products</a></li>
<li class="chapter" data-level="3.2" data-path="3-2-spectraleigen-decomposition.html"><a href="3-2-spectraleigen-decomposition.html"><i class="fa fa-check"></i><b>3.2</b> Spectral/eigen decomposition</a><ul>
<li class="chapter" data-level="3.2.1" data-path="3-2-spectraleigen-decomposition.html"><a href="3-2-spectraleigen-decomposition.html#eigenvalues-and-eigenvectors"><i class="fa fa-check"></i><b>3.2.1</b> Eigenvalues and eigenvectors</a></li>
<li class="chapter" data-level="3.2.2" data-path="3-2-spectraleigen-decomposition.html"><a href="3-2-spectraleigen-decomposition.html#spectral-decomposition"><i class="fa fa-check"></i><b>3.2.2</b> Spectral decomposition</a></li>
<li class="chapter" data-level="3.2.3" data-path="3-2-spectraleigen-decomposition.html"><a href="3-2-spectraleigen-decomposition.html#matrixroots"><i class="fa fa-check"></i><b>3.2.3</b> Matrix square roots</a></li>
</ul></li>
<li class="chapter" data-level="3.3" data-path="3-3-linalg-SVD.html"><a href="3-3-linalg-SVD.html"><i class="fa fa-check"></i><b>3.3</b> Singular Value Decomposition (SVD)</a><ul>
<li class="chapter" data-level="3.3.1" data-path="3-3-linalg-SVD.html"><a href="3-3-linalg-SVD.html#examples"><i class="fa fa-check"></i><b>3.3.1</b> Examples</a></li>
</ul></li>
<li class="chapter" data-level="3.4" data-path="3-4-svdopt.html"><a href="3-4-svdopt.html"><i class="fa fa-check"></i><b>3.4</b> SVD optimization results</a></li>
<li class="chapter" data-level="3.5" data-path="3-5-low-rank-approximation.html"><a href="3-5-low-rank-approximation.html"><i class="fa fa-check"></i><b>3.5</b> Low-rank approximation</a><ul>
<li class="chapter" data-level="3.5.1" data-path="3-5-low-rank-approximation.html"><a href="3-5-low-rank-approximation.html#matrix-norms"><i class="fa fa-check"></i><b>3.5.1</b> Matrix norms</a></li>
<li class="chapter" data-level="3.5.2" data-path="3-5-low-rank-approximation.html"><a href="3-5-low-rank-approximation.html#eckart-young-mirsky-theorem"><i class="fa fa-check"></i><b>3.5.2</b> Eckart-Young-Mirsky Theorem</a></li>
<li class="chapter" data-level="3.5.3" data-path="3-5-low-rank-approximation.html"><a href="3-5-low-rank-approximation.html#example-image-compression"><i class="fa fa-check"></i><b>3.5.3</b> Example: image compression</a></li>
</ul></li>
<li class="chapter" data-level="3.6" data-path="3-6-tasks-ch3.html"><a href="3-6-tasks-ch3.html"><i class="fa fa-check"></i><b>3.6</b> Computer tasks</a></li>
<li class="chapter" data-level="3.7" data-path="3-7-exercises-ch3.html"><a href="3-7-exercises-ch3.html"><i class="fa fa-check"></i><b>3.7</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="part-ii-dimension-reduction-methods.html"><a href="part-ii-dimension-reduction-methods.html"><i class="fa fa-check"></i>PART II: Dimension reduction methods</a><ul>
<li class="chapter" data-level="" data-path="part-ii-dimension-reduction-methods.html"><a href="part-ii-dimension-reduction-methods.html#a-warning"><i class="fa fa-check"></i>A warning</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="4-pca.html"><a href="4-pca.html"><i class="fa fa-check"></i><b>4</b> Principal component analysis (PCA)</a><ul>
<li class="chapter" data-level="4.1" data-path="4-1-pca-an-informal-introduction.html"><a href="4-1-pca-an-informal-introduction.html"><i class="fa fa-check"></i><b>4.1</b> PCA: an informal introduction</a><ul>
<li class="chapter" data-level="4.1.1" data-path="4-1-pca-an-informal-introduction.html"><a href="4-1-pca-an-informal-introduction.html#notation-recap"><i class="fa fa-check"></i><b>4.1.1</b> Notation recap</a></li>
<li class="chapter" data-level="4.1.2" data-path="4-1-pca-an-informal-introduction.html"><a href="4-1-pca-an-informal-introduction.html#first-principal-component"><i class="fa fa-check"></i><b>4.1.2</b> First principal component</a></li>
<li class="chapter" data-level="4.1.3" data-path="4-1-pca-an-informal-introduction.html"><a href="4-1-pca-an-informal-introduction.html#second-principal-component"><i class="fa fa-check"></i><b>4.1.3</b> Second principal component</a></li>
<li class="chapter" data-level="4.1.4" data-path="4-1-pca-an-informal-introduction.html"><a href="4-1-pca-an-informal-introduction.html#geometric-interpretation-1"><i class="fa fa-check"></i><b>4.1.4</b> Geometric interpretation</a></li>
<li class="chapter" data-level="4.1.5" data-path="4-1-pca-an-informal-introduction.html"><a href="4-1-pca-an-informal-introduction.html#example"><i class="fa fa-check"></i><b>4.1.5</b> Example</a></li>
<li class="chapter" data-level="4.1.6" data-path="4-1-pca-an-informal-introduction.html"><a href="4-1-pca-an-informal-introduction.html#example-iris"><i class="fa fa-check"></i><b>4.1.6</b> Example: Iris</a></li>
</ul></li>
<li class="chapter" data-level="4.2" data-path="4-2-pca-a-formal-description-with-proofs.html"><a href="4-2-pca-a-formal-description-with-proofs.html"><i class="fa fa-check"></i><b>4.2</b> PCA: a formal description with proofs</a><ul>
<li class="chapter" data-level="4.2.1" data-path="4-2-pca-a-formal-description-with-proofs.html"><a href="4-2-pca-a-formal-description-with-proofs.html#properties-of-principal-components"><i class="fa fa-check"></i><b>4.2.1</b> Properties of principal components</a></li>
<li class="chapter" data-level="4.2.2" data-path="4-2-pca-a-formal-description-with-proofs.html"><a href="4-2-pca-a-formal-description-with-proofs.html#example-football"><i class="fa fa-check"></i><b>4.2.2</b> Example: Football</a></li>
<li class="chapter" data-level="4.2.3" data-path="4-2-pca-a-formal-description-with-proofs.html"><a href="4-2-pca-a-formal-description-with-proofs.html#pcawithR"><i class="fa fa-check"></i><b>4.2.3</b> PCA based on <span class="math inline">\(\boldsymbol R\)</span> versus PCA based on <span class="math inline">\(\boldsymbol S\)</span></a></li>
<li class="chapter" data-level="4.2.4" data-path="4-2-pca-a-formal-description-with-proofs.html"><a href="4-2-pca-a-formal-description-with-proofs.html#population-pca"><i class="fa fa-check"></i><b>4.2.4</b> Population PCA</a></li>
<li class="chapter" data-level="4.2.5" data-path="4-2-pca-a-formal-description-with-proofs.html"><a href="4-2-pca-a-formal-description-with-proofs.html#pca-under-transformations-of-variables"><i class="fa fa-check"></i><b>4.2.5</b> PCA under transformations of variables</a></li>
</ul></li>
<li class="chapter" data-level="4.3" data-path="4-3-an-alternative-view-of-pca.html"><a href="4-3-an-alternative-view-of-pca.html"><i class="fa fa-check"></i><b>4.3</b> An alternative view of PCA</a><ul>
<li class="chapter" data-level="4.3.1" data-path="4-3-an-alternative-view-of-pca.html"><a href="4-3-an-alternative-view-of-pca.html#example-mnist-handwritten-digits"><i class="fa fa-check"></i><b>4.3.1</b> Example: MNIST handwritten digits</a></li>
</ul></li>
<li class="chapter" data-level="4.4" data-path="4-4-pca-comptask.html"><a href="4-4-pca-comptask.html"><i class="fa fa-check"></i><b>4.4</b> Computer tasks</a></li>
<li class="chapter" data-level="4.5" data-path="4-5-exercises-1.html"><a href="4-5-exercises-1.html"><i class="fa fa-check"></i><b>4.5</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="5-cca.html"><a href="5-cca.html"><i class="fa fa-check"></i><b>5</b> Canonical Correlation Analysis</a><ul>
<li class="chapter" data-level="5.1" data-path="5-1-cca1.html"><a href="5-1-cca1.html"><i class="fa fa-check"></i><b>5.1</b> The first pair of canonical variables</a><ul>
<li class="chapter" data-level="5.1.1" data-path="5-1-cca1.html"><a href="5-1-cca1.html#example-premier-league-football"><i class="fa fa-check"></i><b>5.1.1</b> Example: Premier league football</a></li>
</ul></li>
<li class="chapter" data-level="5.2" data-path="5-2-the-full-set-of-canonical-correlations.html"><a href="5-2-the-full-set-of-canonical-correlations.html"><i class="fa fa-check"></i><b>5.2</b> The full set of canonical correlations</a></li>
<li class="chapter" data-level="5.3" data-path="5-3-properties.html"><a href="5-3-properties.html"><i class="fa fa-check"></i><b>5.3</b> Properties</a><ul>
<li class="chapter" data-level="5.3.1" data-path="5-3-properties.html"><a href="5-3-properties.html#connection-with-linear-regression-when-q1"><i class="fa fa-check"></i><b>5.3.1</b> Connection with linear regression when <span class="math inline">\(q=1\)</span></a></li>
<li class="chapter" data-level="5.3.2" data-path="5-3-properties.html"><a href="5-3-properties.html#invarianceequivariance-properties-of-cca"><i class="fa fa-check"></i><b>5.3.2</b> Invariance/equivariance properties of CCA</a></li>
</ul></li>
<li class="chapter" data-level="5.4" data-path="5-4-example-1.html"><a href="5-4-example-1.html"><i class="fa fa-check"></i><b>5.4</b> Example?</a></li>
<li class="chapter" data-level="5.5" data-path="5-5-exercises-2.html"><a href="5-5-exercises-2.html"><i class="fa fa-check"></i><b>5.5</b> Exercises</a></li>
<li class="chapter" data-level="5.6" data-path="5-6-computer-tasks-1.html"><a href="5-6-computer-tasks-1.html"><i class="fa fa-check"></i><b>5.6</b> Computer tasks</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="6-mds.html"><a href="6-mds.html"><i class="fa fa-check"></i><b>6</b> Multidimensional Scaling</a><ul>
<li class="chapter" data-level="6.1" data-path="6-1-classical-multidimensional-scaling.html"><a href="6-1-classical-multidimensional-scaling.html"><i class="fa fa-check"></i><b>6.1</b> Classical Multidimensional Scaling</a><ul>
<li class="chapter" data-level="6.1.1" data-path="6-1-classical-multidimensional-scaling.html"><a href="6-1-classical-multidimensional-scaling.html#example-2"><i class="fa fa-check"></i><b>6.1.1</b> Example</a></li>
</ul></li>
<li class="chapter" data-level="6.2" data-path="6-2-principal-coordinates.html"><a href="6-2-principal-coordinates.html"><i class="fa fa-check"></i><b>6.2</b> Principal Coordinates</a></li>
<li class="chapter" data-level="6.3" data-path="6-3-similarity-measures.html"><a href="6-3-similarity-measures.html"><i class="fa fa-check"></i><b>6.3</b> Similarity measures</a></li>
<li class="chapter" data-level="6.4" data-path="6-4-exercises-3.html"><a href="6-4-exercises-3.html"><i class="fa fa-check"></i><b>6.4</b> Exercises</a></li>
<li class="chapter" data-level="6.5" data-path="6-5-computer-tasks-2.html"><a href="6-5-computer-tasks-2.html"><i class="fa fa-check"></i><b>6.5</b> Computer Tasks</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="part-iii-inference-using-the-multivariate-normal-distribution-mvn.html"><a href="part-iii-inference-using-the-multivariate-normal-distribution-mvn.html"><i class="fa fa-check"></i>Part III: Inference using the Multivariate Normal Distribution (MVN)</a></li>
<li class="chapter" data-level="7" data-path="7-multinormal.html"><a href="7-multinormal.html"><i class="fa fa-check"></i><b>7</b> The Multivariate Normal Distribution</a><ul>
<li class="chapter" data-level="7.1" data-path="7-1-definition-and-properties-of-the-mvn.html"><a href="7-1-definition-and-properties-of-the-mvn.html"><i class="fa fa-check"></i><b>7.1</b> Definition and Properties of the MVN</a><ul>
<li class="chapter" data-level="7.1.1" data-path="7-1-definition-and-properties-of-the-mvn.html"><a href="7-1-definition-and-properties-of-the-mvn.html#transformations"><i class="fa fa-check"></i><b>7.1.1</b> Transformations</a></li>
<li class="chapter" data-level="7.1.2" data-path="7-1-definition-and-properties-of-the-mvn.html"><a href="7-1-definition-and-properties-of-the-mvn.html#moment-generating-functions"><i class="fa fa-check"></i><b>7.1.2</b> Moment Generating Functions</a></li>
<li class="chapter" data-level="7.1.3" data-path="7-1-definition-and-properties-of-the-mvn.html"><a href="7-1-definition-and-properties-of-the-mvn.html#sampling-results-for-the-mvn"><i class="fa fa-check"></i><b>7.1.3</b> Sampling results for the MVN</a></li>
</ul></li>
<li class="chapter" data-level="7.2" data-path="7-2-the-wishart-distribution.html"><a href="7-2-the-wishart-distribution.html"><i class="fa fa-check"></i><b>7.2</b> The Wishart distribution</a><ul>
<li class="chapter" data-level="7.2.1" data-path="7-2-the-wishart-distribution.html"><a href="7-2-the-wishart-distribution.html#properties-1"><i class="fa fa-check"></i><b>7.2.1</b> Properties</a></li>
<li class="chapter" data-level="7.2.2" data-path="7-2-the-wishart-distribution.html"><a href="7-2-the-wishart-distribution.html#cochrans-theorem"><i class="fa fa-check"></i><b>7.2.2</b> Cochranâs theorem</a></li>
</ul></li>
<li class="chapter" data-level="7.3" data-path="7-3-hotellings-t2-distribution.html"><a href="7-3-hotellings-t2-distribution.html"><i class="fa fa-check"></i><b>7.3</b> Hotellingâs <span class="math inline">\(T^2\)</span> distribution</a></li>
<li class="chapter" data-level="7.4" data-path="7-4-exercises-4.html"><a href="7-4-exercises-4.html"><i class="fa fa-check"></i><b>7.4</b> Exercises</a></li>
<li class="chapter" data-level="7.5" data-path="7-5-computer-tasks-3.html"><a href="7-5-computer-tasks-3.html"><i class="fa fa-check"></i><b>7.5</b> Computer tasks</a></li>
</ul></li>
<li class="divider"></li>
<li> <a href="https://rich-d-wilkinson.github.io/teaching.html" target="blank">University of Nottingham</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Multivariate Statistics</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="definition-and-properties-of-the-mvn" class="section level2">
<h2><span class="header-section-number">7.1</span> Definition and Properties of the MVN</h2>

<div class="definition">
<span id="def:mvn" class="definition"><strong>Definition 7.1  </strong></span>A random vector <span class="math inline">\(\boldsymbol x=(x_1, \ldots , x_p)^\top\)</span> has a <span class="math inline">\(p\)</span>-dimensional MVN distribution if and only if <span class="math inline">\(\boldsymbol a^\top \boldsymbol x\)</span> is a univariate normal random variable for all fixed <span class="math inline">\(p \times 1\)</span> vectors <span class="math inline">\(\boldsymbol a\)</span>.
</div>

<p><strong>Notation</strong>: If <span class="math inline">\(\boldsymbol x\)</span> (<span class="math inline">\(p \times 1\)</span>) is MVN with mean <span class="math inline">\(\boldsymbol \mu\)</span> and covariance matrix <span class="math inline">\(\boldsymbol \Sigma\)</span> then we write
<span class="math display">\[ \boldsymbol x\sim N_p (\boldsymbol \mu, \boldsymbol \Sigma).\]</span></p>

<div class="proposition">
<span id="prp:six1" class="proposition"><strong>Proposition 7.1  </strong></span>If <span class="math inline">\(\stackrel{p \times 1}{\boldsymbol x}\)</span> is a multivariate normal random variable, then for each constant matrix <span class="math inline">\(\boldsymbol A\)</span> (<span class="math inline">\(q \times p\)</span>) and constant vector <span class="math inline">\(\boldsymbol c\)</span> (<span class="math inline">\(q \times 1\)</span>), <span class="math inline">\(\boldsymbol y= \boldsymbol A\boldsymbol x+ \boldsymbol c\)</span> has a <span class="math inline">\(q\)</span>-dimensional MVN.
</div>


<div class="proof">
 <span class="proof"><em>Proof. </em></span> Let <span class="math inline">\(\boldsymbol b\)</span> (<span class="math inline">\(q \times 1)\)</span> be a fixed vector. Then
<span class="math display">\[ \boldsymbol b^\top \boldsymbol y= \boldsymbol b^\top \boldsymbol A\boldsymbol x+ \boldsymbol b^\top \boldsymbol c= \boldsymbol a^\top \boldsymbol x+ \boldsymbol b^\top \boldsymbol c\]</span>
where <span class="math inline">\(\boldsymbol a^\top = \boldsymbol b^\top \boldsymbol A\)</span>. Now <span class="math inline">\(\boldsymbol a^\top \boldsymbol x\)</span> is univariate normal for all <span class="math inline">\(\boldsymbol a\)</span> since <span class="math inline">\(\boldsymbol x\)</span> is MVN. Therefore <span class="math inline">\(\boldsymbol b^\top \boldsymbol y\)</span> is univariate normal for all <span class="math inline">\(\boldsymbol b\)</span>, so <span class="math inline">\(\boldsymbol y\)</span> is MVN.
</div>


<div class="corollary">
<span id="cor:csix1" class="corollary"><strong>Corollary 7.1  </strong></span>Any subset of the components of a MVN vector <span class="math inline">\(\boldsymbol x\)</span> is also MVN.
</div>


<div class="definition">
<span id="def:mvnpdf" class="definition"><strong>Definition 7.2  </strong></span>If the population covariance matrix <span class="math inline">\(\boldsymbol \Sigma\)</span> (<span class="math inline">\(p \times p\)</span>) is positive definite (i.e.Â full rank), so that <span class="math inline">\(\boldsymbol \Sigma^{-1}\)</span> exists,
then the <strong>probability density function</strong> (pdf) of the MVN distribution is given by
<span class="math display">\[ f(\boldsymbol x) = \frac{1}{| 2 \pi \boldsymbol \Sigma|^{1/2}} \exp \left(-\frac{1}{2}(\boldsymbol x- \boldsymbol \mu)^\top \boldsymbol \Sigma^{-1} (\boldsymbol x- \boldsymbol \mu) \right).\]</span>
</div>

<p>If <span class="math inline">\(p=1\)</span>, so that <span class="math inline">\(\boldsymbol x= x\)</span>, <span class="math inline">\(\boldsymbol \mu= \mu\)</span> and <span class="math inline">\(\boldsymbol \Sigma= \sigma^2\)</span>, say, then the pdf simplifies to
<span class="math display">\[\begin{eqnarray*}
f(\boldsymbol x) &amp;=&amp; \frac{1}{|2 \pi \sigma^2|^{1/2}} \exp \left(-\frac{1}{2}(x - \mu) (\sigma^2)^{-1} (x - \mu) \right)\\
&amp;=&amp; \frac{1}{(2 \pi \sigma^2)^{1/2}} \exp \left(-\frac{1}{2 \sigma^2}(x - \mu)^2 \right)
\end{eqnarray*}\]</span>
which is the familiar pdf of the univariate normal distribution <span class="math inline">\(N(\mu,\sigma^2)\)</span>.</p>
<p>If <span class="math inline">\(p&gt;1\)</span> and <span class="math inline">\(\boldsymbol \Sigma= \operatorname{diag}(\sigma_1^2, \ldots, \sigma_p^2)\)</span> then
<span class="math display">\[\begin{eqnarray*}
f(\boldsymbol x) &amp;=&amp; \frac{1}{(2 \pi)^{p/2}} \exp \left(-\frac{1}{2}(\boldsymbol x- \boldsymbol \mu)^\top \boldsymbol \Sigma^{-1}(\boldsymbol x- \boldsymbol \mu) \right)\\
&amp;=&amp; \frac{1}{(2 \pi)^{p/2}} \exp \left(-\frac{1}{2} \sum_{i=1}^p \frac{(x_i - \mu_i)^2}{\sigma_i^2} \right)\\
&amp;=&amp; \left(\frac{1}{\sqrt{2 \pi}} \exp \left(-\frac{1}{2\sigma_1^2} (x_1 - \mu_1)^2 \right)\right)\\
 &amp;&amp; \qquad \qquad \times \ldots \left(\frac{1}{\sqrt{2 \pi}} \exp \left(-\frac{1}{2\sigma_p^2} (x_p - \mu_p)^2 \right)\right)
\end{eqnarray*}\]</span>
Thus, by the factorisation theorem for probability densities, the components of <span class="math inline">\(\boldsymbol x\)</span> have independent univariate normal distributions: <span class="math inline">\(x_i \sim N(\mu_i, \sigma_i^2)\)</span>.</p>
<p>If <span class="math inline">\(p=2\)</span> we can plot <span class="math inline">\(f(\boldsymbol x)\)</span> using contour plots. Below, Iâve generated 1000 points from four different normal distributions using mean vectors
<span class="math display">\[\boldsymbol \mu_1=\boldsymbol \mu_3=\boldsymbol \mu_4=\begin{pmatrix}0 \\0 \\\end{pmatrix}, \quad \boldsymbol \mu_2=\begin{pmatrix}1 \\-1 \\\end{pmatrix}\]</span>
and covariance matrices
<span class="math display">\[\boldsymbol \Sigma_1=\boldsymbol \Sigma_2=\begin{pmatrix}1&amp;0 \\0&amp;1 \\\end{pmatrix}, \quad \boldsymbol \Sigma_3=\begin{pmatrix}1&amp;0 \\0&amp;0.05 \\\end{pmatrix}, \quad \boldsymbol \Sigma_4=\begin{pmatrix}1&amp;0.9 \\0.9&amp;1 \\\end{pmatrix}.\]</span></p>
<p><img src="07-mvn_files/figure-html/unnamed-chunk-3-1.png" width="960" /></p>
<p>The contours on each plot are obtained by finding values of <span class="math inline">\(\boldsymbol x\)</span> for which <span class="math inline">\(f(\boldsymbol x)=c\)</span>. The constant <span class="math inline">\(c\)</span> is chosen so that the the shapes
enclose 66% and 95% of the data.</p>
<div id="ellipses" class="section level4 unnumbered">
<h4>Ellipses</h4>
<p>What is the shape of the contours in the plots above? They are defined by <span class="math inline">\(f(\boldsymbol x)=c\)</span>, which implies</p>
<p><span class="math display" id="eq:mvnellipse">\[\begin{equation}
(\boldsymbol x- \boldsymbol \mu)^\top \boldsymbol \Sigma^{-1} (\boldsymbol x- \boldsymbol \mu)=c&#39; \tag{7.1}
\end{equation}\]</span>
for some constant <span class="math inline">\(c&#39;\)</span>.
This is the equation of an ellipse. To see this, note that a standard ellipse in <span class="math inline">\(\mathbb{R}^2\)</span> is given by the equation
<span class="math display" id="eq:ellipse">\[\begin{equation}
\frac{x^2}{a^2}+\frac{y^2}{b^2}=1 \quad (a&gt;b&gt;0). \tag{7.2}
\end{equation}\]</span>
and recall that a standard ellipse has axes of symmetry given by the <span class="math inline">\(x\)</span>-axis and <span class="math inline">\(y\)</span>-axis
(if <span class="math inline">\(a&gt;b\)</span>, the <span class="math inline">\(x\)</span>-axis is the major axis, and the <span class="math inline">\(y\)</span>-axis the minor axis). For example, <span class="math inline">\(a=10, b=3\)</span> gives the ellipse:</p>
<p><img src="07-mvn_files/figure-html/unnamed-chunk-4-1.png" width="432" /></p>
<p>If we define
<span class="math inline">\({\mathbf A}=\left( \begin{array}{cc} a^2&amp;0\\ 0&amp;b^2 \end{array} \right)\)</span> and write <span class="math inline">\({\mathbf x}=\binom{x}{y}\)</span>,
then Equation <a href="7-1-definition-and-properties-of-the-mvn.html#eq:ellipse">(7.2)</a> can be written in the form
<span class="math display">\[ \boldsymbol x^\top {\mathbf A}^{-1}\boldsymbol x=c&#39;. \]</span>
To shift the centre of the ellipse from the origin to the point <span class="math inline">\(\boldsymbol \mu\)</span> we modify the equation to be
<span class="math display">\[ (\boldsymbol x-\boldsymbol \mu)^\top {\mathbf A}^{-1}(\boldsymbol x-\boldsymbol \mu) =c&#39;.\]</span></p>
<p>What if instead of using a diagonal matrix <span class="math inline">\(\boldsymbol A\)</span>, we use a non-diagonal matrix <span class="math inline">\(\boldsymbol \Sigma\)</span> as in Equation <a href="7-1-definition-and-properties-of-the-mvn.html#eq:mvnellipse">(7.1)</a>? If <span class="math inline">\(\boldsymbol \Sigma\)</span> has spectral decomposition
<span class="math inline">\(\boldsymbol \Sigma= \boldsymbol V\boldsymbol \Lambda\boldsymbol V^\top\)</span>, then
<span class="math display">\[ (\boldsymbol x-\boldsymbol \mu)^\top {\boldsymbol \Sigma}^{-1}(\boldsymbol x-\boldsymbol \mu) = (\boldsymbol x-\boldsymbol \mu)^\top \boldsymbol V\boldsymbol \Lambda^{-1}\boldsymbol V^\top(\boldsymbol x-\boldsymbol \mu) = \boldsymbol y^\top \boldsymbol \Lambda^{-1}\boldsymbol y\]</span>
where <span class="math inline">\(\boldsymbol \Lambda\)</span> is a diagonal matrix of eigenvalues, and <span class="math inline">\(\boldsymbol y= \boldsymbol V^\top (\boldsymbol x-\boldsymbol u)\)</span>. Because <span class="math inline">\(\boldsymbol V\)</span> is an orthogonal matrix (a rotation), we can see that this is the equation of a standard ellipse when using the eigenvectors as the coordinate system. Or in other words, it is an ellipse
with major axis given by the first eigenvector, and minor axis given by the second eigenvector, centered aroubd <span class="math inline">\(\boldsymbol \mu\)</span>.</p>
<p>Analogous results for ellipsoids and quadratic forms hold in three and higher dimensions.</p>
</div>
<div id="transformations" class="section level3">
<h3><span class="header-section-number">7.1.1</span> Transformations</h3>

<div class="proposition">
<span id="prp:six2" class="proposition"><strong>Proposition 7.2  </strong></span> If <span class="math inline">\(\boldsymbol x\sim N_p(\boldsymbol \mu,\boldsymbol \Sigma)\)</span> and <span class="math inline">\(\boldsymbol y= \boldsymbol A\boldsymbol x+ \boldsymbol c\)</span>, where <span class="math inline">\(\boldsymbol A\)</span> (<span class="math inline">\(q \times p\)</span>) and <span class="math inline">\(\boldsymbol c\)</span> (<span class="math inline">\(q \times 1\)</span>) are constant, then
<span class="math display">\[\boldsymbol y\sim N_q(\boldsymbol A\boldsymbol \mu+ \boldsymbol c, \boldsymbol A\boldsymbol \Sigma\boldsymbol A^\top).\]</span>
</div>


<div class="proof">
 <span class="proof"><em>Proof. </em></span> We know <span class="math inline">\(\boldsymbol y\)</span> is MVN by Proposition <a href="7-1-definition-and-properties-of-the-mvn.html#prp:six1">7.1</a>. We also know <span class="math inline">\({\mathbb{E}}(\boldsymbol y)\)</span> and <span class="math inline">\({\mathbb{V}\operatorname{ar}}(\boldsymbol y)\)</span> from Section <a href="#randvec"><strong>??</strong></a>.
</div>

<p><br></br></p>
<p>This implies that a linear transformation of a MVN random variable is also MVN. We can use this result to prove two important corollaries. The first corollary is useful for simulating data from a general MVN distribution.</p>

<div class="corollary">
<span id="cor:csix2" class="corollary"><strong>Corollary 7.2  </strong></span> If <span class="math inline">\(\boldsymbol x\sim N_p(\boldsymbol 0,\mathbf I_p)\)</span> and <span class="math inline">\(\boldsymbol y= \boldsymbol \Sigma^{1/2} \boldsymbol x+ \boldsymbol \mu\)</span> then <span class="math display">\[\boldsymbol y\sim N_p(\boldsymbol \mu,\boldsymbol \Sigma).\]</span>
</div>


<div class="proof">
 <span class="proof"><em>Proof. </em></span> Apply <a href="7-1-definition-and-properties-of-the-mvn.html#prp:six2">7.2</a> with <span class="math inline">\(\boldsymbol A= \boldsymbol \Sigma^{1/2}\)</span> and <span class="math inline">\(\boldsymbol c= \boldsymbol \mu\)</span>. Therefore
<span class="math display">\[{\mathbb{E}}(\boldsymbol y) = \boldsymbol \Sigma^{1/2} \boldsymbol 0_p + \boldsymbol \mu= \boldsymbol \mu\quad \mbox{and}\quad {\mathbb{V}\operatorname{ar}}(\boldsymbol y) = \boldsymbol \Sigma^{1/2} \mathbf I_p \boldsymbol \Sigma^{1/2} = \boldsymbol \Sigma.\]</span>
</div>

<p>The second corollary says that any MVN random variable can be transformed into standard form.</p>

<div class="corollary">
<span id="cor:csix3" class="corollary"><strong>Corollary 7.3  </strong></span>Suppose <span class="math inline">\(\boldsymbol x\sim N_p(\boldsymbol \mu,\boldsymbol \Sigma)\)</span>, where <span class="math inline">\(\boldsymbol \Sigma\)</span> has full rank. Then<br />
<span class="math display">\[\boldsymbol y= \boldsymbol \Sigma^{-1/2}(\boldsymbol x- \boldsymbol \mu) \sim N_p(\boldsymbol 0,\mathbf I_p).\]</span>
</div>


<div class="proof">
 <span class="proof"><em>Proof. </em></span> Apply Proposition <a href="7-1-definition-and-properties-of-the-mvn.html#prp:six2">7.2</a> with <span class="math inline">\(\boldsymbol A= \boldsymbol \Sigma^{-1/2}\)</span> and <span class="math inline">\(\boldsymbol c= - \boldsymbol \Sigma^{-1/2} \boldsymbol \mu\)</span>.
</div>

</div>
<div id="moment-generating-functions" class="section level3">
<h3><span class="header-section-number">7.1.2</span> Moment Generating Functions</h3>

<div class="definition">
<p><span id="def:mgf" class="definition"><strong>Definition 7.3  </strong></span>The <strong>moment generating function</strong> of a random vector <span class="math inline">\(\boldsymbol x\in \mathbb{R}^p\)</span> is given by
<span class="math display">\[
M({\boldsymbol t})={\mathbb{E}}[e^{{\boldsymbol t}^\top \boldsymbol x}],
\]</span>
and is defined for all <span class="math inline">\({\boldsymbol t}\in \mathbb{R}^p\)</span> for which <span class="math inline">\(M({\boldsymbol t})\)</span> is finite.</p>
</div>


<div class="proposition">
<span id="prp:six3" class="proposition"><strong>Proposition 7.3  </strong></span> The moment generating function of <span class="math inline">\(\boldsymbol x\sim N_p(\boldsymbol \mu, \boldsymbol \Sigma)\)</span> is given by
<span class="math display" id="eq:Mt">\[\begin{equation}
M({\boldsymbol t})=\exp \left (\boldsymbol \mu^\top  {\boldsymbol t}+ \frac{1}{2} {\boldsymbol t}^\top \boldsymbol \Sigma{\boldsymbol t}\right).
\tag{7.3}
\end{equation}\]</span>
</div>


<div class="proof">
<p> <span class="proof"><em>Proof. </em></span> For fixed <span class="math inline">\({\boldsymbol t}\)</span>, define the random variable <span class="math inline">\(Y=\boldsymbol x^\top {\boldsymbol t}\)</span>. From Proposition <a href="7-1-definition-and-properties-of-the-mvn.html#prp:six2">7.2</a>,
<span class="math inline">\(Y \sim N(\mu_{{\boldsymbol t}}, {\sigma_{\boldsymbol t}}^2)\)</span>,
where <span class="math inline">\(\mu_{\boldsymbol t}=\boldsymbol \mu^\top {\boldsymbol t}\)</span> and <span class="math inline">\(\sigma^2_{\boldsymbol t}={\boldsymbol t}^\top \boldsymbol \Sigma{\boldsymbol t}\)</span>.</p>
<p>If <span class="math inline">\(\sigma_{\boldsymbol t}=0\)</span> then <span class="math inline">\(Y=\boldsymbol \mu^\top {\boldsymbol t}\)</span> with probability one, and <span class="math inline">\(M({\boldsymbol t})=e^{\boldsymbol \mu^\top {\boldsymbol t}}\)</span> which agrees with <a href="7-1-definition-and-properties-of-the-mvn.html#eq:Mt">(7.3)</a>.</p>
<p>Assume <span class="math inline">\(\sigma_{{\boldsymbol t}}&gt;0\)</span>. Then
<span class="math display">\[\begin{align*}
M({\boldsymbol t})&amp;={\mathbb{E}}[e^{\boldsymbol x^\top {\boldsymbol t}}]\\
&amp;={\mathbb{E}}[e^{Y}]=\int_{-\infty}^\infty \exp(y) \frac{1}{\sqrt{2\pi \sigma_{\boldsymbol t}^2}}
\exp\left (-\frac{1}{2}\frac{(y-\mu_{\boldsymbol t})^2}{\sigma_{\boldsymbol t}^2} \right )dy.
\end{align*}\]</span></p>
<p>The integral above can be evaluated by completing the square in the exponent, using the identity
<span class="math display">\[
y-\frac{1}{2}\frac{(y-\mu_{\boldsymbol t})^2}{\sigma_{\boldsymbol t}^2}=\mu_{\boldsymbol t}
+\frac{1}{2}\sigma_{\boldsymbol t}^2-\frac{1}{2}\frac{(y-\mu_{\boldsymbol t}-\sigma_{\boldsymbol t}^2)^2}{\sigma_{\boldsymbol t}^2}.
\]</span></p>
Consequently
<span class="math display">\[\begin{align*}
M({\boldsymbol t})&amp;=\int_{-\infty}^\infty \exp \left\{\mu_{\boldsymbol t}+\frac{1}{2}\sigma_{\boldsymbol t}^2 \right\}
\frac{1}{\sqrt{2 \pi \sigma_{\boldsymbol t}^2}}\exp \left\{ -\frac{1}{2} \frac{(y-\mu_{\boldsymbol t}-\sigma_{\boldsymbol t}^2)^2}
{\sigma_{\boldsymbol t}^2}\right\}dy\\
&amp;=\exp\left( \mu_{\boldsymbol t}+ \frac{1}{2}\sigma_{\boldsymbol t}^2  \right)\\
&amp;=\exp\left(
\boldsymbol \mu^\top {\boldsymbol t}+ \frac{1}{2}{\boldsymbol t}^\top \boldsymbol \Sigma{\boldsymbol t}\right),
\end{align*}\]</span>
as required.
</div>


<div class="proposition">
<span id="prp:six4" class="proposition"><strong>Proposition 7.4  </strong></span>Two vectors <span class="math inline">\(\boldsymbol x\)</span> (<span class="math inline">\(p \times 1\)</span>) and <span class="math inline">\(\boldsymbol y\)</span> (<span class="math inline">\(q \times 1\)</span>) which are jointly multivariate normal are independent if and only if they are uncorrelated, i.e. <span class="math inline">\({\mathbb{C}\operatorname{ov}}(\boldsymbol x,\boldsymbol y) = \boldsymbol 0_{p,q}\)</span>.
</div>


<div class="proof">
<p> <span class="proof"><em>Proof. </em></span> We prove this result using the factorisation theorem for moment generating functions (MGFs), which is now stated.
Let
<span class="math display">\[{\boldsymbol t}=\begin{pmatrix}{\boldsymbol t}_1\\ {\boldsymbol t}_2\end{pmatrix}\]</span>
where <span class="math inline">\({\boldsymbol t}_1 \in \mathbb{R}^p\)</span>, <span class="math inline">\({\boldsymbol t}_2 \in \mathbb{R}^q\)</span> and <span class="math inline">\({\boldsymbol t}\in \mathbb{R}^{p+q}\)</span>. The joint MGF of two arbitrary random vectors <span class="math inline">\(\stackrel{p \times 1}{\boldsymbol x}\)</span> and <span class="math inline">\(\stackrel{q \times 1}{\boldsymbol y}\)</span> is <span class="math display">\[
 M({\boldsymbol t}_1, {\boldsymbol t}_2)={\mathbb{E}}[e^{{\boldsymbol t}_1^\top \boldsymbol x+ {\boldsymbol t}_2^\top \boldsymbol y}].
  \]</span>
The factorisation theorem for MGFs states that
<span class="math inline">\(\boldsymbol x\)</span> and <span class="math inline">\(\boldsymbol y\)</span> are independent if and only if <span class="math inline">\(M({\boldsymbol t}_1 , {\boldsymbol t}_2)\)</span> factorises, i.e.,
<span class="math display">\[
M({\boldsymbol t}_1 , {\boldsymbol t}_2)=M_1({\boldsymbol t}_1)M_2({\boldsymbol t}_2)
\]</span>
for some functions <span class="math inline">\(M_1\)</span> and <span class="math inline">\(M_2\)</span>, in which case <span class="math inline">\(M_1\)</span> and <span class="math inline">\(M_2\)</span> are the marginal MGFs of <span class="math inline">\(\boldsymbol x\)</span> and <span class="math inline">\(\boldsymbol y\)</span>.</p>
Now suppose <span class="math inline">\(\boldsymbol x\)</span> and <span class="math inline">\(\boldsymbol y\)</span> are multivariate normal random variables with
<span class="math display" id="eq:def1">\[\begin{equation}
{\mathbb{E}}[\boldsymbol x]=\boldsymbol \mu_{\boldsymbol x}, \qquad \qquad {\mathbb{E}}[\boldsymbol y]=\boldsymbol \mu_{\boldsymbol y}, \quad  {\mathbb{V}\operatorname{ar}}(\boldsymbol x)=\boldsymbol \Sigma_{\boldsymbol x\boldsymbol x},
\quad  {\mathbb{V}\operatorname{ar}}(\boldsymbol y)=\boldsymbol \Sigma_{\boldsymbol y\boldsymbol y},
\tag{7.4}
\end{equation}\]</span>
and
<span class="math display" id="eq:def2">\[\begin{equation}
{\mathbb{C}\operatorname{ov}}(\boldsymbol x,\boldsymbol y)=\boldsymbol \Sigma_{\boldsymbol x\boldsymbol y}=\boldsymbol \Sigma_{\boldsymbol y\boldsymbol x}^\top = {\mathbb{C}\operatorname{ov}}(\boldsymbol y, \boldsymbol x)^\top.
\tag{7.5}
\end{equation}\]</span>
Using Proposition <a href="7-1-definition-and-properties-of-the-mvn.html#prp:six3">7.3</a> and definitions <a href="7-1-definition-and-properties-of-the-mvn.html#eq:def1">(7.4)</a> and <a href="7-1-definition-and-properties-of-the-mvn.html#eq:def2">(7.5)</a>,
<span class="math display">\[\begin{align*}
M({\boldsymbol t}_1, {\boldsymbol t}_2)&amp;=\exp\left ( \boldsymbol \mu^\top {\boldsymbol t}+ \frac{1}{2}{\boldsymbol t}^\top \boldsymbol \Sigma{\boldsymbol t}\right )\\
&amp;=\exp\bigg (\boldsymbol \mu_{\boldsymbol x}^\top {\boldsymbol t}_1 +\boldsymbol \mu_{\boldsymbol y}^\top {\boldsymbol t}_2+\frac{1}{2}{\boldsymbol t}_1^\top \boldsymbol \Sigma_{\boldsymbol x\boldsymbol x}{{\boldsymbol t}_1}\\
&amp; \qquad \qquad +\frac{1}{2}{\boldsymbol t}_2^\top  \boldsymbol \Sigma_{\boldsymbol y\boldsymbol y}{\boldsymbol t}_2+\frac{1}{2} 2{\boldsymbol t}_1^\top \boldsymbol \Sigma_{\boldsymbol x\boldsymbol y}{\boldsymbol t}_2 \bigg)\\
&amp;=M_1({\boldsymbol t}_1)M_2({\boldsymbol t}_2)M_3({\boldsymbol t}_1, {\boldsymbol t}_2),
\end{align*}\]</span>
where <span class="math inline">\(M_1({\boldsymbol t}_1)\)</span> and <span class="math inline">\(M_2({\boldsymbol t}_2)\)</span> are the marginal MGFs of <span class="math inline">\(\boldsymbol x\)</span> and <span class="math inline">\(\boldsymbol y\)</span> respectively, and
<span class="math display">\[
M_3({\boldsymbol t}_1, {\boldsymbol t}_2)=\exp\left ({\boldsymbol t}_1^\top \boldsymbol \Sigma_{\boldsymbol x\boldsymbol y}{\boldsymbol t}_2 \right ).
\]</span>
Thus, by the factorisation theorem, <span class="math inline">\(\boldsymbol x\)</span> and <span class="math inline">\(\boldsymbol y\)</span> are independent if and only if <span class="math inline">\(M_3({\boldsymbol t}_1, {\boldsymbol t}_2)\)</span> is constant with respect to
<span class="math inline">\({\boldsymbol t}_1\)</span> and <span class="math inline">\({\boldsymbol t}_2\)</span>, which is the case if and only if <span class="math inline">\(\boldsymbol \Sigma_{\boldsymbol x\boldsymbol y}={\mathbf 0}_{p,q}\)</span>.
</div>

<p><br></br></p>
<p>Proposition <a href="7-1-definition-and-properties-of-the-mvn.html#prp:six4">7.4</a> means that zero correlation implies independence for the MVN distribution. This is not true in general for other distributions.</p>
<p><strong>Note</strong>: Propositions <a href="7-1-definition-and-properties-of-the-mvn.html#prp:six1">7.1</a> - <a href="7-1-definition-and-properties-of-the-mvn.html#prp:six4">7.4</a> holds regardless irregardless of whether the covariance matrix <span class="math inline">\(\boldsymbol \Sigma\)</span> is invertible.</p>
<p>The term <span class="math inline">\((\boldsymbol x-\boldsymbol \mu)^\top \boldsymbol \Sigma^{-1} (\boldsymbol x-\boldsymbol \mu)\)</span> appears in the exponent of the pdf and
will be important later. We now derive its distribution:</p>

<div class="proposition">
<span id="prp:six5" class="proposition"><strong>Proposition 7.5  </strong></span>If <span class="math inline">\(\boldsymbol x\sim N_p(\boldsymbol \mu, \boldsymbol \Sigma)\)</span> and <span class="math inline">\(\boldsymbol \Sigma\)</span> is positive definite then
<span class="math display">\[(\boldsymbol x-\boldsymbol \mu)^\top \boldsymbol \Sigma^{-1} (\boldsymbol x-\boldsymbol \mu) \sim \chi_p^2.\]</span>
</div>


<div class="proof">
 <span class="proof"><em>Proof. </em></span> Define <span class="math inline">\(\boldsymbol y= \boldsymbol \Sigma^{-1/2} (\boldsymbol x-\boldsymbol \mu)\)</span> so
<span class="math display">\[\begin{align*}
(\boldsymbol x-\boldsymbol \mu)^\top \boldsymbol \Sigma^{-1} (\boldsymbol x-\boldsymbol \mu) &amp;= \left(\boldsymbol \Sigma^{-1/2} (\boldsymbol x-\boldsymbol \mu) \right)^\top \left(\boldsymbol \Sigma^{-1/2} (\boldsymbol x-\boldsymbol \mu) \right)\\
&amp;= \boldsymbol y^\top \boldsymbol y= \sum_{i=1}^p y_i^2
\end{align*}\]</span>
By Corollary <a href="7-1-definition-and-properties-of-the-mvn.html#cor:csix3">7.3</a>, <span class="math inline">\(\boldsymbol y\sim N_p (\boldsymbol 0, \mathbf I_p)\)</span>, and so the components of <span class="math inline">\(\boldsymbol y\)</span> have independent univariate normal distributions with mean 0 and variance 1. Recall from univariate statistics that if <span class="math inline">\(z \sim N(0,1)\)</span> then <span class="math inline">\(z^2 \sim \chi^2_1\)</span> and if <span class="math inline">\(z_1, \ldots, z_n\)</span> are iid <span class="math inline">\(N(0,1)\)</span> then <span class="math inline">\(\sum_{i=1}^n z_i^2 \sim \chi_n^2\)</span>. It therefore follows that <span class="math display">\[\sum_{i=1}^p y_i^2 \sim \chi^2_p.\]</span>
</div>

<p>We saw earlier in this section chapter that the MVN distribution in <span class="math inline">\(p\)</span> dimensions has constant density on ellipses or ellipsoids given by <span class="math inline">\(f(\boldsymbol x) = c\)</span> for some constant <span class="math inline">\(c &gt; 0\)</span>, and that we can rearrange this equation to be of the form
<span class="math display">\[U(\boldsymbol x) = (\boldsymbol x-\boldsymbol \mu)^\top \boldsymbol \Sigma^{-1} (\boldsymbol x-\boldsymbol \mu) = k\]</span>
where <span class="math inline">\(k = - 2 \log(c) - \log |2 \pi \boldsymbol \Sigma| &gt; 0\)</span> is a combination of the constant, <span class="math inline">\(c\)</span>, and the normalising constant in the pdf. Proposition <a href="7-1-definition-and-properties-of-the-mvn.html#prp:six5">7.5</a> means we can calculate the probability, <span class="math inline">\(P(U(\boldsymbol x)&lt;k)\)</span>, which is the probability of <span class="math inline">\(\boldsymbol x\)</span> lying within a particular ellipsoid.</p>
</div>
<div id="sampling-results-for-the-mvn" class="section level3">
<h3><span class="header-section-number">7.1.3</span> Sampling results for the MVN</h3>
<p>In this section we present two important results which are natural
generalisations of what happens in the univariate case.</p>

<div class="proposition">
<span id="prp:six6" class="proposition"><strong>Proposition 7.6  </strong></span>If <span class="math inline">\(\boldsymbol x_1, \ldots, \boldsymbol x_n\)</span> is an IID random sample from <span class="math inline">\(N_p(\boldsymbol \mu, \boldsymbol \Sigma)\)</span>, then the sample mean and sample variance matrix
<span class="math display">\[\bar{\boldsymbol x} = \frac{1}{n} \sum_{i=1}^n \boldsymbol x_i, \quad \boldsymbol S= \frac{1}{n} \sum_{i=1}^n (\boldsymbol x_i - \bar{\boldsymbol x})(\boldsymbol x_i-\bar{\boldsymbol x})^\top\]</span> are independent.
</div>


<div class="proof">
 <span class="proof"><em>Proof. </em></span> From Proposition <a href="7-1-definition-and-properties-of-the-mvn.html#prp:six1">7.1</a> and Proposition <a href="7-1-definition-and-properties-of-the-mvn.html#prp:six2">7.2</a> we can see that if <span class="math inline">\(\boldsymbol x_1, \ldots, \boldsymbol x_n \sim N_p(\boldsymbol \mu, \boldsymbol \Sigma)\)</span> then <span class="math inline">\(\bar{\boldsymbol x} \sim N_p (\boldsymbol \mu, n^{-1}\boldsymbol \Sigma)\)</span>. Then
<span class="math display">\[\begin{align*}
{\mathbb{C}\operatorname{ov}}(\bar{\boldsymbol x},\boldsymbol y_i)&amp;={\mathbb{C}\operatorname{ov}}(\bar{\boldsymbol x}, \boldsymbol x_i -\bar{\boldsymbol x})\\
&amp;={\mathbb{C}\operatorname{ov}}(\bar{\boldsymbol x}, \boldsymbol x_i) - {\mathbb{C}\operatorname{ov}}(\bar{\boldsymbol x}, \bar{\boldsymbol x})\\
&amp;=n^{-1}\sum_{j=1}^n \left \{{\mathbb{E}}[(\boldsymbol x_j -\boldsymbol \mu)(\boldsymbol x_i-\boldsymbol \mu)^\top]\right \}\\
&amp; \qquad \qquad -{\mathbb{E}}[(\bar{\boldsymbol x}-\boldsymbol \mu)(\bar{\boldsymbol x}-\boldsymbol \mu)^\top]\\
&amp;=n^{-1}\boldsymbol \Sigma- n^{-1}\boldsymbol \Sigma\\
&amp;= {\mathbf 0}_{p,p}.
\end{align*}\]</span>
Thus Proposition <a href="7-1-definition-and-properties-of-the-mvn.html#prp:six4">7.4</a> gives that <span class="math inline">\(\bar{\boldsymbol x}\)</span> and <span class="math inline">\(\boldsymbol y_i\)</span> are independent, and therefore <span class="math inline">\(\bar{\boldsymbol x}\)</span> and
<span class="math display">\[
\boldsymbol S=\frac{1}{n}\sum_{i=1}^n \boldsymbol y_i \boldsymbol y_i^\top =n^{-1}\sum_{i=1}^n (\boldsymbol x_i -\bar{\boldsymbol x})(\boldsymbol x_i -\bar{\boldsymbol x})^\top
\]</span>
are independent.
</div>

<p><br></br></p>
<p>Recall from above that if <span class="math inline">\(\boldsymbol x_1, \ldots, \boldsymbol x_n\)</span> is a random sample from <span class="math inline">\(N_p(\boldsymbol \mu, \boldsymbol \Sigma)\)</span> then <span class="math display">\[\bar{\boldsymbol x} \sim N_p (\boldsymbol \mu, \frac{1}{n}\boldsymbol \Sigma).\]</span> This result is also approximately true for large samples from non-normal distributions, as is now stated in the multivariate central limit theorem.</p>

<div class="proposition">
<span id="prp:clt" class="proposition"><strong>Proposition 7.7  </strong></span><strong>Central limit theorem</strong> Let <span class="math inline">\(\boldsymbol x_1, \boldsymbol x_2, \ldots \in \mathbb{R}^\p\)</span> be a sample of independent and identically distributed random vectors from a distribution with mean <span class="math inline">\(\boldsymbol \mu\)</span> and finite variance matrix <span class="math inline">\(\boldsymbol \Sigma\)</span>. Then asymptotically as <span class="math inline">\(n \rightarrow \infty\)</span>, <span class="math inline">\(\sqrt{n}(\bar{\boldsymbol x}-\boldsymbol \mu)\)</span> converges in distribution to <span class="math inline">\(N_p ({\mathbf 0}_p, \boldsymbol \Sigma)\)</span>.
</div>


<div class="proof">
 <span class="proof"><em>Proof. </em></span> Beyond the scope of this module.
</div>

</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="7-multinormal.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="7-2-the-wishart-distribution.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["MultivariateStatistics.pdf"],
"toc": {
"collapse": "section"
},
"pandoc_args": "--top-level-division=[chapter|part]"
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
