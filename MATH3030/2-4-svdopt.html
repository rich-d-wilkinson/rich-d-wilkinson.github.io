<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>2.4 SVD optimization results | Multivariate Statistics</title>
  <meta name="description" content="The lecture notes for MATH3030/4068: Multivariate Analysis / Applied Multivariate Statistics" />
  <meta name="generator" content="bookdown 0.21 and GitBook 2.6.7" />

  <meta property="og:title" content="2.4 SVD optimization results | Multivariate Statistics" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="The lecture notes for MATH3030/4068: Multivariate Analysis / Applied Multivariate Statistics" />
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="2.4 SVD optimization results | Multivariate Statistics" />
  
  <meta name="twitter:description" content="The lecture notes for MATH3030/4068: Multivariate Analysis / Applied Multivariate Statistics" />
  

<meta name="author" content="Prof. Richard Wilkinson" />



  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="2-3-linalg-SVD.html"/>
<link rel="next" href="2-5-low-rank-approximation.html"/>
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />









<script src="libs/kePrint-0.0.1/kePrint.js"></script>
<link href="libs/lightable-0.0.1/lightable.css" rel="stylesheet" />


<style type="text/css">
a.sourceLine { display: inline-block; line-height: 1.25; }
a.sourceLine { pointer-events: none; color: inherit; text-decoration: inherit; }
a.sourceLine:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode { white-space: pre; position: relative; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
code.sourceCode { white-space: pre-wrap; }
a.sourceLine { text-indent: -1em; padding-left: 1em; }
}
pre.numberSource a.sourceLine
  { position: relative; left: -4em; }
pre.numberSource a.sourceLine::before
  { content: attr(data-line-number);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; pointer-events: all; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {  }
@media screen {
a.sourceLine::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Applied multivariate statistics</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Introduction</a></li>
<li class="chapter" data-level="1" data-path="1-linalg-prelim.html"><a href="1-linalg-prelim.html"><i class="fa fa-check"></i><b>1</b> Review of linear algebra</a><ul>
<li class="chapter" data-level="1.1" data-path="1-1-linalg-basics.html"><a href="1-1-linalg-basics.html"><i class="fa fa-check"></i><b>1.1</b> Basics</a><ul>
<li class="chapter" data-level="1.1.1" data-path="1-1-linalg-basics.html"><a href="1-1-linalg-basics.html#notation"><i class="fa fa-check"></i><b>1.1.1</b> Notation</a></li>
<li class="chapter" data-level="1.1.2" data-path="1-1-linalg-basics.html"><a href="1-1-linalg-basics.html#elementary-matrix-operations"><i class="fa fa-check"></i><b>1.1.2</b> Elementary matrix operations</a></li>
<li class="chapter" data-level="1.1.3" data-path="1-1-linalg-basics.html"><a href="1-1-linalg-basics.html#special-matrices"><i class="fa fa-check"></i><b>1.1.3</b> Special matrices</a></li>
<li class="chapter" data-level="1.1.4" data-path="1-1-linalg-basics.html"><a href="1-1-linalg-basics.html#vectordiff"><i class="fa fa-check"></i><b>1.1.4</b> Vector Differentiation</a></li>
</ul></li>
<li class="chapter" data-level="1.2" data-path="1-2-linalg-vecspaces.html"><a href="1-2-linalg-vecspaces.html"><i class="fa fa-check"></i><b>1.2</b> Vector spaces</a><ul>
<li class="chapter" data-level="1.2.1" data-path="1-2-linalg-vecspaces.html"><a href="1-2-linalg-vecspaces.html#linear-independence"><i class="fa fa-check"></i><b>1.2.1</b> Linear independence</a></li>
<li class="chapter" data-level="1.2.2" data-path="1-2-linalg-vecspaces.html"><a href="1-2-linalg-vecspaces.html#colsspace"><i class="fa fa-check"></i><b>1.2.2</b> Row and column spaces</a></li>
<li class="chapter" data-level="1.2.3" data-path="1-2-linalg-vecspaces.html"><a href="1-2-linalg-vecspaces.html#linear-transformations"><i class="fa fa-check"></i><b>1.2.3</b> Linear transformations</a></li>
</ul></li>
<li class="chapter" data-level="1.3" data-path="1-3-linalg-innerprod.html"><a href="1-3-linalg-innerprod.html"><i class="fa fa-check"></i><b>1.3</b> Inner product spaces</a><ul>
<li class="chapter" data-level="1.3.1" data-path="1-3-linalg-innerprod.html"><a href="1-3-linalg-innerprod.html#normed"><i class="fa fa-check"></i><b>1.3.1</b> Distances, and angles</a></li>
<li class="chapter" data-level="1.3.2" data-path="1-3-linalg-innerprod.html"><a href="1-3-linalg-innerprod.html#orthogonal-matrices"><i class="fa fa-check"></i><b>1.3.2</b> Orthogonal matrices</a></li>
<li class="chapter" data-level="1.3.3" data-path="1-3-linalg-innerprod.html"><a href="1-3-linalg-innerprod.html#projection-matrix"><i class="fa fa-check"></i><b>1.3.3</b> Projections</a></li>
</ul></li>
<li class="chapter" data-level="1.4" data-path="1-4-centering-matrix.html"><a href="1-4-centering-matrix.html"><i class="fa fa-check"></i><b>1.4</b> The Centering Matrix</a></li>
<li class="chapter" data-level="1.5" data-path="1-5-tasks-ch2.html"><a href="1-5-tasks-ch2.html"><i class="fa fa-check"></i><b>1.5</b> Computer tasks</a></li>
<li class="chapter" data-level="1.6" data-path="1-6-exercises-ch2.html"><a href="1-6-exercises-ch2.html"><i class="fa fa-check"></i><b>1.6</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="2-linalg-decomp.html"><a href="2-linalg-decomp.html"><i class="fa fa-check"></i><b>2</b> Matrix decompositions</a><ul>
<li class="chapter" data-level="2.1" data-path="2-1-matrix-matrix.html"><a href="2-1-matrix-matrix.html"><i class="fa fa-check"></i><b>2.1</b> Matrix-matrix products</a></li>
<li class="chapter" data-level="2.2" data-path="2-2-spectraleigen-decomposition.html"><a href="2-2-spectraleigen-decomposition.html"><i class="fa fa-check"></i><b>2.2</b> Spectral/eigen decomposition</a><ul>
<li class="chapter" data-level="2.2.1" data-path="2-2-spectraleigen-decomposition.html"><a href="2-2-spectraleigen-decomposition.html#eigenvalues-and-eigenvectors"><i class="fa fa-check"></i><b>2.2.1</b> Eigenvalues and eigenvectors</a></li>
<li class="chapter" data-level="2.2.2" data-path="2-2-spectraleigen-decomposition.html"><a href="2-2-spectraleigen-decomposition.html#spectral-decomposition"><i class="fa fa-check"></i><b>2.2.2</b> Spectral decomposition</a></li>
<li class="chapter" data-level="2.2.3" data-path="2-2-spectraleigen-decomposition.html"><a href="2-2-spectraleigen-decomposition.html#matrixroots"><i class="fa fa-check"></i><b>2.2.3</b> Matrix square roots</a></li>
</ul></li>
<li class="chapter" data-level="2.3" data-path="2-3-linalg-SVD.html"><a href="2-3-linalg-SVD.html"><i class="fa fa-check"></i><b>2.3</b> Singular Value Decomposition (SVD)</a><ul>
<li class="chapter" data-level="2.3.1" data-path="2-3-linalg-SVD.html"><a href="2-3-linalg-SVD.html#examples"><i class="fa fa-check"></i><b>2.3.1</b> Examples</a></li>
</ul></li>
<li class="chapter" data-level="2.4" data-path="2-4-svdopt.html"><a href="2-4-svdopt.html"><i class="fa fa-check"></i><b>2.4</b> SVD optimization results</a></li>
<li class="chapter" data-level="2.5" data-path="2-5-low-rank-approximation.html"><a href="2-5-low-rank-approximation.html"><i class="fa fa-check"></i><b>2.5</b> Low-rank approximation</a><ul>
<li class="chapter" data-level="2.5.1" data-path="2-5-low-rank-approximation.html"><a href="2-5-low-rank-approximation.html#matrix-norms"><i class="fa fa-check"></i><b>2.5.1</b> Matrix norms</a></li>
<li class="chapter" data-level="2.5.2" data-path="2-5-low-rank-approximation.html"><a href="2-5-low-rank-approximation.html#eckart-young-mirsky-theorem"><i class="fa fa-check"></i><b>2.5.2</b> Eckart-Young-Mirsky Theorem</a></li>
<li class="chapter" data-level="2.5.3" data-path="2-5-low-rank-approximation.html"><a href="2-5-low-rank-approximation.html#example-image-compression"><i class="fa fa-check"></i><b>2.5.3</b> Example: image compression</a></li>
</ul></li>
<li class="chapter" data-level="2.6" data-path="2-6-tasks-ch3.html"><a href="2-6-tasks-ch3.html"><i class="fa fa-check"></i><b>2.6</b> Computer tasks</a></li>
<li class="chapter" data-level="2.7" data-path="2-7-exercises-ch3.html"><a href="2-7-exercises-ch3.html"><i class="fa fa-check"></i><b>2.7</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="3-cca.html"><a href="3-cca.html"><i class="fa fa-check"></i><b>3</b> Canonical Correlation Analysis (CCA)</a><ul>
<li class="chapter" data-level="3.1" data-path="3-1-cca1.html"><a href="3-1-cca1.html"><i class="fa fa-check"></i><b>3.1</b> The first pair of canonical variables</a><ul>
<li class="chapter" data-level="3.1.1" data-path="3-1-cca1.html"><a href="3-1-cca1.html#the-first-canonical-components"><i class="fa fa-check"></i><b>3.1.1</b> The first canonical components</a></li>
<li class="chapter" data-level="3.1.2" data-path="3-1-cca1.html"><a href="3-1-cca1.html#premcca"><i class="fa fa-check"></i><b>3.1.2</b> Example: Premier league football</a></li>
</ul></li>
<li class="chapter" data-level="3.2" data-path="3-2-the-full-set-of-canonical-correlations.html"><a href="3-2-the-full-set-of-canonical-correlations.html"><i class="fa fa-check"></i><b>3.2</b> The full set of canonical correlations</a><ul>
<li class="chapter" data-level="3.2.1" data-path="3-2-the-full-set-of-canonical-correlations.html"><a href="3-2-the-full-set-of-canonical-correlations.html#example-continued"><i class="fa fa-check"></i><b>3.2.1</b> Example continued</a></li>
</ul></li>
<li class="chapter" data-level="3.3" data-path="3-3-properties.html"><a href="3-3-properties.html"><i class="fa fa-check"></i><b>3.3</b> Properties</a><ul>
<li class="chapter" data-level="3.3.1" data-path="3-3-properties.html"><a href="3-3-properties.html#connection-with-linear-regression-when-q1"><i class="fa fa-check"></i><b>3.3.1</b> Connection with linear regression when <span class="math inline">\(q=1\)</span></a></li>
<li class="chapter" data-level="3.3.2" data-path="3-3-properties.html"><a href="3-3-properties.html#invarianceequivariance-properties-of-cca"><i class="fa fa-check"></i><b>3.3.2</b> Invariance/equivariance properties of CCA</a></li>
</ul></li>
<li class="chapter" data-level="3.4" data-path="3-4-computer-tasks.html"><a href="3-4-computer-tasks.html"><i class="fa fa-check"></i><b>3.4</b> Computer tasks</a></li>
<li class="chapter" data-level="3.5" data-path="3-5-exercises.html"><a href="3-5-exercises.html"><i class="fa fa-check"></i><b>3.5</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="4-mds.html"><a href="4-mds.html"><i class="fa fa-check"></i><b>4</b> Multidimensional Scaling (MDS)</a><ul>
<li class="chapter" data-level="4.1" data-path="4-1-classical-mds.html"><a href="4-1-classical-mds.html"><i class="fa fa-check"></i><b>4.1</b> Classical MDS</a><ul>
<li class="chapter" data-level="4.1.1" data-path="4-1-classical-mds.html"><a href="4-1-classical-mds.html#non-euclidean-distance-matrices"><i class="fa fa-check"></i><b>4.1.1</b> Non-Euclidean distance matrices</a></li>
<li class="chapter" data-level="4.1.2" data-path="4-1-classical-mds.html"><a href="4-1-classical-mds.html#principal-coordinate-analysis"><i class="fa fa-check"></i><b>4.1.2</b> Principal Coordinate Analysis</a></li>
</ul></li>
<li class="chapter" data-level="4.2" data-path="4-2-properties-1.html"><a href="4-2-properties-1.html"><i class="fa fa-check"></i><b>4.2</b> Properties</a></li>
<li class="chapter" data-level="4.3" data-path="4-3-non-classical-mds.html"><a href="4-3-non-classical-mds.html"><i class="fa fa-check"></i><b>4.3</b> Non-classical MDS</a></li>
<li class="chapter" data-level="4.4" data-path="4-4-similarity-measures.html"><a href="4-4-similarity-measures.html"><i class="fa fa-check"></i><b>4.4</b> Similarity measures</a><ul>
<li class="chapter" data-level="4.4.1" data-path="4-4-similarity-measures.html"><a href="4-4-similarity-measures.html#binary-attributes"><i class="fa fa-check"></i><b>4.4.1</b> Binary attributes</a></li>
<li class="chapter" data-level="4.4.2" data-path="4-4-similarity-measures.html"><a href="4-4-similarity-measures.html#example"><i class="fa fa-check"></i><b>4.4.2</b> Example</a></li>
</ul></li>
<li class="chapter" data-level="4.5" data-path="4-5-non-metric-mds.html"><a href="4-5-non-metric-mds.html"><i class="fa fa-check"></i><b>4.5</b> Non-metric MDS</a></li>
<li class="chapter" data-level="4.6" data-path="4-6-exercises-1.html"><a href="4-6-exercises-1.html"><i class="fa fa-check"></i><b>4.6</b> Exercises</a></li>
<li class="chapter" data-level="4.7" data-path="4-7-computer-tasks-1.html"><a href="4-7-computer-tasks-1.html"><i class="fa fa-check"></i><b>4.7</b> Computer Tasks</a></li>
</ul></li>
<li class="divider"></li>
<li> <a href="https://rich-d-wilkinson.github.io/teaching.html" target="blank">University of Nottingham</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Multivariate Statistics</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="svdopt" class="section level2">
<h2><span class="header-section-number">2.4</span> SVD optimization results</h2>
<p>Why are eigenvalues and singular values useful in statistics? It is because they appear as the result of some important optimization problems. We’ll see more about this in later chapters, but we’ll prove a few preliminary results here.</p>
<p>For example, suppose <span class="math inline">\(\mathbf x\in\mathbb{R}^n\)</span> is a random variable with <span class="math inline">\({\mathbb{C}\operatorname{ov}}(\mathbf x)=\boldsymbol{\Sigma}\)</span> (an <span class="math inline">\(n \times n\)</span> matrix), then can we find a projection of <span class="math inline">\(\mathbf x\)</span> that has either maximum or minimum variance? I.e., can we find <span class="math inline">\(\mathbf a\)</span> such that <span class="math display">\[{\mathbb{V}\operatorname{ar}}(\mathbf a^\top\mathbf x)=\mathbf a^\top \boldsymbol{\Sigma}\mathbf a\]</span> is maximized or minimized?
To make the question interesting we need to constrain the length of <span class="math inline">\(\mathbf a\)</span> so lets assume that <span class="math inline">\(||\mathbf a||_2 = \sqrt{\mathbf a^\top \mathbf a}=1\)</span>, otherwise we could just take <span class="math inline">\(\mathbf a=\boldsymbol 0\)</span> to obtain a projection with variance zero. So we want to solve the optimization problems involving the quadratic form <span class="math inline">\(\mathbf a^\top \boldsymbol{\Sigma}\mathbf a\)</span>:</p>
<p><span class="math display" id="eq:eigenopt">\[\begin{equation}
\max_{\mathbf a: \;\mathbf a^\top \mathbf a=1}\mathbf A^\top \boldsymbol{\Sigma}\mathbf A, \quad \mbox{and}\quad
\min_{\mathbf a: \;\mathbf a^\top \mathbf a=1}\mathbf A^\top \boldsymbol{\Sigma}\mathbf A.
\tag{2.2}
\end{equation}\]</span></p>
<p>Given that <span class="math inline">\(\boldsymbol{\Sigma}\)</span> is symmetric, we can write it as
<span class="math display">\[\boldsymbol{\Sigma}= \mathbf V\boldsymbol \Lambda\mathbf V^\top \]</span>
where <span class="math inline">\(\boldsymbol \Lambda\)</span> is the diagonal matrix of eigenvalues of <span class="math inline">\(\boldsymbol{\Sigma}\)</span>, and <span class="math inline">\(\mathbf V\)</span> is an orthogonal matrix of eigenvectors. If we let <span class="math inline">\(\mathbf b=\mathbf V^\top \mathbf a\)</span> then
<span class="math display">\[\mathbf a^\top \boldsymbol{\Sigma}\mathbf a= \mathbf b^\top \boldsymbol \Lambda\mathbf b= \sum_{i=1}^n \lambda_i b_i^2\]</span>
and given that the eigenvalues are ordered <span class="math inline">\(\lambda_1\geq \lambda_2 \geq \ldots\)</span> and that <span class="math display">\[\sum_{i=1}^n b_i^2=\mathbf b^\top\mathbf b=\mathbf a^\top \mathbf V\mathbf V^\top\mathbf a=\mathbf a^\top\mathbf a=1,\]</span>
we can see that the maximumn is <span class="math inline">\(\lambda_1\)</span> obtained by setting <span class="math inline">\(\mathbf b=(1\;0\;0 \ldots)^\top\)</span>. Then
<span class="math display">\[\begin{align*}
\mathbf V^\top \mathbf a&amp;= \mathbf b\\
\mathbf V\mathbf V^\top \mathbf a&amp;=\mathbf V\mathbf b\\
\mathbf a&amp;= \mathbf v_1
\end{align*}\]</span>
so we can see that the maximum is obtained when <span class="math inline">\(\mathbf a=\mathbf v_1\)</span>, the eigenvector of <span class="math inline">\(\boldsymbol{\Sigma}\)</span> corresponding to the largest eigenvalue <span class="math inline">\(\lambda_1\)</span>.</p>
<p>Similarly, the minimum is <span class="math inline">\(\lambda_n\)</span>, which obtained by setting <span class="math inline">\(\mathbf b=(\;0\;0 \ldots\;0\;1)^\top\)</span> which corresponds to <span class="math inline">\(\mathbf a=\mathbf v_n\)</span>.</p>

<div class="proposition">
<span id="prp:two8" class="proposition"><strong>Proposition 2.7  </strong></span>For any symmetric <span class="math inline">\(n \times n\)</span> matrix <span class="math inline">\(\boldsymbol{\Sigma}\)</span>,
<span class="math display">\[\max_{\mathbf a: \mathbf a^\top \mathbf a=1} \mathbf a^\top\boldsymbol{\Sigma}\mathbf a=\lambda_1,\]</span><br />
where the maximum occurs at <span class="math inline">\(\mathbf a=\pm \mathbf v_1\)</span>, and
<span class="math display">\[\min_{\mathbf a: \mathbf a^\top \mathbf a=1} \mathbf a^\top\boldsymbol{\Sigma}\mathbf a=\lambda_n\]</span>
where the minimum occurs at <span class="math inline">\(\mathbf a= \pm \mathbf v_n\)</span>, where <span class="math inline">\(\lambda_i, \mathbf v_i\)</span> are the ordered eigenpairs of <span class="math inline">\(\boldsymbol{\Sigma}\)</span>.
</div>

<p>Note that <span class="math display">\[\frac{\mathbf a^\top \boldsymbol{\Sigma}\mathbf a}{\mathbf a^\top\mathbf a}=\frac{\mathbf a^\top \boldsymbol{\Sigma}\mathbf a}{||\mathbf a||^2} = (\frac{\mathbf a}{||\mathbf a||})^\top \boldsymbol{\Sigma}(\frac{\mathbf a}{||\mathbf a||})\]</span>
and so another way to write the maximization problems <a href="2-4-svdopt.html#eq:eigenopt">(2.2)</a> is as unconstrained optimization problems:</p>
<p><span class="math display">\[\max_{\mathbf a}\frac{\mathbf a^\top \boldsymbol{\Sigma}\mathbf a}{\mathbf a^\top\mathbf a}\quad \mbox{ and } \quad \min_{\mathbf a}\frac{\mathbf a^\top \boldsymbol{\Sigma}\mathbf a}{\mathbf a^\top\mathbf a}.\]</span></p>
<p>We obtain a similar result for non-square matrices using the singular value decomposition.</p>

<div class="proposition">
<span id="prp:svdmax1" class="proposition"><strong>Proposition 2.8  </strong></span>For any matrix <span class="math inline">\(\mathbf A\)</span>
<span class="math display">\[\max_{\mathbf x: ||\mathbf x||_2=1}||\mathbf A\mathbf x||_2=\max_{\mathbf x}\frac{||\mathbf A\mathbf x||_2}{||\mathbf x||_2}=\sigma_1\]</span>
the first singular value of <span class="math inline">\(\mathbf A\)</span>, with the maximum achieved at <span class="math inline">\(\mathbf x=\mathbf v_1\)</span> (the first right singular vector).
</div>


<div class="proof">
 <span class="proof"><em>Proof. </em></span> This is follows from <a href="2-4-svdopt.html#prp:two8">2.7</a> as
<span class="math display">\[||\mathbf A\mathbf x||_2^2=\mathbf x^\top \mathbf A^\top\mathbf A\mathbf x.\]</span>
</div>

<p><br>
Here, <span class="math inline">\(||\mathbf x|_2\)</span> denotes the Euclidean norm for vectors:
<span class="math display">\[||\mathbf x||_2 = (\mathbf x^\top \mathbf x)^{\frac{1}{2}}.\]</span></p>
<p>Finally, we will need the following result when we study canonical correlation analysis:</p>

<div class="proposition">
<span id="prp:svdmax2" class="proposition"><strong>Proposition 2.9  </strong></span>For any matrix <span class="math inline">\(\mathbf A\)</span>, we have
<span class="math display">\[
\max_{\mathbf a, \mathbf b:\, \vert \vert \mathbf a\vert \vert=\vert \vert \mathbf b\vert \vert =1} \mathbf a^\top \mathbf A\mathbf b=\sigma_1.
\]</span>
with the maximum obtained at <span class="math inline">\(\mathbf a=\mathbf u_1\)</span> and <span class="math inline">\(\mathbf b=\mathbf v_1\)</span>, the first left and right singular vectors of <span class="math inline">\(\mathbf A\)</span>.
</div>


<div class="proof">
 <span class="proof"><em>Proof. </em></span> See Section <a href="3-1-cca1.html#cca1">3.1</a>
</div>

<!--
So for example, if $\bx\in \mathbb{R}^n$ and $\by\in \mathbb{R}^p$ are random variables with 
$$\cov(\bx, \by)=\bA$$
then this result tells us the one dimensional projections that maximise $\cov(\ba^\top\bx, \bb^\top\by)=\ba^\top \bA \bb$. 
-->
<p>We’ll see much more of this kind of thing in Chapters <a href="#pca"><strong>??</strong></a> and <a href="3-cca.html#cca">3</a>.</p>
<!--DO I NEED RAYLEIGH QUOTIENTS?-->
</div>
            </section>

          </div>
        </div>
      </div>
<a href="2-3-linalg-SVD.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="2-5-low-rank-approximation.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["MultivariateStatistics.pdf"],
"toc": {
"collapse": "section"
},
"pandoc_args": "--top-level-division=[chapter|part]"
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
