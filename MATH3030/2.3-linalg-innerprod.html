<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>2.3 Inner product spaces | Multivariate Statistics</title>
  <meta name="description" content="The lecture notes for MATH3030/4068: Multivariate Analysis / Applied Multivariate Statistics" />
  <meta name="generator" content="bookdown 0.24.4 and GitBook 2.6.7" />

  <meta property="og:title" content="2.3 Inner product spaces | Multivariate Statistics" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="The lecture notes for MATH3030/4068: Multivariate Analysis / Applied Multivariate Statistics" />
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="2.3 Inner product spaces | Multivariate Statistics" />
  
  <meta name="twitter:description" content="The lecture notes for MATH3030/4068: Multivariate Analysis / Applied Multivariate Statistics" />
  

<meta name="author" content="Prof. Richard Wilkinson" />



  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="2.2-linalg-vecspaces.html"/>
<link rel="next" href="2.4-centering-matrix.html"/>
<script src="libs/header-attrs-2.11/header-attrs.js"></script>
<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections-1.0.1/anchor-sections.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.0.1/anchor-sections.js"></script>
<script src="libs/kePrint-0.0.1/kePrint.js"></script>
<link href="libs/lightable-0.0.1/lightable.css" rel="stylesheet" />


<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>


<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Applied multivariate statistics</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Introduction</a></li>
<li class="chapter" data-level="" data-path="part-i-prerequisites.html"><a href="part-i-prerequisites.html"><i class="fa fa-check"></i>PART I: Prerequisites</a></li>
<li class="chapter" data-level="1" data-path="1-stat-prelim.html"><a href="1-stat-prelim.html"><i class="fa fa-check"></i><b>1</b> Statistical Preliminaries</a>
<ul>
<li class="chapter" data-level="1.1" data-path="1.1-notation.html"><a href="1.1-notation.html"><i class="fa fa-check"></i><b>1.1</b> Notation</a>
<ul>
<li class="chapter" data-level="1.1.1" data-path="1.1-notation.html"><a href="1.1-notation.html#example-datasets"><i class="fa fa-check"></i><b>1.1.1</b> Example datasets</a></li>
<li class="chapter" data-level="1.1.2" data-path="1.1-notation.html"><a href="1.1-notation.html#aims-of-multivariate-data-analysis"><i class="fa fa-check"></i><b>1.1.2</b> Aims of multivariate data analysis</a></li>
</ul></li>
<li class="chapter" data-level="1.2" data-path="1.2-exploratory-data-analysis-eda.html"><a href="1.2-exploratory-data-analysis-eda.html"><i class="fa fa-check"></i><b>1.2</b> Exploratory data analysis (EDA)</a>
<ul>
<li class="chapter" data-level="1.2.1" data-path="1.2-exploratory-data-analysis-eda.html"><a href="1.2-exploratory-data-analysis-eda.html#data-visualization"><i class="fa fa-check"></i><b>1.2.1</b> Data visualization</a></li>
<li class="chapter" data-level="1.2.2" data-path="1.2-exploratory-data-analysis-eda.html"><a href="1.2-exploratory-data-analysis-eda.html#summary-statistics"><i class="fa fa-check"></i><b>1.2.2</b> Summary statistics</a></li>
</ul></li>
<li class="chapter" data-level="1.3" data-path="1.3-randvec.html"><a href="1.3-randvec.html"><i class="fa fa-check"></i><b>1.3</b> Random vectors and matrices</a>
<ul>
<li class="chapter" data-level="1.3.1" data-path="1.3-randvec.html"><a href="1.3-randvec.html#estimators"><i class="fa fa-check"></i><b>1.3.1</b> Estimators</a></li>
</ul></li>
<li class="chapter" data-level="1.4" data-path="1.4-computer-tasks.html"><a href="1.4-computer-tasks.html"><i class="fa fa-check"></i><b>1.4</b> Computer tasks</a></li>
<li class="chapter" data-level="1.5" data-path="1.5-exercises.html"><a href="1.5-exercises.html"><i class="fa fa-check"></i><b>1.5</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="2-linalg-prelim.html"><a href="2-linalg-prelim.html"><i class="fa fa-check"></i><b>2</b> Review of linear algebra</a>
<ul>
<li class="chapter" data-level="2.1" data-path="2.1-linalg-basics.html"><a href="2.1-linalg-basics.html"><i class="fa fa-check"></i><b>2.1</b> Basics</a>
<ul>
<li class="chapter" data-level="2.1.1" data-path="2.1-linalg-basics.html"><a href="2.1-linalg-basics.html#notation-1"><i class="fa fa-check"></i><b>2.1.1</b> Notation</a></li>
<li class="chapter" data-level="2.1.2" data-path="2.1-linalg-basics.html"><a href="2.1-linalg-basics.html#elementary-matrix-operations"><i class="fa fa-check"></i><b>2.1.2</b> Elementary matrix operations</a></li>
<li class="chapter" data-level="2.1.3" data-path="2.1-linalg-basics.html"><a href="2.1-linalg-basics.html#special-matrices"><i class="fa fa-check"></i><b>2.1.3</b> Special matrices</a></li>
<li class="chapter" data-level="2.1.4" data-path="2.1-linalg-basics.html"><a href="2.1-linalg-basics.html#vectordiff"><i class="fa fa-check"></i><b>2.1.4</b> Vector Differentiation</a></li>
</ul></li>
<li class="chapter" data-level="2.2" data-path="2.2-linalg-vecspaces.html"><a href="2.2-linalg-vecspaces.html"><i class="fa fa-check"></i><b>2.2</b> Vector spaces</a>
<ul>
<li class="chapter" data-level="2.2.1" data-path="2.2-linalg-vecspaces.html"><a href="2.2-linalg-vecspaces.html#linear-independence"><i class="fa fa-check"></i><b>2.2.1</b> Linear independence</a></li>
<li class="chapter" data-level="2.2.2" data-path="2.2-linalg-vecspaces.html"><a href="2.2-linalg-vecspaces.html#colsspace"><i class="fa fa-check"></i><b>2.2.2</b> Row and column spaces</a></li>
<li class="chapter" data-level="2.2.3" data-path="2.2-linalg-vecspaces.html"><a href="2.2-linalg-vecspaces.html#linear-transformations"><i class="fa fa-check"></i><b>2.2.3</b> Linear transformations</a></li>
</ul></li>
<li class="chapter" data-level="2.3" data-path="2.3-linalg-innerprod.html"><a href="2.3-linalg-innerprod.html"><i class="fa fa-check"></i><b>2.3</b> Inner product spaces</a>
<ul>
<li class="chapter" data-level="2.3.1" data-path="2.3-linalg-innerprod.html"><a href="2.3-linalg-innerprod.html#normed"><i class="fa fa-check"></i><b>2.3.1</b> Distances, and angles</a></li>
<li class="chapter" data-level="2.3.2" data-path="2.3-linalg-innerprod.html"><a href="2.3-linalg-innerprod.html#orthogonal-matrices"><i class="fa fa-check"></i><b>2.3.2</b> Orthogonal matrices</a></li>
<li class="chapter" data-level="2.3.3" data-path="2.3-linalg-innerprod.html"><a href="2.3-linalg-innerprod.html#projection-matrix"><i class="fa fa-check"></i><b>2.3.3</b> Projections</a></li>
</ul></li>
<li class="chapter" data-level="2.4" data-path="2.4-centering-matrix.html"><a href="2.4-centering-matrix.html"><i class="fa fa-check"></i><b>2.4</b> The Centering Matrix</a></li>
<li class="chapter" data-level="2.5" data-path="2.5-tasks-ch2.html"><a href="2.5-tasks-ch2.html"><i class="fa fa-check"></i><b>2.5</b> Computer tasks</a></li>
<li class="chapter" data-level="2.6" data-path="2.6-exercises-ch2.html"><a href="2.6-exercises-ch2.html"><i class="fa fa-check"></i><b>2.6</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="3-linalg-decomp.html"><a href="3-linalg-decomp.html"><i class="fa fa-check"></i><b>3</b> Matrix decompositions</a>
<ul>
<li class="chapter" data-level="3.1" data-path="3.1-matrix-matrix.html"><a href="3.1-matrix-matrix.html"><i class="fa fa-check"></i><b>3.1</b> Matrix-matrix products</a></li>
<li class="chapter" data-level="3.2" data-path="3.2-spectraleigen-decomposition.html"><a href="3.2-spectraleigen-decomposition.html"><i class="fa fa-check"></i><b>3.2</b> Spectral/eigen decomposition</a>
<ul>
<li class="chapter" data-level="3.2.1" data-path="3.2-spectraleigen-decomposition.html"><a href="3.2-spectraleigen-decomposition.html#eigenvalues-and-eigenvectors"><i class="fa fa-check"></i><b>3.2.1</b> Eigenvalues and eigenvectors</a></li>
<li class="chapter" data-level="3.2.2" data-path="3.2-spectraleigen-decomposition.html"><a href="3.2-spectraleigen-decomposition.html#spectral-decomposition"><i class="fa fa-check"></i><b>3.2.2</b> Spectral decomposition</a></li>
<li class="chapter" data-level="3.2.3" data-path="3.2-spectraleigen-decomposition.html"><a href="3.2-spectraleigen-decomposition.html#matrixroots"><i class="fa fa-check"></i><b>3.2.3</b> Matrix square roots</a></li>
</ul></li>
<li class="chapter" data-level="3.3" data-path="3.3-linalg-SVD.html"><a href="3.3-linalg-SVD.html"><i class="fa fa-check"></i><b>3.3</b> Singular Value Decomposition (SVD)</a>
<ul>
<li class="chapter" data-level="3.3.1" data-path="3.3-linalg-SVD.html"><a href="3.3-linalg-SVD.html#examples"><i class="fa fa-check"></i><b>3.3.1</b> Examples</a></li>
</ul></li>
<li class="chapter" data-level="3.4" data-path="3.4-svdopt.html"><a href="3.4-svdopt.html"><i class="fa fa-check"></i><b>3.4</b> SVD optimization results</a></li>
<li class="chapter" data-level="3.5" data-path="3.5-lowrank.html"><a href="3.5-lowrank.html"><i class="fa fa-check"></i><b>3.5</b> Low-rank approximation</a>
<ul>
<li class="chapter" data-level="3.5.1" data-path="3.5-lowrank.html"><a href="3.5-lowrank.html#matrix-norms"><i class="fa fa-check"></i><b>3.5.1</b> Matrix norms</a></li>
<li class="chapter" data-level="3.5.2" data-path="3.5-lowrank.html"><a href="3.5-lowrank.html#eckart-young-mirsky-theorem"><i class="fa fa-check"></i><b>3.5.2</b> Eckart-Young-Mirsky Theorem</a></li>
<li class="chapter" data-level="3.5.3" data-path="3.5-lowrank.html"><a href="3.5-lowrank.html#example-image-compression"><i class="fa fa-check"></i><b>3.5.3</b> Example: image compression</a></li>
</ul></li>
<li class="chapter" data-level="3.6" data-path="3.6-tasks-ch3.html"><a href="3.6-tasks-ch3.html"><i class="fa fa-check"></i><b>3.6</b> Computer tasks</a></li>
<li class="chapter" data-level="3.7" data-path="3.7-exercises-ch3.html"><a href="3.7-exercises-ch3.html"><i class="fa fa-check"></i><b>3.7</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="part-ii-dimension-reduction-methods.html"><a href="part-ii-dimension-reduction-methods.html"><i class="fa fa-check"></i>PART II: Dimension reduction methods</a>
<ul>
<li class="chapter" data-level="" data-path="part-ii-dimension-reduction-methods.html"><a href="part-ii-dimension-reduction-methods.html#a-warning"><i class="fa fa-check"></i>A warning</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="4-pca.html"><a href="4-pca.html"><i class="fa fa-check"></i><b>4</b> Principal Component Analysis (PCA)</a>
<ul>
<li class="chapter" data-level="4.1" data-path="4.1-pca-an-informal-introduction.html"><a href="4.1-pca-an-informal-introduction.html"><i class="fa fa-check"></i><b>4.1</b> PCA: an informal introduction</a>
<ul>
<li class="chapter" data-level="4.1.1" data-path="4.1-pca-an-informal-introduction.html"><a href="4.1-pca-an-informal-introduction.html#notation-recap"><i class="fa fa-check"></i><b>4.1.1</b> Notation recap</a></li>
<li class="chapter" data-level="4.1.2" data-path="4.1-pca-an-informal-introduction.html"><a href="4.1-pca-an-informal-introduction.html#first-principal-component"><i class="fa fa-check"></i><b>4.1.2</b> First principal component</a></li>
<li class="chapter" data-level="4.1.3" data-path="4.1-pca-an-informal-introduction.html"><a href="4.1-pca-an-informal-introduction.html#second-principal-component"><i class="fa fa-check"></i><b>4.1.3</b> Second principal component</a></li>
<li class="chapter" data-level="4.1.4" data-path="4.1-pca-an-informal-introduction.html"><a href="4.1-pca-an-informal-introduction.html#geometric-interpretation-1"><i class="fa fa-check"></i><b>4.1.4</b> Geometric interpretation</a></li>
<li class="chapter" data-level="4.1.5" data-path="4.1-pca-an-informal-introduction.html"><a href="4.1-pca-an-informal-introduction.html#example"><i class="fa fa-check"></i><b>4.1.5</b> Example</a></li>
<li class="chapter" data-level="4.1.6" data-path="4.1-pca-an-informal-introduction.html"><a href="4.1-pca-an-informal-introduction.html#example-iris"><i class="fa fa-check"></i><b>4.1.6</b> Example: Iris</a></li>
</ul></li>
<li class="chapter" data-level="4.2" data-path="4.2-pca-a-formal-description-with-proofs.html"><a href="4.2-pca-a-formal-description-with-proofs.html"><i class="fa fa-check"></i><b>4.2</b> PCA: a formal description with proofs</a>
<ul>
<li class="chapter" data-level="4.2.1" data-path="4.2-pca-a-formal-description-with-proofs.html"><a href="4.2-pca-a-formal-description-with-proofs.html#properties-of-principal-components"><i class="fa fa-check"></i><b>4.2.1</b> Properties of principal components</a></li>
<li class="chapter" data-level="4.2.2" data-path="4.2-pca-a-formal-description-with-proofs.html"><a href="4.2-pca-a-formal-description-with-proofs.html#pca:football"><i class="fa fa-check"></i><b>4.2.2</b> Example: Football</a></li>
<li class="chapter" data-level="4.2.3" data-path="4.2-pca-a-formal-description-with-proofs.html"><a href="4.2-pca-a-formal-description-with-proofs.html#pcawithR"><i class="fa fa-check"></i><b>4.2.3</b> PCA based on <span class="math inline">\(\mathbf R\)</span> versus PCA based on <span class="math inline">\(\mathbf S\)</span></a></li>
<li class="chapter" data-level="4.2.4" data-path="4.2-pca-a-formal-description-with-proofs.html"><a href="4.2-pca-a-formal-description-with-proofs.html#population-pca"><i class="fa fa-check"></i><b>4.2.4</b> Population PCA</a></li>
<li class="chapter" data-level="4.2.5" data-path="4.2-pca-a-formal-description-with-proofs.html"><a href="4.2-pca-a-formal-description-with-proofs.html#pca-under-transformations-of-variables"><i class="fa fa-check"></i><b>4.2.5</b> PCA under transformations of variables</a></li>
</ul></li>
<li class="chapter" data-level="4.3" data-path="4.3-an-alternative-view-of-pca.html"><a href="4.3-an-alternative-view-of-pca.html"><i class="fa fa-check"></i><b>4.3</b> An alternative view of PCA</a>
<ul>
<li class="chapter" data-level="4.3.1" data-path="4.3-an-alternative-view-of-pca.html"><a href="4.3-an-alternative-view-of-pca.html#pca-mnist"><i class="fa fa-check"></i><b>4.3.1</b> Example: MNIST handwritten digits</a></li>
</ul></li>
<li class="chapter" data-level="4.4" data-path="4.4-pca-comptask.html"><a href="4.4-pca-comptask.html"><i class="fa fa-check"></i><b>4.4</b> Computer tasks</a></li>
<li class="chapter" data-level="4.5" data-path="4.5-exercises-1.html"><a href="4.5-exercises-1.html"><i class="fa fa-check"></i><b>4.5</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="5-cca.html"><a href="5-cca.html"><i class="fa fa-check"></i><b>5</b> Canonical Correlation Analysis (CCA)</a>
<ul>
<li class="chapter" data-level="5.1" data-path="5.1-cca1.html"><a href="5.1-cca1.html"><i class="fa fa-check"></i><b>5.1</b> The first pair of canonical variables</a>
<ul>
<li class="chapter" data-level="5.1.1" data-path="5.1-cca1.html"><a href="5.1-cca1.html#the-first-canonical-components"><i class="fa fa-check"></i><b>5.1.1</b> The first canonical components</a></li>
<li class="chapter" data-level="5.1.2" data-path="5.1-cca1.html"><a href="5.1-cca1.html#premcca"><i class="fa fa-check"></i><b>5.1.2</b> Example: Premier league football</a></li>
</ul></li>
<li class="chapter" data-level="5.2" data-path="5.2-the-full-set-of-canonical-correlations.html"><a href="5.2-the-full-set-of-canonical-correlations.html"><i class="fa fa-check"></i><b>5.2</b> The full set of canonical correlations</a>
<ul>
<li class="chapter" data-level="5.2.1" data-path="5.2-the-full-set-of-canonical-correlations.html"><a href="5.2-the-full-set-of-canonical-correlations.html#example-continued"><i class="fa fa-check"></i><b>5.2.1</b> Example continued</a></li>
</ul></li>
<li class="chapter" data-level="5.3" data-path="5.3-properties.html"><a href="5.3-properties.html"><i class="fa fa-check"></i><b>5.3</b> Properties</a>
<ul>
<li class="chapter" data-level="5.3.1" data-path="5.3-properties.html"><a href="5.3-properties.html#connection-with-linear-regression-when-q1"><i class="fa fa-check"></i><b>5.3.1</b> Connection with linear regression when <span class="math inline">\(q=1\)</span></a></li>
<li class="chapter" data-level="5.3.2" data-path="5.3-properties.html"><a href="5.3-properties.html#invarianceequivariance-properties-of-cca"><i class="fa fa-check"></i><b>5.3.2</b> Invariance/equivariance properties of CCA</a></li>
</ul></li>
<li class="chapter" data-level="5.4" data-path="5.4-computer-tasks-1.html"><a href="5.4-computer-tasks-1.html"><i class="fa fa-check"></i><b>5.4</b> Computer tasks</a></li>
<li class="chapter" data-level="5.5" data-path="5.5-exercises-2.html"><a href="5.5-exercises-2.html"><i class="fa fa-check"></i><b>5.5</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="6-mds.html"><a href="6-mds.html"><i class="fa fa-check"></i><b>6</b> Multidimensional Scaling (MDS)</a>
<ul>
<li class="chapter" data-level="6.1" data-path="6.1-classical-mds.html"><a href="6.1-classical-mds.html"><i class="fa fa-check"></i><b>6.1</b> Classical MDS</a>
<ul>
<li class="chapter" data-level="6.1.1" data-path="6.1-classical-mds.html"><a href="6.1-classical-mds.html#non-euclidean-distance-matrices"><i class="fa fa-check"></i><b>6.1.1</b> Non-Euclidean distance matrices</a></li>
<li class="chapter" data-level="6.1.2" data-path="6.1-classical-mds.html"><a href="6.1-classical-mds.html#principal-coordinate-analysis"><i class="fa fa-check"></i><b>6.1.2</b> Principal Coordinate Analysis</a></li>
</ul></li>
<li class="chapter" data-level="6.2" data-path="6.2-similarity.html"><a href="6.2-similarity.html"><i class="fa fa-check"></i><b>6.2</b> Similarity measures</a>
<ul>
<li class="chapter" data-level="6.2.1" data-path="6.2-similarity.html"><a href="6.2-similarity.html#binary-attributes"><i class="fa fa-check"></i><b>6.2.1</b> Binary attributes</a></li>
<li class="chapter" data-level="6.2.2" data-path="6.2-similarity.html"><a href="6.2-similarity.html#example-classical-mds-with-the-mnist-data"><i class="fa fa-check"></i><b>6.2.2</b> Example: Classical MDS with the MNIST data</a></li>
</ul></li>
<li class="chapter" data-level="6.3" data-path="6.3-non-metric-mds.html"><a href="6.3-non-metric-mds.html"><i class="fa fa-check"></i><b>6.3</b> Non-metric MDS</a></li>
<li class="chapter" data-level="6.4" data-path="6.4-exercises-3.html"><a href="6.4-exercises-3.html"><i class="fa fa-check"></i><b>6.4</b> Exercises</a></li>
<li class="chapter" data-level="6.5" data-path="6.5-computer-tasks-2.html"><a href="6.5-computer-tasks-2.html"><i class="fa fa-check"></i><b>6.5</b> Computer Tasks</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="part-iii-inference-using-the-multivariate-normal-distribution-mvn.html"><a href="part-iii-inference-using-the-multivariate-normal-distribution-mvn.html"><i class="fa fa-check"></i>Part III: Inference using the Multivariate Normal Distribution (MVN)</a></li>
<li class="chapter" data-level="7" data-path="7-multinormal.html"><a href="7-multinormal.html"><i class="fa fa-check"></i><b>7</b> The Multivariate Normal Distribution</a>
<ul>
<li class="chapter" data-level="7.1" data-path="7.1-definition-and-properties-of-the-mvn.html"><a href="7.1-definition-and-properties-of-the-mvn.html"><i class="fa fa-check"></i><b>7.1</b> Definition and Properties of the MVN</a>
<ul>
<li class="chapter" data-level="7.1.1" data-path="7.1-definition-and-properties-of-the-mvn.html"><a href="7.1-definition-and-properties-of-the-mvn.html#basics"><i class="fa fa-check"></i><b>7.1.1</b> Basics</a></li>
<li class="chapter" data-level="7.1.2" data-path="7.1-definition-and-properties-of-the-mvn.html"><a href="7.1-definition-and-properties-of-the-mvn.html#transformations"><i class="fa fa-check"></i><b>7.1.2</b> Transformations</a></li>
<li class="chapter" data-level="7.1.3" data-path="7.1-definition-and-properties-of-the-mvn.html"><a href="7.1-definition-and-properties-of-the-mvn.html#independence"><i class="fa fa-check"></i><b>7.1.3</b> Independence</a></li>
<li class="chapter" data-level="7.1.4" data-path="7.1-definition-and-properties-of-the-mvn.html"><a href="7.1-definition-and-properties-of-the-mvn.html#confidence-ellipses"><i class="fa fa-check"></i><b>7.1.4</b> Confidence ellipses</a></li>
<li class="chapter" data-level="7.1.5" data-path="7.1-definition-and-properties-of-the-mvn.html"><a href="7.1-definition-and-properties-of-the-mvn.html#sampling-results-for-the-mvn"><i class="fa fa-check"></i><b>7.1.5</b> Sampling results for the MVN</a></li>
</ul></li>
<li class="chapter" data-level="7.2" data-path="7.2-the-wishart-distribution.html"><a href="7.2-the-wishart-distribution.html"><i class="fa fa-check"></i><b>7.2</b> The Wishart distribution</a>
<ul>
<li class="chapter" data-level="7.2.1" data-path="7.2-the-wishart-distribution.html"><a href="7.2-the-wishart-distribution.html#properties-1"><i class="fa fa-check"></i><b>7.2.1</b> Properties</a></li>
<li class="chapter" data-level="7.2.2" data-path="7.2-the-wishart-distribution.html"><a href="7.2-the-wishart-distribution.html#cochrans-theorem"><i class="fa fa-check"></i><b>7.2.2</b> Cochran’s theorem</a></li>
</ul></li>
<li class="chapter" data-level="7.3" data-path="7.3-hotellings-t2-distribution.html"><a href="7.3-hotellings-t2-distribution.html"><i class="fa fa-check"></i><b>7.3</b> Hotelling’s <span class="math inline">\(T^2\)</span> distribution</a></li>
<li class="chapter" data-level="7.4" data-path="7.4-inference-based-on-the-mvn.html"><a href="7.4-inference-based-on-the-mvn.html"><i class="fa fa-check"></i><b>7.4</b> Inference based on the MVN</a>
<ul>
<li class="chapter" data-level="7.4.1" data-path="7.4-inference-based-on-the-mvn.html"><a href="7.4-inference-based-on-the-mvn.html#onesampleSigma"><i class="fa fa-check"></i><b>7.4.1</b> <span class="math inline">\(\boldsymbol{\Sigma}\)</span> known</a></li>
<li class="chapter" data-level="7.4.2" data-path="7.4-inference-based-on-the-mvn.html"><a href="7.4-inference-based-on-the-mvn.html#onesample"><i class="fa fa-check"></i><b>7.4.2</b> <span class="math inline">\(\boldsymbol{\Sigma}\)</span> unknown: 1 sample</a></li>
<li class="chapter" data-level="7.4.3" data-path="7.4-inference-based-on-the-mvn.html"><a href="7.4-inference-based-on-the-mvn.html#boldsymbolsigma-unknown-2-samples"><i class="fa fa-check"></i><b>7.4.3</b> <span class="math inline">\(\boldsymbol{\Sigma}\)</span> unknown: 2 samples</a></li>
</ul></li>
<li class="chapter" data-level="7.5" data-path="7.5-exercises-4.html"><a href="7.5-exercises-4.html"><i class="fa fa-check"></i><b>7.5</b> Exercises</a></li>
<li class="chapter" data-level="7.6" data-path="7.6-computer-tasks-3.html"><a href="7.6-computer-tasks-3.html"><i class="fa fa-check"></i><b>7.6</b> Computer tasks</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="part-iv-classification-and-clustering.html"><a href="part-iv-classification-and-clustering.html"><i class="fa fa-check"></i>Part IV: Classification and Clustering</a></li>
<li class="chapter" data-level="8" data-path="8-lda.html"><a href="8-lda.html"><i class="fa fa-check"></i><b>8</b> Discriminant analysis</a>
<ul>
<li class="chapter" data-level="" data-path="8-lda.html"><a href="8-lda.html#linear-discriminant-analysis"><i class="fa fa-check"></i>Linear discriminant analysis</a></li>
<li class="chapter" data-level="8.1" data-path="8.1-lda-ML.html"><a href="8.1-lda-ML.html"><i class="fa fa-check"></i><b>8.1</b> Maximum likelihood (ML) discriminant rule</a>
<ul>
<li class="chapter" data-level="8.1.1" data-path="8.1-lda-ML.html"><a href="8.1-lda-ML.html#multivariate-normal-populations"><i class="fa fa-check"></i><b>8.1.1</b> Multivariate normal populations</a></li>
<li class="chapter" data-level="8.1.2" data-path="8.1-lda-ML.html"><a href="8.1-lda-ML.html#sample-lda"><i class="fa fa-check"></i><b>8.1.2</b> The sample ML discriminant rule</a></li>
<li class="chapter" data-level="8.1.3" data-path="8.1-lda-ML.html"><a href="8.1-lda-ML.html#two-populations"><i class="fa fa-check"></i><b>8.1.3</b> Two populations</a></li>
<li class="chapter" data-level="8.1.4" data-path="8.1-lda-ML.html"><a href="8.1-lda-ML.html#more-than-two-populations"><i class="fa fa-check"></i><b>8.1.4</b> More than two populations</a></li>
</ul></li>
<li class="chapter" data-level="8.2" data-path="8.2-lda-Bayes.html"><a href="8.2-lda-Bayes.html"><i class="fa fa-check"></i><b>8.2</b> Bayes discriminant rule</a>
<ul>
<li class="chapter" data-level="8.2.1" data-path="8.2-lda-Bayes.html"><a href="8.2-lda-Bayes.html#example-lda-using-the-iris-data"><i class="fa fa-check"></i><b>8.2.1</b> Example: LDA using the Iris data</a></li>
<li class="chapter" data-level="8.2.2" data-path="8.2-lda-Bayes.html"><a href="8.2-lda-Bayes.html#quadratic-discriminant-analysis-qda"><i class="fa fa-check"></i><b>8.2.2</b> Quadratic Discriminant Analysis (QDA)</a></li>
<li class="chapter" data-level="8.2.3" data-path="8.2-lda-Bayes.html"><a href="8.2-lda-Bayes.html#prediction-accuracy"><i class="fa fa-check"></i><b>8.2.3</b> Prediction accuracy</a></li>
</ul></li>
<li class="chapter" data-level="8.3" data-path="8.3-FLDA.html"><a href="8.3-FLDA.html"><i class="fa fa-check"></i><b>8.3</b> Fisher’s linear discriminant rule</a>
<ul>
<li class="chapter" data-level="8.3.1" data-path="8.3-FLDA.html"><a href="8.3-FLDA.html#iris-example-continued-1"><i class="fa fa-check"></i><b>8.3.1</b> Iris example continued</a></li>
<li class="chapter" data-level="8.3.2" data-path="8.3-FLDA.html"><a href="8.3-FLDA.html#links-between-methods"><i class="fa fa-check"></i><b>8.3.2</b> Links between methods</a></li>
</ul></li>
<li class="chapter" data-level="8.4" data-path="8.4-computer-tasks-4.html"><a href="8.4-computer-tasks-4.html"><i class="fa fa-check"></i><b>8.4</b> Computer tasks</a></li>
<li class="chapter" data-level="8.5" data-path="8.5-exercises-5.html"><a href="8.5-exercises-5.html"><i class="fa fa-check"></i><b>8.5</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="9" data-path="9-cluster.html"><a href="9-cluster.html"><i class="fa fa-check"></i><b>9</b> Cluster Analysis</a>
<ul>
<li class="chapter" data-level="9.1" data-path="9.1-k-means-clustering.html"><a href="9.1-k-means-clustering.html"><i class="fa fa-check"></i><b>9.1</b> K-means clustering</a>
<ul>
<li class="chapter" data-level="9.1.1" data-path="9.1-k-means-clustering.html"><a href="9.1-k-means-clustering.html#estimating-boldsymbol-delta"><i class="fa fa-check"></i><b>9.1.1</b> Estimating <span class="math inline">\(\boldsymbol \delta\)</span></a></li>
<li class="chapter" data-level="9.1.2" data-path="9.1-k-means-clustering.html"><a href="9.1-k-means-clustering.html#k-means"><i class="fa fa-check"></i><b>9.1.2</b> K-means</a></li>
<li class="chapter" data-level="9.1.3" data-path="9.1-k-means-clustering.html"><a href="9.1-k-means-clustering.html#example-iris-data"><i class="fa fa-check"></i><b>9.1.3</b> Example: Iris data</a></li>
<li class="chapter" data-level="9.1.4" data-path="9.1-k-means-clustering.html"><a href="9.1-k-means-clustering.html#choosing-k"><i class="fa fa-check"></i><b>9.1.4</b> Choosing <span class="math inline">\(K\)</span></a></li>
</ul></li>
<li class="chapter" data-level="9.2" data-path="9.2-model-based-clustering.html"><a href="9.2-model-based-clustering.html"><i class="fa fa-check"></i><b>9.2</b> Model-based clustering</a>
<ul>
<li class="chapter" data-level="9.2.1" data-path="9.2-model-based-clustering.html"><a href="9.2-model-based-clustering.html#maximum-likelihood-estimation"><i class="fa fa-check"></i><b>9.2.1</b> Maximum-likelihood estimation</a></li>
<li class="chapter" data-level="9.2.2" data-path="9.2-model-based-clustering.html"><a href="9.2-model-based-clustering.html#multivariate-gaussian-clusters"><i class="fa fa-check"></i><b>9.2.2</b> Multivariate Gaussian clusters</a></li>
<li class="chapter" data-level="9.2.3" data-path="9.2-model-based-clustering.html"><a href="9.2-model-based-clustering.html#example-iris-1"><i class="fa fa-check"></i><b>9.2.3</b> Example: Iris</a></li>
</ul></li>
<li class="chapter" data-level="9.3" data-path="9.3-hierarchical-clustering-methods.html"><a href="9.3-hierarchical-clustering-methods.html"><i class="fa fa-check"></i><b>9.3</b> Hierarchical clustering methods</a>
<ul>
<li class="chapter" data-level="9.3.1" data-path="9.3-hierarchical-clustering-methods.html"><a href="9.3-hierarchical-clustering-methods.html#distance-measures"><i class="fa fa-check"></i><b>9.3.1</b> Distance measures</a></li>
<li class="chapter" data-level="9.3.2" data-path="9.3-hierarchical-clustering-methods.html"><a href="9.3-hierarchical-clustering-methods.html#toy-example"><i class="fa fa-check"></i><b>9.3.2</b> Toy Example</a></li>
<li class="chapter" data-level="9.3.3" data-path="9.3-hierarchical-clustering-methods.html"><a href="9.3-hierarchical-clustering-methods.html#comparison-of-methods"><i class="fa fa-check"></i><b>9.3.3</b> Comparison of methods</a></li>
</ul></li>
<li class="chapter" data-level="9.4" data-path="9.4-summary.html"><a href="9.4-summary.html"><i class="fa fa-check"></i><b>9.4</b> Summary</a></li>
<li class="chapter" data-level="9.5" data-path="9.5-computer-tasks-5.html"><a href="9.5-computer-tasks-5.html"><i class="fa fa-check"></i><b>9.5</b> Computer tasks</a></li>
<li class="chapter" data-level="9.6" data-path="9.6-exercises-6.html"><a href="9.6-exercises-6.html"><i class="fa fa-check"></i><b>9.6</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="10" data-path="10-lm.html"><a href="10-lm.html"><i class="fa fa-check"></i><b>10</b> Linear Models</a>
<ul>
<li class="chapter" data-level="" data-path="10-lm.html"><a href="10-lm.html#notation-3"><i class="fa fa-check"></i>Notation</a></li>
<li class="chapter" data-level="10.1" data-path="10.1-ordinary-least-squares-ols.html"><a href="10.1-ordinary-least-squares-ols.html"><i class="fa fa-check"></i><b>10.1</b> Ordinary least squares (OLS)</a>
<ul>
<li class="chapter" data-level="10.1.1" data-path="10.1-ordinary-least-squares-ols.html"><a href="10.1-ordinary-least-squares-ols.html#geometry"><i class="fa fa-check"></i><b>10.1.1</b> Geometry</a></li>
<li class="chapter" data-level="10.1.2" data-path="10.1-ordinary-least-squares-ols.html"><a href="10.1-ordinary-least-squares-ols.html#normal-linear-model"><i class="fa fa-check"></i><b>10.1.2</b> Normal linear model</a></li>
<li class="chapter" data-level="10.1.3" data-path="10.1-ordinary-least-squares-ols.html"><a href="10.1-ordinary-least-squares-ols.html#linear-models-in-r"><i class="fa fa-check"></i><b>10.1.3</b> Linear models in R</a></li>
<li class="chapter" data-level="10.1.4" data-path="10.1-ordinary-least-squares-ols.html"><a href="10.1-ordinary-least-squares-ols.html#problems-with-ols"><i class="fa fa-check"></i><b>10.1.4</b> Problems with OLS</a></li>
</ul></li>
<li class="chapter" data-level="10.2" data-path="10.2-principal-component-regression-pcr.html"><a href="10.2-principal-component-regression-pcr.html"><i class="fa fa-check"></i><b>10.2</b> Principal component regression (PCR)</a>
<ul>
<li class="chapter" data-level="10.2.1" data-path="10.2-principal-component-regression-pcr.html"><a href="10.2-principal-component-regression-pcr.html#pcr-in-r"><i class="fa fa-check"></i><b>10.2.1</b> PCR in R</a></li>
</ul></li>
<li class="chapter" data-level="10.3" data-path="10.3-shrinkage-methods.html"><a href="10.3-shrinkage-methods.html"><i class="fa fa-check"></i><b>10.3</b> Shrinkage methods</a>
<ul>
<li class="chapter" data-level="10.3.1" data-path="10.3-shrinkage-methods.html"><a href="10.3-shrinkage-methods.html#ridge-regression-in-r"><i class="fa fa-check"></i><b>10.3.1</b> Ridge regression in R</a></li>
</ul></li>
<li class="chapter" data-level="10.4" data-path="10.4-multi-output-linear-model.html"><a href="10.4-multi-output-linear-model.html"><i class="fa fa-check"></i><b>10.4</b> Multi-output Linear Model</a>
<ul>
<li class="chapter" data-level="10.4.1" data-path="10.4-multi-output-linear-model.html"><a href="10.4-multi-output-linear-model.html#normal-linear-model-1"><i class="fa fa-check"></i><b>10.4.1</b> Normal linear model</a></li>
<li class="chapter" data-level="10.4.2" data-path="10.4-multi-output-linear-model.html"><a href="10.4-multi-output-linear-model.html#reduced-rank-regression"><i class="fa fa-check"></i><b>10.4.2</b> Reduced rank regression</a></li>
</ul></li>
<li class="chapter" data-level="10.5" data-path="10.5-computer-tasks-6.html"><a href="10.5-computer-tasks-6.html"><i class="fa fa-check"></i><b>10.5</b> Computer tasks</a></li>
<li class="chapter" data-level="10.6" data-path="10.6-exercises-7.html"><a href="10.6-exercises-7.html"><i class="fa fa-check"></i><b>10.6</b> Exercises</a></li>
</ul></li>
<li class="divider"></li>
<li> <a href="https://rich-d-wilkinson.github.io/teaching.html" target="blank">University of Nottingham</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Multivariate Statistics</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="linalg-innerprod" class="section level2" number="2.3">
<h2><span class="header-section-number">2.3</span> Inner product spaces</h2>
<div id="normed" class="section level3" number="2.3.1">
<h3><span class="header-section-number">2.3.1</span> Distances, and angles</h3>
<p>Vector spaces are not particularly interesting from a statistical point of view until we equip them with a sense of geometry, i.e. distance and angle.</p>

<div class="definition">
<span id="def:innerprod" class="definition"><strong>Definition 2.8  </strong></span>A real <strong>inner product space</strong> <span class="math inline">\((V, \langle\cdot,\cdot\rangle)\)</span> is a real vector space <span class="math inline">\(V\)</span> equipped with a map
<span class="math display">\[ \langle\cdot,\cdot\rangle : V \times V \rightarrow \mathbb{R}\]</span>
such that
</div>
<ol style="list-style-type: decimal">
<li><span class="math inline">\(\langle\cdot,\cdot\rangle\)</span> is a linear map in both arguments:
<span class="math display">\[\langle \alpha \mathbf v_1+\beta \mathbf v_2, \mathbf u\rangle = \alpha \langle \mathbf v_1, \mathbf u\rangle + \beta \langle \mathbf v_2, \mathbf u\rangle\]</span>
for all <span class="math inline">\(\mathbf v_1, \mathbf v_2, \mathbf u\in V\)</span> and <span class="math inline">\(\alpha, \beta \in \mathbb{R}\)</span>.</li>
<li><span class="math inline">\(\langle\cdot,\cdot\rangle\)</span> is symmetric in its arguments: <span class="math inline">\(\langle \mathbf v, \mathbf u\rangle = \langle \mathbf u, \mathbf v\rangle\)</span> for all <span class="math inline">\(\mathbf u,\mathbf v\in V\)</span></li>
<li><span class="math inline">\(\langle\cdot,\cdot\rangle\)</span> is positive definite: <span class="math inline">\(\langle \mathbf v, \mathbf v\rangle \geq 0\)</span> for all <span class="math inline">\(\mathbf v\in V\)</span> with equality if and only if <span class="math inline">\(\mathbf v={\mathbf 0}\)</span>.</li>
</ol>
<p>An inner product provides a vector space with the concepts of</p>
<ul>
<li><p><strong>distance</strong>: for all <span class="math inline">\(v\in V\)</span> define the <strong>norm</strong> of <span class="math inline">\(v\)</span> to be <span class="math display">\[||\mathbf v|| = \langle \mathbf v, \mathbf v\rangle ^{\frac{1}{2}}\]</span>
Thus any inner-product space <span class="math inline">\((V, \langle\cdot,\cdot\rangle)\)</span> is also a normed space <span class="math inline">\((V, ||\cdot||)\)</span>, and a metric space <span class="math inline">\((V, d(\mathbf x,\mathbf y)=||\mathbf x-\mathbf y||)\)</span>.</p></li>
<li><p><strong>angle</strong>: for <span class="math inline">\(\mathbf u, \mathbf v\in V\)</span> we define the angle between <span class="math inline">\(\mathbf u\)</span> and <span class="math inline">\(\mathbf v\)</span> to be <span class="math inline">\(\theta\)</span> where
<span class="math display">\[\begin{align*}
\langle \mathbf u,\mathbf v\rangle &amp;= ||\mathbf u||.||\mathbf v||\cos \theta\\
\implies \theta &amp;= \cos^{-1}\left( \frac{\langle \mathbf u, \mathbf v\rangle}{||\mathbf u|| \;||\mathbf v||}\right)
\end{align*}\]</span>
We will primarily be interested in the concept of <strong>orthogonality</strong>. We say <span class="math inline">\(\mathbf u, \mathbf v\in V\)</span> are orthogonal if
<span class="math display">\[\langle \mathbf u, \mathbf v\rangle =0\]</span>
i.e., the <em>angle</em> between them is <span class="math inline">\(\frac{\pi}{2}\)</span>.</p></li>
</ul>
<p>If you have done any functional analysis, you may recall that a Hilbert space is a <em>complete</em> inner-product space, and a Banach space is a complete normed space. This is an applied module, so we will skirt much of the technical detail, but note that some of the proofs formally require us to be working in a Banach or Hilbert space. We will not concern ourselves with such detail.</p>

<div class="example">
<p><span id="exm:Rp3" class="example"><strong>Example 2.11  </strong></span>We will mostly be working with the Euclidean vector spaces <span class="math inline">\(V=\mathbb{R}^n\)</span>, in which we use the <em>Euclidean</em> inner product
<span class="math display">\[\langle \mathbf u, \mathbf v\rangle = \mathbf u^\top \mathbf v\]</span>
sometimes called the <strong>scalar</strong> or <strong>dot product</strong> of <span class="math inline">\(\mathbf u\)</span> and <span class="math inline">\(\mathbf v\)</span>. Sometimes this gets weighted by a matrix so that
<span class="math display">\[\langle \mathbf u, \mathbf v\rangle_Q = \mathbf u^\top \mathbf Q\mathbf v.\]</span></p>
<p>The norm associated with the dot product is the square root of the sum of squared errors, denoted by <span class="math inline">\(|| \cdot ||_2\)</span>.
The <strong>length</strong> of <span class="math inline">\(\mathbf u\)</span> is then
<span class="math display">\[||\mathbf u||_2=\sqrt{\mathbf u^\top \mathbf u} =\left( \sum_{i=1}^n u_i^2\right)^\frac{1}{2}\geq 0.\]</span>
Note that <span class="math inline">\(||\mathbf u||_2=0\)</span> if and only if <span class="math inline">\(\mathbf u={\mathbf 0}_n\)</span> where <span class="math inline">\(\stackrel{n\times 1}{\mathbf 0}_n=(0,0,\dots ,0)^\top\)</span>.</p>
We say <span class="math inline">\(\mathbf u\)</span> is orthogonal to <span class="math inline">\(\mathbf v\)</span> if <span class="math inline">\(\mathbf u^\top \mathbf v=0\)</span>.
For example, if
<span class="math display">\[\mathbf u=\left(\begin{array}{c}1\\2\end{array}\right) \mbox{ and } \mathbf v=\left(\begin{array}{c}-2\\1\end{array}\right)\]</span>
then
<span class="math display">\[||\mathbf u||_2 = \sqrt{5}\mbox{ and } \mathbf u^\top \mathbf v=0.\]</span>
We will write <span class="math inline">\(\mathbf u\perp \mathbf v\)</span> if <span class="math inline">\(\mathbf u\)</span> is orthogonal to <span class="math inline">\(\mathbf v\)</span>.
</div>

<div class="definition">
<span id="def:pnorms" class="definition"><strong>Definition 2.9  </strong></span><strong>p-norm:</strong> The subscript <span class="math inline">\(2\)</span> hints at a wider family of norms. We define the <span class="math inline">\(L_p\)</span> norm to be
<span class="math display">\[|| \mathbf v||_p = \left(\sum_{i=1}^n |v_i|^p\right)^\frac{1}{p}.\]</span>
</div>
<!-- OKAY TO HERE-->
</div>
<div id="orthogonal-matrices" class="section level3" number="2.3.2">
<h3><span class="header-section-number">2.3.2</span> Orthogonal matrices</h3>

<div class="definition">
<p><span id="def:orthogonal" class="definition"><strong>Definition 2.10  </strong></span>A <strong>unit vector</strong> <span class="math inline">\(\mathbf v\)</span> is a vector satisfying <span class="math inline">\(||{\mathbf v}||=1\)</span>, i.e., it is a vector of length <span class="math inline">\(1\)</span>. Vectors <span class="math inline">\(\mathbf u\)</span> and <span class="math inline">\(\mathbf v\)</span> are orthonormal if
<span class="math display">\[||\mathbf u||=||\mathbf v|| = 1 \mbox{ and } \langle \mathbf u, \mathbf v\rangle =0.\]</span></p>
<p>An <span class="math inline">\(n\times n\)</span> matrix <span class="math inline">\({\mathbf Q}\)</span> is an <strong>orthogonal matrix</strong> if <span class="math display">\[{\mathbf Q}\mathbf Q^\top = {\mathbf Q}^\top {\mathbf Q}={\mathbf I}_n.\]</span></p>
<p>Equivalently, a matrix <span class="math inline">\(\mathbf Q\)</span> is orthogonal if <span class="math inline">\({\mathbf Q}^{-1}={\mathbf Q}^\top.\)</span></p>
If <span class="math inline">\({\mathbf Q}=[\mathbf q_1,\ldots, \mathbf q_n]\)</span> is an orthogonal matrix, then the columns <span class="math inline">\(\mathbf q_1, \ldots, \mathbf q_n\)</span> are mutually <strong>orthonormal</strong> vectors, i.e.
<span class="math display">\[
\mathbf q_j^\top \mathbf q_k=\begin{cases} 1 &amp;\hbox{ if }  j=k\\
0 &amp;\hbox{ if }   j \neq k. \\
\end{cases}
\]</span>
</div>
<p><br></p>

<div class="lemma">
<span id="lem:lemorthog" class="lemma"><strong>Lemma 2.2  </strong></span>Let <span class="math inline">\(\mathbf Q\)</span> be a <span class="math inline">\(n\times p\)</span> matrix and suppose <span class="math inline">\(\mathbf Q^\top \mathbf Q=\mathbf I_p\)</span>, where <span class="math inline">\(\mathbf I_p\)</span> is the <span class="math inline">\(p \times p\)</span> identity matrix. If <span class="math inline">\(\mathbf Q\)</span> is a square matrix (<span class="math inline">\(n=p\)</span>), then <span class="math inline">\(\mathbf Q\mathbf Q^\top = \mathbf I_p\)</span>. If <span class="math inline">\(\mathbf Q\)</span> is not square (<span class="math inline">\(n\not =p\)</span>), then <span class="math inline">\(\mathbf Q\mathbf Q^\top \not = I_n\)</span>.
</div>
<div class="proof">
<p><span id="unlabeled-div-2" class="proof"><em>Proof</em>. </span>Suppose <span class="math inline">\(n=p\)</span>, and think of <span class="math inline">\(\mathbf Q\)</span> as a linear map
<span class="math display">\[\begin{align*}
\mathbf Q: &amp;\mathbb{R}^n \rightarrow \mathbb{R}^n\\
&amp;\mathbf v\mapsto \mathbf Q\mathbf v
\end{align*}\]</span>
By the rank-nullity theorem,
<span class="math display">\[\dim \operatorname{Ker}(\mathbf Q) + \dim \operatorname{Im}(\mathbf Q) =n\]</span>
and because <span class="math inline">\(\mathbf Q\)</span> has a left-inverse, we must have <span class="math inline">\(\dim \operatorname{Ker}(\mathbf Q)=0\)</span>, as otherwise <span class="math inline">\(\mathbf Q^\top\)</span> would have to map from a vector space of dimension less than <span class="math inline">\(n\)</span> to <span class="math inline">\(\mathbb{R}^n\)</span>. So <span class="math inline">\(\mathbf Q\)</span> is of full rank, and thus must also have a right inverse, <span class="math inline">\(\mathbf B\)</span> say, with <span class="math inline">\(\mathbf Q\mathbf B=\mathbf I_n\)</span>. If we left multiply by <span class="math inline">\(\mathbf Q^\top\)</span> we get
<span class="math display">\[\begin{align*}
\mathbf Q\mathbf B&amp;=\mathbf I_n\\
\mathbf Q^\top\mathbf Q\mathbf B&amp;=\mathbf Q^\top\\
\mathbf I_n \mathbf B&amp;= \mathbf Q^\top\\
\mathbf B&amp;= \mathbf Q^\top\\
\end{align*}\]</span>
and so we have that <span class="math inline">\(\mathbf Q^{-1}=\mathbf Q^\top\)</span>.</p>
<p>Now suppose <span class="math inline">\(\mathbf Q\)</span> is <span class="math inline">\(n \times p\)</span> with <span class="math inline">\(n\not = p\)</span>. Then as
<span class="math inline">\(\mathbf Q^\top \mathbf Q=\mathbf I_{p\times p}\)</span>, we must have <span class="math inline">\(\operatorname{tr}(\mathbf Q^\top \mathbf Q)=p\)</span>. This implies that
<span class="math display">\[\operatorname{tr}(\mathbf Q\mathbf Q^\top)=\operatorname{tr}(\mathbf Q^\top \mathbf Q)=m\]</span> and so
we cannot have <span class="math inline">\(\mathbf Q\mathbf Q^\top=\mathbf I_{n}\)</span> as <span class="math inline">\(\operatorname{tr}{\mathbf I_{n}}=n\)</span>.</p>
</div>
<p><br></p>

<div class="corollary">
<span id="cor:two1" class="corollary"><strong>Corollary 2.2  </strong></span>If <span class="math inline">\(\mathbf q_1, \ldots , \mathbf q_n\)</span> are mutually orthogonal <span class="math inline">\(n \times 1\)</span> unit vectors then
<span class="math display">\[
\sum_{i=1}^n \mathbf q_i \mathbf q_i^\top = {\mathbf I}_n.
\]</span>
</div>
<div class="proof">
<p><span id="unlabeled-div-3" class="proof"><em>Proof</em>. </span>Let <span class="math inline">\(\mathbf Q\)</span> be the matrix with <span class="math inline">\(i^{th}\)</span> column <span class="math inline">\(\mathbf q_i\)</span>
<span class="math display">\[\mathbf Q=\left(
    \begin{array}{ccc}
    | &amp;&amp;|\\
    \mathbf q_1&amp; \ldots&amp; \mathbf q_n\\
    | &amp;&amp;|
      \end{array}\right).\]</span>
Then <span class="math inline">\(\mathbf Q^\top \mathbf Q=\mathbf I_n\)</span>, and <span class="math inline">\(\mathbf Q\)</span> is <span class="math inline">\(n\times n\)</span>. Thus by Lemma <a href="2.3-linalg-innerprod.html#lem:lemorthog">2.2</a>, we must also have <span class="math inline">\(\mathbf Q\mathbf Q^\top=\mathbf I_n\)</span> and if we think about matrix-matrix multiplication as columns times rows (c.f. section <a href="3.1-matrix-matrix.html#matrix-matrix">3.1</a>), we get
<span class="math display">\[\mathbf I_n=\mathbf Q\mathbf Q^\top=\left(
    \begin{array}{ccc}
    | &amp;&amp;|\\
    \mathbf q_1&amp; \ldots&amp; \mathbf q_n\\
    | &amp;&amp;|
      \end{array}\right) \left(
    \begin{array}{ccc}
    - &amp;\mathbf q_1^\top&amp;-\\
    &amp; \vdots&amp; \\
    - &amp;\mathbf q_n^\top&amp;-
      \end{array}\right) = \sum_{i=1}^n \mathbf q_i \mathbf q_i^\top\]</span>
as required.</p>
</div>
<!-- OKAY TO HERE -->
</div>
<div id="projection-matrix" class="section level3" number="2.3.3">
<h3><span class="header-section-number">2.3.3</span> Projections</h3>

<div class="definition">
<span id="def:projection" class="definition"><strong>Definition 2.11  </strong></span><span class="math inline">\(\stackrel{n \times n}{\mathbf P}\)</span> is a <em>projection</em>
matrix if
<span class="math display">\[\mathbf P^2 =\mathbf P\]</span>
i.e., if it is idempotent.
</div>
<p>View <span class="math inline">\(\mathbf P\)</span> as a map from a vector space <span class="math inline">\(W\)</span> to itself. Let <span class="math inline">\(U=\operatorname{Im}(\mathbf P)\)</span> and <span class="math inline">\(V=\operatorname{Ker}(\mathbf P)\)</span> be the image and kernel of <span class="math inline">\(\mathbf P\)</span>.</p>

<div class="proposition">
<span id="prp:projec1" class="proposition"><strong>Proposition 2.3  </strong></span>We can write <span class="math inline">\(\mathbf w\in W\)</span> as the sum of <span class="math inline">\(\mathbf u\in U\)</span> and <span class="math inline">\(\mathbf v\in V\)</span>.
</div>
<div class="proof">
<p><span id="unlabeled-div-4" class="proof"><em>Proof</em>. </span>Let <span class="math inline">\(\mathbf w\in W\)</span>. Then
<span class="math display">\[\mathbf w= \mathbf I_n \mathbf w=(\mathbf I-\mathbf P)\mathbf w+ \mathbf P\mathbf w\]</span>
Now <span class="math inline">\(\mathbf P\mathbf w\in \operatorname{Im}(\mathbf P)\)</span> and <span class="math inline">\((\mathbf I-\mathbf P)\mathbf w\in \operatorname{Ker}(\mathbf P)\)</span> as
<span class="math display">\[\mathbf P(\mathbf I-\mathbf P)\mathbf w= (\mathbf P-\mathbf P^2)\mathbf w=\boldsymbol 0.\]</span></p>
</div>

<div class="proposition">
<span id="prp:projIP" class="proposition"><strong>Proposition 2.4  </strong></span>If <span class="math inline">\(\stackrel{n \times n}{\mathbf P}\)</span> is a projection matrix then <span class="math inline">\({\mathbf I}_n - \mathbf P\)</span> is also
a projection matrix.
</div>
<p>The kernel and image of <span class="math inline">\(\mathbf I-\mathbf P\)</span> are the image and kernel (respectively) of <span class="math inline">\(\mathbf P\)</span>:
<span class="math display">\[\begin{align*}
\operatorname{Ker}(\mathbf I-\mathbf P) &amp;= U=\operatorname{Im}(\mathbf P)\\
\operatorname{Im}(\mathbf I-\mathbf P) &amp;= V=\operatorname{Ker}(\mathbf P).
\end{align*}\]</span></p>
<!-- OKAY TO HERE -->
<div id="orthogproj" class="section level4" number="2.3.3.1">
<h4><span class="header-section-number">2.3.3.1</span> Orthogonal projection</h4>
<p>We are mostly interested in <strong>orthogonal</strong> projections.</p>

<div class="definition">
<span id="def:orthogproj" class="definition"><strong>Definition 2.12  </strong></span>If <span class="math inline">\(W\)</span> is an inner product space, and <span class="math inline">\(U\)</span> is a subspace of <span class="math inline">\(W\)</span>, then the orthogonal projection of <span class="math inline">\(\mathbf w\in W\)</span> onto <span class="math inline">\(U\)</span> is the unique vector <span class="math inline">\(\mathbf u\in U\)</span> that minimizes
<span class="math display">\[||\mathbf w-\mathbf u||.\]</span>
</div>
<p>In other words, the orthogonal projection of <span class="math inline">\(\mathbf w\)</span> onto <span class="math inline">\(U\)</span> is the <em>best possible approximation</em> of <span class="math inline">\(\mathbf w\)</span> in <span class="math inline">\(U\)</span>.</p>
<p>As above, we can split <span class="math inline">\(W\)</span> into <span class="math inline">\(U\)</span> and its orthogonal compliment
<span class="math display">\[U^\perp = \{\mathbf x\in W: \langle \mathbf x,\mathbf u\rangle = 0\}\]</span>
i.e., <span class="math inline">\(W=U \oplus U^\perp\)</span> so that any <span class="math inline">\(\mathbf w\in W\)</span> can be uniquely written as
<span class="math inline">\(\mathbf w=\mathbf u+\mathbf v\)</span> with <span class="math inline">\(\mathbf u\in U\)</span> and <span class="math inline">\(\mathbf v\in U^\perp\)</span>. This makes clear why we call <span class="math inline">\(\mathbf u= \arg \min_{\mathbf u\in \mathbf U} ||\mathbf w-\mathbf u||\)</span> the <em>orthogonal</em> projection of <span class="math inline">\(\mathbf w\)</span>, it is because it splits <span class="math inline">\(\mathbf w\)</span> into two orthogonal components: <span class="math inline">\(\mathbf w= \mathbf u+\mathbf v\)</span> with <span class="math inline">\(\langle \mathbf u, \mathbf v\rangle=0\)</span>.</p>

<div class="proposition">
<span id="prp:orthogprojection" class="proposition"><strong>Proposition 2.5  </strong></span>If <span class="math inline">\(\{\mathbf u_1, \ldots, \mathbf u_k\}\)</span> is a basis for <span class="math inline">\(U\)</span>, then the orthogonal projection matrix (i.e., the matrix that projects <span class="math inline">\(\mathbf w\in W\)</span> onto <span class="math inline">\(U\)</span>) is
<span class="math display">\[\mathbf P_U = \mathbf A(\mathbf A^\top \mathbf A)^{-1}\mathbf A^\top\]</span> where
<span class="math inline">\(\mathbf A=[\mathbf u_1\; \ldots\; \mathbf u_k]\)</span> is the matrix with columns given by the basis vectors.
</div>
<!-- OKAY TO HERE-->
<div class="proof">
<p><span id="unlabeled-div-5" class="proof"><em>Proof</em>. </span>We need to find <span class="math inline">\(\mathbf u= \sum \lambda_i \mathbf u_i = \mathbf A\boldsymbol \lambda\)</span> that minimizes <span class="math inline">\(||\mathbf w-\mathbf u||\)</span>.</p>
<p><span class="math display">\[\begin{align*}
||\mathbf w-\mathbf u||^2 &amp;= \langle \mathbf w-\mathbf u, \mathbf w-\mathbf u\rangle\\
&amp;= \mathbf w^\top \mathbf w- 2\mathbf u^\top \mathbf w+ \mathbf u^\top \mathbf u\\
&amp;= \mathbf w^\top \mathbf w-2\boldsymbol \lambda^\top \mathbf A^\top \mathbf w+ \boldsymbol \lambda^\top \mathbf A^\top \mathbf A\boldsymbol \lambda.
\end{align*}\]</span></p>
<p>Differentiating with respect to <span class="math inline">\(\boldsymbol \lambda\)</span> and setting equal to zero gives
<span class="math display">\[\boldsymbol 0=-2 \mathbf A^\top \mathbf w+2 \mathbf A^\top \mathbf A\boldsymbol \lambda\]</span>
and hence
<span class="math display">\[ \boldsymbol \lambda= (\mathbf A^\top \mathbf A)^{-1}\mathbf A^\top \mathbf w.\]</span>
The orthogonal projection of <span class="math inline">\(\mathbf w\)</span> is hence
<span class="math display">\[ \mathbf A\boldsymbol \lambda= \mathbf A(\mathbf A^\top \mathbf A)^{-1}\mathbf A^\top \mathbf w\]</span>
and the projection matrix is
<span class="math display">\[\mathbf P_U = \mathbf A(\mathbf A^\top \mathbf A)^{-1}\mathbf A^\top. \]</span></p>
</div>
<p><strong>Notes:</strong></p>
<ol style="list-style-type: decimal">
<li><p>If <span class="math inline">\(\{\mathbf u_1, \ldots, \mathbf u_k\}\)</span> is an orthonormal basis for <span class="math inline">\(U\)</span> then <span class="math inline">\(\mathbf A^\top \mathbf A= \mathbf I\)</span> and <span class="math inline">\(\mathbf P_U = \mathbf A\mathbf A^\top\)</span>. We can then write
<span class="math display">\[\mathbf P_U\mathbf w= \sum_i (\mathbf u_i^\top \mathbf w) \mathbf u_i\]</span>
and
<span class="math display">\[\mathbf P_U = \sum_{i=1}^k \mathbf u_i\mathbf u_i^\top.\]</span>
Note that if <span class="math inline">\(U=W\)</span> (so that <span class="math inline">\(\mathbf P_U\)</span> is a projection from <span class="math inline">\(W\)</span> onto <span class="math inline">\(W\)</span>, i.e., the identity), then <span class="math inline">\(\mathbf A\)</span> is a square matrix (<span class="math inline">\(n\times n\)</span>) and thus <span class="math inline">\(\mathbf A^\top\mathbf A=\mathbf I_n \implies \mathbf A\mathbf A^\top\)</span> and thus <span class="math inline">\(\mathbf P_U=\mathbf I_n\)</span> as required. The coordinates (with respect to the orthonormal basis <span class="math inline">\(\{\mathbf u_1, \ldots, \mathbf u_k\}\)</span>) of a point <span class="math inline">\(\mathbf w\)</span> projected onto <span class="math inline">\(U\)</span> are <span class="math inline">\(\mathbf A^\top \mathbf w\)</span>.</p></li>
<li><p><span class="math inline">\(\mathbf P_U^2=\mathbf P_U\)</span>, so <span class="math inline">\(\mathbf P_U\)</span> is a projection matrix in the sense of definition <a href="2.3-linalg-innerprod.html#def:projection">2.11</a>.</p></li>
<li><p><span class="math inline">\(\mathbf P_U\)</span> is symmetric (<span class="math inline">\(\mathbf P_U^\top=\mathbf P_U\)</span>). This is true for orthogonal projection matrices, but not in general for projection matrices.</p></li>
</ol>

<div class="example">
<span id="exm:proj2" class="example"><strong>Example 2.12  </strong></span>Consider the vector space <span class="math inline">\(\mathbb{R}^2\)</span> and let <span class="math inline">\(\mathbf u=\frac{1}{\sqrt{2}}\left(\begin{array}{c}1\\1\end{array}\right)\)</span>. The projection of <span class="math inline">\(\mathbf v\in \mathbb{R}^2\)</span> onto <span class="math inline">\(\mathbf u\)</span> is given by <span class="math inline">\((\mathbf v^\top \mathbf u) \mathbf u\)</span>. So for example, if <span class="math inline">\(\mathbf v= (2, \; 1)^\top\)</span>, then its projection onto <span class="math inline">\(\mathbf u\)</span> is
<span class="math display">\[\mathbf P_U \mathbf v= \frac{3}{\sqrt{2}}\left(\begin{array}{c}1\\1\end{array}\right).\]</span>
Alternatively, if we treat <span class="math inline">\(\mathbf u\)</span> as a basis for <span class="math inline">\(U\)</span>, then the coordinate of <span class="math inline">\(\mathbf P_U \mathbf v\)</span> with respect to the basis is <span class="math inline">\(3\)</span>.
To check this, draw a picture!
</div>
<!-- OKAY TO HERE -->
</div>
<div id="ch2linreg" class="section level4" number="2.3.3.2">
<h4><span class="header-section-number">2.3.3.2</span> Geometric interpretation of linear regresssion</h4>
<p>Consider the linear regression model
<span class="math display">\[\mathbf y= \mathbf X\boldsymbol \beta+\mathbf e\]</span>
where <span class="math inline">\(\mathbf y\in\mathbb{R}^n\)</span> is the vector of observations, <span class="math inline">\(\mathbf X\)</span> is the <span class="math inline">\(n\times p\)</span> design matrix, <span class="math inline">\(\boldsymbol \beta\)</span> is the <span class="math inline">\(p\times 1\)</span> vector of parameters that we wish to estimate, and <span class="math inline">\(\mathbf e\)</span> is a <span class="math inline">\(n\times 1\)</span> vector of zero-mean errors.</p>
<p>Least-squares regression tries to find the value of <span class="math inline">\(\boldsymbol \beta\in \mathbb{R}^p\)</span> that minimizes the sum of squared errors, i.e., we try to find <span class="math inline">\(\boldsymbol \beta\)</span> to minimize
<span class="math display">\[||\mathbf y- \mathbf X\boldsymbol \beta||_2^2\]</span></p>
<p>We know that <span class="math inline">\(\mathbf X\boldsymbol \beta\)</span> is in the column space of <span class="math inline">\(\mathbf X\)</span>, and so we can see that linear regression aims to find the <em>orthogonal projection</em> onto <span class="math inline">\(\mathcal{C}(X)\)</span>.
<span class="math display">\[\mathbf P_U\mathbf y=\arg \min_{\mathbf y&#39;: \mathbf y&#39; \in \mathcal{C}(X)} ||\mathbf y-\mathbf y&#39;||_2.\]</span></p>
<p>By Proposition <a href="2.3-linalg-innerprod.html#prp:orthogprojection">2.5</a> this is
<span class="math display">\[\mathbf P_U\mathbf y= \mathbf X(\mathbf X^\top \mathbf X)^{-1}\mathbf X^\top \mathbf y=\hat{\mathbf y}\]</span>
which equals the usual prediction obtained in linear regression (<span class="math inline">\(\hat{\mathbf y}\)</span> are often called the fitted values). We can also see that the choice of <span class="math inline">\(\boldsymbol \beta\)</span> that specifies this point in <span class="math inline">\(\mathcal{C}(X)\)</span> is
<span class="math display">\[\hat{\boldsymbol \beta}=(\mathbf X^\top \mathbf X)^{-1}\mathbf X^\top \mathbf y\]</span>
which is the usual least-squares estimator.</p>
</div>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="2.2-linalg-vecspaces.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="2.4-centering-matrix.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["MultivariateStatistics.pdf"],
"search": {
"engine": "fuse",
"options": null
},
"toc": {
"collapse": "section"
},
"pandoc_args": "--top-level-division=[chapter|part]"
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
