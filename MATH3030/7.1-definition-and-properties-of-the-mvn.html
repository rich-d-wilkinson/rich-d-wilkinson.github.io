<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>7.1 Definition and Properties of the MVN | Multivariate Statistics</title>
  <meta name="description" content="The lecture notes for MATH3030/4068: Multivariate Analysis / Applied Multivariate Statistics" />
  <meta name="generator" content="bookdown 0.24.4 and GitBook 2.6.7" />

  <meta property="og:title" content="7.1 Definition and Properties of the MVN | Multivariate Statistics" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="The lecture notes for MATH3030/4068: Multivariate Analysis / Applied Multivariate Statistics" />
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="7.1 Definition and Properties of the MVN | Multivariate Statistics" />
  
  <meta name="twitter:description" content="The lecture notes for MATH3030/4068: Multivariate Analysis / Applied Multivariate Statistics" />
  

<meta name="author" content="Dr Katie Severn" />



  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="7-multinormal.html"/>
<link rel="next" href="7.2-the-wishart-distribution.html"/>
<script src="libs/header-attrs-2.11/header-attrs.js"></script>
<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections-1.0.1/anchor-sections.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.0.1/anchor-sections.js"></script>
<script src="libs/kePrint-0.0.1/kePrint.js"></script>
<link href="libs/lightable-0.0.1/lightable.css" rel="stylesheet" />


<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { color: #008000; } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { color: #008000; font-weight: bold; } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>


<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Applied multivariate statistics</a></li>

<li class="divider"></li>
<li><a href="index.html#introduction" id="toc-introduction">Introduction</a></li>
<li><a href="part-i-prerequisites.html#part-i-prerequisites" id="toc-part-i-prerequisites">PART I: Prerequisites</a></li>
<li><a href="1-stat-prelim.html#stat-prelim" id="toc-stat-prelim"><span class="toc-section-number">1</span> Statistical Preliminaries</a>
<ul>
<li><a href="1.1-notation.html#notation" id="toc-notation"><span class="toc-section-number">1.1</span> Notation</a>
<ul>
<li><a href="1.1-notation.html#example-datasets" id="toc-example-datasets"><span class="toc-section-number">1.1.1</span> Example datasets</a></li>
<li><a href="1.1-notation.html#aims-of-multivariate-data-analysis" id="toc-aims-of-multivariate-data-analysis"><span class="toc-section-number">1.1.2</span> Aims of multivariate data analysis</a></li>
</ul></li>
<li><a href="1.2-exploratory-data-analysis-eda.html#exploratory-data-analysis-eda" id="toc-exploratory-data-analysis-eda"><span class="toc-section-number">1.2</span> Exploratory data analysis (EDA)</a>
<ul>
<li><a href="1.2-exploratory-data-analysis-eda.html#data-visualization" id="toc-data-visualization"><span class="toc-section-number">1.2.1</span> Data visualization</a></li>
<li><a href="1.2-exploratory-data-analysis-eda.html#summary-statistics" id="toc-summary-statistics"><span class="toc-section-number">1.2.2</span> Summary statistics</a></li>
</ul></li>
<li><a href="1.3-randvec.html#randvec" id="toc-randvec"><span class="toc-section-number">1.3</span> Random vectors and matrices</a>
<ul>
<li><a href="1.3-randvec.html#estimators" id="toc-estimators"><span class="toc-section-number">1.3.1</span> Estimators</a></li>
</ul></li>
<li><a href="1.4-computer-tasks.html#computer-tasks" id="toc-computer-tasks"><span class="toc-section-number">1.4</span> Computer tasks</a></li>
<li><a href="1.5-exercises.html#exercises" id="toc-exercises"><span class="toc-section-number">1.5</span> Exercises</a></li>
</ul></li>
<li><a href="2-linalg-prelim.html#linalg-prelim" id="toc-linalg-prelim"><span class="toc-section-number">2</span> Review of linear algebra</a>
<ul>
<li><a href="2.1-linalg-basics.html#linalg-basics" id="toc-linalg-basics"><span class="toc-section-number">2.1</span> Basics</a>
<ul>
<li><a href="2.1-linalg-basics.html#notation-1" id="toc-notation-1"><span class="toc-section-number">2.1.1</span> Notation</a></li>
<li><a href="2.1-linalg-basics.html#elementary-matrix-operations" id="toc-elementary-matrix-operations"><span class="toc-section-number">2.1.2</span> Elementary matrix operations</a></li>
<li><a href="2.1-linalg-basics.html#special-matrices" id="toc-special-matrices"><span class="toc-section-number">2.1.3</span> Special matrices</a></li>
<li><a href="2.1-linalg-basics.html#vectordiff" id="toc-vectordiff"><span class="toc-section-number">2.1.4</span> Vector Differentiation</a></li>
</ul></li>
<li><a href="2.2-linalg-vecspaces.html#linalg-vecspaces" id="toc-linalg-vecspaces"><span class="toc-section-number">2.2</span> Vector spaces</a>
<ul>
<li><a href="2.2-linalg-vecspaces.html#linear-independence" id="toc-linear-independence"><span class="toc-section-number">2.2.1</span> Linear independence</a></li>
<li><a href="2.2-linalg-vecspaces.html#colsspace" id="toc-colsspace"><span class="toc-section-number">2.2.2</span> Row and column spaces</a></li>
<li><a href="2.2-linalg-vecspaces.html#linear-transformations" id="toc-linear-transformations"><span class="toc-section-number">2.2.3</span> Linear transformations</a></li>
</ul></li>
<li><a href="2.3-linalg-innerprod.html#linalg-innerprod" id="toc-linalg-innerprod"><span class="toc-section-number">2.3</span> Inner product spaces</a>
<ul>
<li><a href="2.3-linalg-innerprod.html#normed" id="toc-normed"><span class="toc-section-number">2.3.1</span> Distances, and angles</a></li>
<li><a href="2.3-linalg-innerprod.html#orthogonal-matrices" id="toc-orthogonal-matrices"><span class="toc-section-number">2.3.2</span> Orthogonal matrices</a></li>
<li><a href="2.3-linalg-innerprod.html#projection-matrix" id="toc-projection-matrix"><span class="toc-section-number">2.3.3</span> Projections</a></li>
</ul></li>
<li><a href="2.4-centering-matrix.html#centering-matrix" id="toc-centering-matrix"><span class="toc-section-number">2.4</span> The Centering Matrix</a></li>
<li><a href="2.5-tasks-ch2.html#tasks-ch2" id="toc-tasks-ch2"><span class="toc-section-number">2.5</span> Computer tasks</a></li>
<li><a href="2.6-exercises-ch2.html#exercises-ch2" id="toc-exercises-ch2"><span class="toc-section-number">2.6</span> Exercises</a></li>
</ul></li>
<li><a href="3-linalg-decomp.html#linalg-decomp" id="toc-linalg-decomp"><span class="toc-section-number">3</span> Matrix decompositions</a>
<ul>
<li><a href="3.1-matrix-matrix.html#matrix-matrix" id="toc-matrix-matrix"><span class="toc-section-number">3.1</span> Matrix-matrix products</a></li>
<li><a href="3.2-spectraleigen-decomposition.html#spectraleigen-decomposition" id="toc-spectraleigen-decomposition"><span class="toc-section-number">3.2</span> Spectral/eigen decomposition</a>
<ul>
<li><a href="3.2-spectraleigen-decomposition.html#eigenvalues-and-eigenvectors" id="toc-eigenvalues-and-eigenvectors"><span class="toc-section-number">3.2.1</span> Eigenvalues and eigenvectors</a></li>
<li><a href="3.2-spectraleigen-decomposition.html#spectral-decomposition" id="toc-spectral-decomposition"><span class="toc-section-number">3.2.2</span> Spectral decomposition</a></li>
<li><a href="3.2-spectraleigen-decomposition.html#matrixroots" id="toc-matrixroots"><span class="toc-section-number">3.2.3</span> Matrix square roots</a></li>
</ul></li>
<li><a href="3.3-linalg-SVD.html#linalg-SVD" id="toc-linalg-SVD"><span class="toc-section-number">3.3</span> Singular Value Decomposition (SVD)</a>
<ul>
<li><a href="3.3-linalg-SVD.html#examples" id="toc-examples"><span class="toc-section-number">3.3.1</span> Examples</a></li>
</ul></li>
<li><a href="3.4-svdopt.html#svdopt" id="toc-svdopt"><span class="toc-section-number">3.4</span> SVD optimization results</a></li>
<li><a href="3.5-lowrank.html#lowrank" id="toc-lowrank"><span class="toc-section-number">3.5</span> Low-rank approximation</a>
<ul>
<li><a href="3.5-lowrank.html#matrix-norms" id="toc-matrix-norms"><span class="toc-section-number">3.5.1</span> Matrix norms</a></li>
<li><a href="3.5-lowrank.html#eckart-young-mirsky-theorem" id="toc-eckart-young-mirsky-theorem"><span class="toc-section-number">3.5.2</span> Eckart-Young-Mirsky Theorem</a></li>
<li><a href="3.5-lowrank.html#example-image-compression" id="toc-example-image-compression"><span class="toc-section-number">3.5.3</span> Example: image compression</a></li>
</ul></li>
<li><a href="3.6-tasks-ch3.html#tasks-ch3" id="toc-tasks-ch3"><span class="toc-section-number">3.6</span> Computer tasks</a></li>
<li><a href="3.7-exercises-ch3.html#exercises-ch3" id="toc-exercises-ch3"><span class="toc-section-number">3.7</span> Exercises</a></li>
</ul></li>
<li><a href="part-ii-dimension-reduction-methods.html#part-ii-dimension-reduction-methods" id="toc-part-ii-dimension-reduction-methods">PART II: Dimension reduction methods</a>
<ul>
<li><a href="part-ii-dimension-reduction-methods.html#a-warning" id="toc-a-warning">A warning</a></li>
</ul></li>
<li><a href="4-pca.html#pca" id="toc-pca"><span class="toc-section-number">4</span> Principal Component Analysis (PCA)</a>
<ul>
<li><a href="4.1-pca-an-informal-introduction.html#pca-an-informal-introduction" id="toc-pca-an-informal-introduction"><span class="toc-section-number">4.1</span> PCA: an informal introduction</a>
<ul>
<li><a href="4.1-pca-an-informal-introduction.html#notation-recap" id="toc-notation-recap"><span class="toc-section-number">4.1.1</span> Notation recap</a></li>
<li><a href="4.1-pca-an-informal-introduction.html#first-principal-component" id="toc-first-principal-component"><span class="toc-section-number">4.1.2</span> First principal component</a></li>
<li><a href="4.1-pca-an-informal-introduction.html#second-principal-component" id="toc-second-principal-component"><span class="toc-section-number">4.1.3</span> Second principal component</a></li>
<li><a href="4.1-pca-an-informal-introduction.html#geometric-interpretation-1" id="toc-geometric-interpretation-1"><span class="toc-section-number">4.1.4</span> Geometric interpretation</a></li>
<li><a href="4.1-pca-an-informal-introduction.html#example" id="toc-example"><span class="toc-section-number">4.1.5</span> Example</a></li>
<li><a href="4.1-pca-an-informal-introduction.html#example-iris" id="toc-example-iris"><span class="toc-section-number">4.1.6</span> Example: Iris</a></li>
</ul></li>
<li><a href="4.2-pca-a-formal-description-with-proofs.html#pca-a-formal-description-with-proofs" id="toc-pca-a-formal-description-with-proofs"><span class="toc-section-number">4.2</span> PCA: a formal description with proofs</a>
<ul>
<li><a href="4.2-pca-a-formal-description-with-proofs.html#properties-of-principal-components" id="toc-properties-of-principal-components"><span class="toc-section-number">4.2.1</span> Properties of principal components</a></li>
<li><a href="4.2-pca-a-formal-description-with-proofs.html#pca:football" id="toc-pca:football"><span class="toc-section-number">4.2.2</span> Example: Football</a></li>
<li><a href="4.2-pca-a-formal-description-with-proofs.html#pcawithR" id="toc-pcawithR"><span class="toc-section-number">4.2.3</span> PCA based on <span class="math inline">\(\mathbf R\)</span> versus PCA based on <span class="math inline">\(\mathbf S\)</span></a></li>
<li><a href="4.2-pca-a-formal-description-with-proofs.html#population-pca" id="toc-population-pca"><span class="toc-section-number">4.2.4</span> Population PCA</a></li>
<li><a href="4.2-pca-a-formal-description-with-proofs.html#pca-under-transformations-of-variables" id="toc-pca-under-transformations-of-variables"><span class="toc-section-number">4.2.5</span> PCA under transformations of variables</a></li>
</ul></li>
<li><a href="4.3-an-alternative-view-of-pca.html#an-alternative-view-of-pca" id="toc-an-alternative-view-of-pca"><span class="toc-section-number">4.3</span> An alternative view of PCA</a>
<ul>
<li><a href="4.3-an-alternative-view-of-pca.html#pca-mnist" id="toc-pca-mnist"><span class="toc-section-number">4.3.1</span> Example: MNIST handwritten digits</a></li>
</ul></li>
<li><a href="4.4-pca-comptask.html#pca-comptask" id="toc-pca-comptask"><span class="toc-section-number">4.4</span> Computer tasks</a></li>
<li><a href="4.5-exercises-1.html#exercises-1" id="toc-exercises-1"><span class="toc-section-number">4.5</span> Exercises</a></li>
</ul></li>
<li><a href="5-cca.html#cca" id="toc-cca"><span class="toc-section-number">5</span> Canonical Correlation Analysis (CCA)</a>
<ul>
<li><a href="5.1-cca1.html#cca1" id="toc-cca1"><span class="toc-section-number">5.1</span> The first pair of canonical variables</a>
<ul>
<li><a href="5.1-cca1.html#the-first-canonical-components" id="toc-the-first-canonical-components"><span class="toc-section-number">5.1.1</span> The first canonical components</a></li>
<li><a href="5.1-cca1.html#premcca" id="toc-premcca"><span class="toc-section-number">5.1.2</span> Example: Premier league football</a></li>
</ul></li>
<li><a href="5.2-the-full-set-of-canonical-correlations.html#the-full-set-of-canonical-correlations" id="toc-the-full-set-of-canonical-correlations"><span class="toc-section-number">5.2</span> The full set of canonical correlations</a>
<ul>
<li><a href="5.2-the-full-set-of-canonical-correlations.html#example-continued" id="toc-example-continued"><span class="toc-section-number">5.2.1</span> Example continued</a></li>
</ul></li>
<li><a href="5.3-properties.html#properties" id="toc-properties"><span class="toc-section-number">5.3</span> Properties</a>
<ul>
<li><a href="5.3-properties.html#connection-with-linear-regression-when-q1" id="toc-connection-with-linear-regression-when-q1"><span class="toc-section-number">5.3.1</span> Connection with linear regression when <span class="math inline">\(q=1\)</span></a></li>
<li><a href="5.3-properties.html#invarianceequivariance-properties-of-cca" id="toc-invarianceequivariance-properties-of-cca"><span class="toc-section-number">5.3.2</span> Invariance/equivariance properties of CCA</a></li>
</ul></li>
<li><a href="5.4-computer-tasks-1.html#computer-tasks-1" id="toc-computer-tasks-1"><span class="toc-section-number">5.4</span> Computer tasks</a></li>
<li><a href="5.5-exercises-2.html#exercises-2" id="toc-exercises-2"><span class="toc-section-number">5.5</span> Exercises</a></li>
</ul></li>
<li><a href="6-mds.html#mds" id="toc-mds"><span class="toc-section-number">6</span> Multidimensional Scaling (MDS)</a>
<ul>
<li><a href="6.1-classical-mds.html#classical-mds" id="toc-classical-mds"><span class="toc-section-number">6.1</span> Classical MDS</a>
<ul>
<li><a href="6.1-classical-mds.html#non-euclidean-distance-matrices" id="toc-non-euclidean-distance-matrices"><span class="toc-section-number">6.1.1</span> Non-Euclidean distance matrices</a></li>
<li><a href="6.1-classical-mds.html#principal-coordinate-analysis" id="toc-principal-coordinate-analysis"><span class="toc-section-number">6.1.2</span> Principal Coordinate Analysis</a></li>
</ul></li>
<li><a href="6.2-similarity.html#similarity" id="toc-similarity"><span class="toc-section-number">6.2</span> Similarity measures</a>
<ul>
<li><a href="6.2-similarity.html#binary-attributes" id="toc-binary-attributes"><span class="toc-section-number">6.2.1</span> Binary attributes</a></li>
<li><a href="6.2-similarity.html#example-classical-mds-with-the-mnist-data" id="toc-example-classical-mds-with-the-mnist-data"><span class="toc-section-number">6.2.2</span> Example: Classical MDS with the MNIST data</a></li>
</ul></li>
<li><a href="6.3-non-metric-mds.html#non-metric-mds" id="toc-non-metric-mds"><span class="toc-section-number">6.3</span> Non-metric MDS</a></li>
<li><a href="6.4-exercises-3.html#exercises-3" id="toc-exercises-3"><span class="toc-section-number">6.4</span> Exercises</a></li>
<li><a href="6.5-computer-tasks-2.html#computer-tasks-2" id="toc-computer-tasks-2"><span class="toc-section-number">6.5</span> Computer Tasks</a></li>
</ul></li>
<li><a href="part-iii-inference-using-the-multivariate-normal-distribution-mvn.html#part-iii-inference-using-the-multivariate-normal-distribution-mvn" id="toc-part-iii-inference-using-the-multivariate-normal-distribution-mvn">Part III: Inference using the Multivariate Normal Distribution (MVN)</a></li>
<li><a href="7-multinormal.html#multinormal" id="toc-multinormal"><span class="toc-section-number">7</span> The Multivariate Normal Distribution</a>
<ul>
<li><a href="7.1-definition-and-properties-of-the-mvn.html#definition-and-properties-of-the-mvn" id="toc-definition-and-properties-of-the-mvn"><span class="toc-section-number">7.1</span> Definition and Properties of the MVN</a>
<ul>
<li><a href="7.1-definition-and-properties-of-the-mvn.html#basics" id="toc-basics"><span class="toc-section-number">7.1.1</span> Basics</a></li>
<li><a href="7.1-definition-and-properties-of-the-mvn.html#transformations" id="toc-transformations"><span class="toc-section-number">7.1.2</span> Transformations</a></li>
<li><a href="7.1-definition-and-properties-of-the-mvn.html#independence" id="toc-independence"><span class="toc-section-number">7.1.3</span> Independence</a></li>
<li><a href="7.1-definition-and-properties-of-the-mvn.html#confidence-ellipses" id="toc-confidence-ellipses"><span class="toc-section-number">7.1.4</span> Confidence ellipses</a></li>
<li><a href="7.1-definition-and-properties-of-the-mvn.html#sampling-results-for-the-mvn" id="toc-sampling-results-for-the-mvn"><span class="toc-section-number">7.1.5</span> Sampling results for the MVN</a></li>
</ul></li>
<li><a href="7.2-the-wishart-distribution.html#the-wishart-distribution" id="toc-the-wishart-distribution"><span class="toc-section-number">7.2</span> The Wishart distribution</a>
<ul>
<li><a href="7.2-the-wishart-distribution.html#properties-1" id="toc-properties-1"><span class="toc-section-number">7.2.1</span> Properties</a></li>
<li><a href="7.2-the-wishart-distribution.html#cochrans-theorem" id="toc-cochrans-theorem"><span class="toc-section-number">7.2.2</span> Cochran’s theorem</a></li>
</ul></li>
<li><a href="7.3-hotellings-t2-distribution.html#hotellings-t2-distribution" id="toc-hotellings-t2-distribution"><span class="toc-section-number">7.3</span> Hotelling’s <span class="math inline">\(T^2\)</span> distribution</a></li>
<li><a href="7.4-inference-based-on-the-mvn.html#inference-based-on-the-mvn" id="toc-inference-based-on-the-mvn"><span class="toc-section-number">7.4</span> Inference based on the MVN</a>
<ul>
<li><a href="7.4-inference-based-on-the-mvn.html#onesampleSigma" id="toc-onesampleSigma"><span class="toc-section-number">7.4.1</span> <span class="math inline">\(\boldsymbol{\Sigma}\)</span> known</a></li>
<li><a href="7.4-inference-based-on-the-mvn.html#onesample" id="toc-onesample"><span class="toc-section-number">7.4.2</span> <span class="math inline">\(\boldsymbol{\Sigma}\)</span> unknown: 1 sample</a></li>
<li><a href="7.4-inference-based-on-the-mvn.html#boldsymbolsigma-unknown-2-samples" id="toc-boldsymbolsigma-unknown-2-samples"><span class="toc-section-number">7.4.3</span> <span class="math inline">\(\boldsymbol{\Sigma}\)</span> unknown: 2 samples</a></li>
</ul></li>
<li><a href="7.5-exercises-4.html#exercises-4" id="toc-exercises-4"><span class="toc-section-number">7.5</span> Exercises</a></li>
<li><a href="7.6-computer-tasks-3.html#computer-tasks-3" id="toc-computer-tasks-3"><span class="toc-section-number">7.6</span> Computer tasks</a></li>
</ul></li>
<li><a href="part-iv-classification-and-clustering.html#part-iv-classification-and-clustering" id="toc-part-iv-classification-and-clustering">Part IV: Classification and Clustering</a></li>
<li><a href="8-lda.html#lda" id="toc-lda"><span class="toc-section-number">8</span> Discriminant analysis</a>
<ul>
<li><a href="8-lda.html#linear-discriminant-analysis" id="toc-linear-discriminant-analysis">Linear discriminant analysis</a></li>
<li><a href="8.1-lda-ML.html#lda-ML" id="toc-lda-ML"><span class="toc-section-number">8.1</span> Maximum likelihood (ML) discriminant rule</a>
<ul>
<li><a href="8.1-lda-ML.html#multivariate-normal-populations" id="toc-multivariate-normal-populations"><span class="toc-section-number">8.1.1</span> Multivariate normal populations</a></li>
<li><a href="8.1-lda-ML.html#sample-lda" id="toc-sample-lda"><span class="toc-section-number">8.1.2</span> The sample ML discriminant rule</a></li>
<li><a href="8.1-lda-ML.html#two-populations" id="toc-two-populations"><span class="toc-section-number">8.1.3</span> Two populations</a></li>
<li><a href="8.1-lda-ML.html#more-than-two-populations" id="toc-more-than-two-populations"><span class="toc-section-number">8.1.4</span> More than two populations</a></li>
</ul></li>
<li><a href="8.2-lda-Bayes.html#lda-Bayes" id="toc-lda-Bayes"><span class="toc-section-number">8.2</span> Bayes discriminant rule</a>
<ul>
<li><a href="8.2-lda-Bayes.html#example-lda-using-the-iris-data" id="toc-example-lda-using-the-iris-data"><span class="toc-section-number">8.2.1</span> Example: LDA using the Iris data</a></li>
<li><a href="8.2-lda-Bayes.html#quadratic-discriminant-analysis-qda" id="toc-quadratic-discriminant-analysis-qda"><span class="toc-section-number">8.2.2</span> Quadratic Discriminant Analysis (QDA)</a></li>
<li><a href="8.2-lda-Bayes.html#prediction-accuracy" id="toc-prediction-accuracy"><span class="toc-section-number">8.2.3</span> Prediction accuracy</a></li>
</ul></li>
<li><a href="8.3-FLDA.html#FLDA" id="toc-FLDA"><span class="toc-section-number">8.3</span> Fisher’s linear discriminant rule</a>
<ul>
<li><a href="8.3-FLDA.html#iris-example-continued-1" id="toc-iris-example-continued-1"><span class="toc-section-number">8.3.1</span> Iris example continued</a></li>
<li><a href="8.3-FLDA.html#links-between-methods" id="toc-links-between-methods"><span class="toc-section-number">8.3.2</span> Links between methods</a></li>
</ul></li>
<li><a href="8.4-computer-tasks-4.html#computer-tasks-4" id="toc-computer-tasks-4"><span class="toc-section-number">8.4</span> Computer tasks</a></li>
<li><a href="8.5-exercises-5.html#exercises-5" id="toc-exercises-5"><span class="toc-section-number">8.5</span> Exercises</a></li>
</ul></li>
<li><a href="9-cluster.html#cluster" id="toc-cluster"><span class="toc-section-number">9</span> Cluster Analysis</a>
<ul>
<li><a href="9.1-k-means-clustering.html#k-means-clustering" id="toc-k-means-clustering"><span class="toc-section-number">9.1</span> K-means clustering</a>
<ul>
<li><a href="9.1-k-means-clustering.html#estimating-boldsymbol-delta" id="toc-estimating-boldsymbol-delta"><span class="toc-section-number">9.1.1</span> Estimating <span class="math inline">\(\boldsymbol \delta\)</span></a></li>
<li><a href="9.1-k-means-clustering.html#k-means" id="toc-k-means"><span class="toc-section-number">9.1.2</span> K-means</a></li>
<li><a href="9.1-k-means-clustering.html#example-iris-data" id="toc-example-iris-data"><span class="toc-section-number">9.1.3</span> Example: Iris data</a></li>
<li><a href="9.1-k-means-clustering.html#choosing-k" id="toc-choosing-k"><span class="toc-section-number">9.1.4</span> Choosing <span class="math inline">\(K\)</span></a></li>
</ul></li>
<li><a href="9.2-model-based-clustering.html#model-based-clustering" id="toc-model-based-clustering"><span class="toc-section-number">9.2</span> Model-based clustering</a>
<ul>
<li><a href="9.2-model-based-clustering.html#maximum-likelihood-estimation" id="toc-maximum-likelihood-estimation"><span class="toc-section-number">9.2.1</span> Maximum-likelihood estimation</a></li>
<li><a href="9.2-model-based-clustering.html#multivariate-gaussian-clusters" id="toc-multivariate-gaussian-clusters"><span class="toc-section-number">9.2.2</span> Multivariate Gaussian clusters</a></li>
<li><a href="9.2-model-based-clustering.html#example-iris-1" id="toc-example-iris-1"><span class="toc-section-number">9.2.3</span> Example: Iris</a></li>
</ul></li>
<li><a href="9.3-hierarchical-clustering-methods.html#hierarchical-clustering-methods" id="toc-hierarchical-clustering-methods"><span class="toc-section-number">9.3</span> Hierarchical clustering methods</a>
<ul>
<li><a href="9.3-hierarchical-clustering-methods.html#distance-measures" id="toc-distance-measures"><span class="toc-section-number">9.3.1</span> Distance measures</a></li>
<li><a href="9.3-hierarchical-clustering-methods.html#toy-example" id="toc-toy-example"><span class="toc-section-number">9.3.2</span> Toy Example</a></li>
<li><a href="9.3-hierarchical-clustering-methods.html#comparison-of-methods" id="toc-comparison-of-methods"><span class="toc-section-number">9.3.3</span> Comparison of methods</a></li>
</ul></li>
<li><a href="9.4-summary.html#summary" id="toc-summary"><span class="toc-section-number">9.4</span> Summary</a></li>
<li><a href="9.5-computer-tasks-5.html#computer-tasks-5" id="toc-computer-tasks-5"><span class="toc-section-number">9.5</span> Computer tasks</a></li>
<li><a href="9.6-exercises-6.html#exercises-6" id="toc-exercises-6"><span class="toc-section-number">9.6</span> Exercises</a></li>
</ul></li>
<li><a href="10-lm.html#lm" id="toc-lm"><span class="toc-section-number">10</span> Linear Models</a>
<ul>
<li><a href="10-lm.html#notation-3" id="toc-notation-3">Notation</a></li>
<li><a href="10.1-ordinary-least-squares-ols.html#ordinary-least-squares-ols" id="toc-ordinary-least-squares-ols"><span class="toc-section-number">10.1</span> Ordinary least squares (OLS)</a>
<ul>
<li><a href="10.1-ordinary-least-squares-ols.html#geometry" id="toc-geometry"><span class="toc-section-number">10.1.1</span> Geometry</a></li>
<li><a href="10.1-ordinary-least-squares-ols.html#normal-linear-model" id="toc-normal-linear-model"><span class="toc-section-number">10.1.2</span> Normal linear model</a></li>
<li><a href="10.1-ordinary-least-squares-ols.html#linear-models-in-r" id="toc-linear-models-in-r"><span class="toc-section-number">10.1.3</span> Linear models in R</a></li>
<li><a href="10.1-ordinary-least-squares-ols.html#problems-with-ols" id="toc-problems-with-ols"><span class="toc-section-number">10.1.4</span> Problems with OLS</a></li>
</ul></li>
<li><a href="10.2-principal-component-regression-pcr.html#principal-component-regression-pcr" id="toc-principal-component-regression-pcr"><span class="toc-section-number">10.2</span> Principal component regression (PCR)</a>
<ul>
<li><a href="10.2-principal-component-regression-pcr.html#pcr-in-r" id="toc-pcr-in-r"><span class="toc-section-number">10.2.1</span> PCR in R</a></li>
</ul></li>
<li><a href="10.3-shrinkage-methods.html#shrinkage-methods" id="toc-shrinkage-methods"><span class="toc-section-number">10.3</span> Shrinkage methods</a>
<ul>
<li><a href="10.3-shrinkage-methods.html#ridge-regression-in-r" id="toc-ridge-regression-in-r"><span class="toc-section-number">10.3.1</span> Ridge regression in R</a></li>
</ul></li>
<li><a href="10.4-multi-output-linear-model.html#multi-output-linear-model" id="toc-multi-output-linear-model"><span class="toc-section-number">10.4</span> Multi-output Linear Model</a>
<ul>
<li><a href="10.4-multi-output-linear-model.html#normal-linear-model-1" id="toc-normal-linear-model-1"><span class="toc-section-number">10.4.1</span> Normal linear model</a></li>
<li><a href="10.4-multi-output-linear-model.html#reduced-rank-regression" id="toc-reduced-rank-regression"><span class="toc-section-number">10.4.2</span> Reduced rank regression</a></li>
</ul></li>
<li><a href="10.5-computer-tasks-6.html#computer-tasks-6" id="toc-computer-tasks-6"><span class="toc-section-number">10.5</span> Computer tasks</a></li>
<li><a href="10.6-exercises-7.html#exercises-7" id="toc-exercises-7"><span class="toc-section-number">10.6</span> Exercises</a></li>
</ul></li>
<li class="divider"></li>
<li> <a href="https://rich-d-wilkinson.github.io/teaching.html" target="blank">University of Nottingham</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Multivariate Statistics</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="definition-and-properties-of-the-mvn" class="section level2" number="7.1">
<h2><span class="header-section-number">7.1</span> Definition and Properties of the MVN</h2>
<div id="basics" class="section level3" number="7.1.1">
<h3><span class="header-section-number">7.1.1</span> Basics</h3>

<div class="definition">
<span id="def:mvn" class="definition"><strong>Definition 7.1  </strong></span>A random vector <span class="math inline">\(\mathbf x=(x_1, \ldots , x_p)^\top\)</span> has a <span class="math inline">\(p\)</span>-dimensional MVN distribution if and only if <span class="math inline">\(\mathbf a^\top \mathbf x\)</span> is a univariate normal random variable for all constant vectors <span class="math inline">\(\mathbf a\in \mathbb{R}^p\)</span>.
</div>
<p>In particular, note that the marginal distribution of each element of <span class="math inline">\(\mathbf x\)</span> has a uni-variate Gaussian distribution.</p>
<p><strong>Notation</strong>: If <span class="math inline">\(\mathbf x\in \mathbb{R}^p\)</span> is MVN with mean <span class="math inline">\({\boldsymbol{\mu}}\in \mathbb{R}^p\)</span> and covariance matrix <span class="math inline">\(\boldsymbol{\Sigma}\in \mathbb{R}^{p\times p}\)</span> then we write
<span class="math display">\[ \mathbf x\sim N_p ({\boldsymbol{\mu}}, \boldsymbol{\Sigma}).\]</span></p>
<p><br></br></p>

<div class="definition">
<span id="def:mvnpdf" class="definition"><strong>Definition 7.2  </strong></span>If the population covariance matrix <span class="math inline">\(\boldsymbol{\Sigma}\)</span> (<span class="math inline">\(p \times p\)</span>) is positive definite (i.e. full rank), so that <span class="math inline">\(\boldsymbol{\Sigma}^{-1}\)</span> exists,
then the <strong>probability density function</strong> (pdf) of the MVN distribution is given by
<span class="math display">\[ f(\mathbf x) = \frac{1}{| 2 \pi \boldsymbol{\Sigma}|^{1/2}} \exp \left(-\frac{1}{2}(\mathbf x- {\boldsymbol{\mu}})^\top \boldsymbol{\Sigma}^{-1} (\mathbf x- {\boldsymbol{\mu}}) \right).\]</span>
Here, I’ve used the notation <span class="math inline">\(|\mathbf A| = \det(\mathbf A)\)</span>.
</div>
<p>If <span class="math inline">\(p=1\)</span>, so that <span class="math inline">\(\mathbf x= x\)</span>, <span class="math inline">\({\boldsymbol{\mu}}= \mu\)</span> and <span class="math inline">\(\boldsymbol{\Sigma}= \sigma^2\)</span>, say, then the pdf simplifies to
<span class="math display">\[\begin{eqnarray*}
f(\mathbf x) &amp;=&amp; \frac{1}{|2 \pi \sigma^2|^{1/2}} \exp \left(-\frac{1}{2}(x - \mu) (\sigma^2)^{-1} (x - \mu) \right)\\
&amp;=&amp; \frac{1}{(2 \pi \sigma^2)^{1/2}} \exp \left(-\frac{1}{2 \sigma^2}(x - \mu)^2 \right)
\end{eqnarray*}\]</span>
which is the familiar pdf of the univariate normal distribution <span class="math inline">\(N(\mu,\sigma^2)\)</span>.</p>
<p>If <span class="math inline">\(p&gt;1\)</span> and <span class="math inline">\(\boldsymbol{\Sigma}= \operatorname{diag}(\sigma_1^2, \ldots, \sigma_p^2)\)</span> then
<span class="math display">\[\begin{eqnarray*}
f(\mathbf x) &amp;=&amp; \frac{1}{((2 \pi)^{p}\prod_{i=1}^p \sigma_i^2)^{1/2}} \exp \left(-\frac{1}{2}(\mathbf x- {\boldsymbol{\mu}})^\top \boldsymbol{\Sigma}^{-1}(\mathbf x- {\boldsymbol{\mu}}) \right)\\
&amp;=&amp; \frac{1}{(2 \pi)^{p/2}\prod_{i=1}^p \sigma_i} \exp \left(-\frac{1}{2} \sum_{i=1}^p \frac{(x_i - \mu_i)^2}{\sigma_i^2} \right)\\
&amp;=&amp; \left(\frac{1}{\sqrt{2 \pi\sigma_1^2}} \exp \left(-\frac{1}{2\sigma_1^2} (x_1 - \mu_1)^2 \right)\right)\\
&amp;&amp; \qquad \qquad \times \ldots \left(\frac{1}{\sqrt{2 \pi \sigma_p^2}} \exp \left(-\frac{1}{2\sigma_p^2} (x_p - \mu_p)^2 \right)\right)
\end{eqnarray*}\]</span>
Thus, by the factorisation theorem for probability densities, the components of <span class="math inline">\(\mathbf x\)</span> have independent univariate normal distributions: <span class="math inline">\(x_i \sim N(\mu_i, \sigma_i^2)\)</span>.</p>
<p>If <span class="math inline">\(p=2\)</span> we can plot <span class="math inline">\(f(\mathbf x)\)</span> using contour plots. Below, I’ve generated 1000 points from four different normal distributions using mean vectors
<span class="math display">\[{\boldsymbol{\mu}}_1={\boldsymbol{\mu}}_3={\boldsymbol{\mu}}_4=\begin{pmatrix}0 \\0 \\\end{pmatrix}, \quad {\boldsymbol{\mu}}_2=\begin{pmatrix}1 \\-1 \\\end{pmatrix}\]</span>
and covariance matrices
<span class="math display">\[\boldsymbol{\Sigma}_1=\boldsymbol{\Sigma}_2=\begin{pmatrix}1&amp;0 \\0&amp;1 \\\end{pmatrix}, \quad \boldsymbol{\Sigma}_3=\begin{pmatrix}1&amp;0 \\0&amp;0.05 \\\end{pmatrix}, \quad \boldsymbol{\Sigma}_4=\begin{pmatrix}1&amp;0.9 \\0.9&amp;1 \\\end{pmatrix}.\]</span></p>
<p><img src="07-mvn_files/figure-html/unnamed-chunk-3-1.png" width="960" /></p>
<p><strong>Note</strong> that the top left and bottom right plots have the <strong>same marginal distribtions</strong> for components <span class="math inline">\(x_1\)</span> and <span class="math inline">\(x_2\)</span>, namely
<span class="math display">\[x_1 \sim N(0, 1)\quad x_2 \sim N(0, 1)\]</span></p>
<p><img src="07-mvn_files/figure-html/unnamed-chunk-4-1.png" width="672" /><img src="07-mvn_files/figure-html/unnamed-chunk-4-2.png" width="672" /></p>
<p>The contours on each plot are obtained by finding values of <span class="math inline">\(\mathbf x\)</span> for which <span class="math inline">\(f(\mathbf x)=c\)</span>. The constant <span class="math inline">\(c\)</span> is chosen so that the the shapes (which we’ll see below are ellipses)
enclose 66% and 95% of the data.</p>
</div>
<div id="transformations" class="section level3" number="7.1.2">
<h3><span class="header-section-number">7.1.2</span> Transformations</h3>

<div class="proposition">
<span id="prp:six2" class="proposition"><strong>Proposition 7.1  </strong></span> If <span class="math inline">\(\mathbf x\sim N_p({\boldsymbol{\mu}},\boldsymbol{\Sigma})\)</span> then if <span class="math display">\[\mathbf y= \mathbf A\mathbf x+ \mathbf c, \mbox{ where } \mathbf A\in \mathbb{R}^{q \times p} \mbox{ and }\mathbf c\in \mathbb{R}^q
  \mbox{ are constant},\]</span> then
<span class="math display">\[\mathbf y\sim N_q(\mathbf A{\boldsymbol{\mu}}+ \mathbf c, \mathbf A\boldsymbol{\Sigma}\mathbf A^\top).\]</span>
</div>
<div class="proof">
<p><span id="unlabeled-div-24" class="proof"><em>Proof</em>. </span>Let <span class="math inline">\(\mathbf b\in \mathbb{R}^q\)</span> be a constant vector. Then
<span class="math display">\[ \mathbf b^\top \mathbf y= \mathbf b^\top \mathbf A\mathbf x+ \mathbf b^\top \mathbf c= \mathbf a^\top \mathbf x+ \mathbf b^\top \mathbf c\]</span>
where <span class="math inline">\(\mathbf a^\top = \mathbf b^\top \mathbf A\)</span>. Now <span class="math inline">\(\mathbf a^\top \mathbf x\)</span> is univariate normal for all <span class="math inline">\(\mathbf a\)</span> since <span class="math inline">\(\mathbf x\)</span> is MVN. Therefore <span class="math inline">\(\mathbf b^\top \mathbf y\)</span> is univariate normal for all <span class="math inline">\(\mathbf b\)</span>, so <span class="math inline">\(\mathbf y\)</span> is MVN.</p>
<p>We can compute <span class="math inline">\({\mathbb{E}}(\mathbf y)=\mathbf A{\boldsymbol{\mu}}+\mathbf c\)</span> and <span class="math inline">\({\mathbb{V}\operatorname{ar}}(\mathbf y)=\mathbf A\boldsymbol{\Sigma}\mathbf A^\top\)</span> using the properties listed in Section <a href="1.3-randvec.html#randvec">1.3</a>.</p>
</div>
<p><br></br></p>
<p>This implies that a linear transformation of a MVN random variable is also MVN. We can use this result to prove two important corollaries. The first corollary is useful for simulating data from a general MVN distribution.</p>

<div class="corollary">
<span id="cor:csix2" class="corollary"><strong>Corollary 7.1  </strong></span> If <span class="math inline">\(\mathbf x\sim N_p(\boldsymbol 0,\mathbf I_p)\)</span> and <span class="math inline">\(\mathbf y= \boldsymbol{\Sigma}^{1/2} \mathbf x+ {\boldsymbol{\mu}}\)</span> then <span class="math display">\[\mathbf y\sim N_p({\boldsymbol{\mu}},\boldsymbol{\Sigma}).\]</span>
</div>
<div class="proof">
<p><span id="unlabeled-div-25" class="proof"><em>Proof</em>. </span>Apply <a href="7.1-definition-and-properties-of-the-mvn.html#prp:six2">7.1</a> with <span class="math inline">\(\mathbf A= \boldsymbol{\Sigma}^{1/2}\)</span> and <span class="math inline">\(\mathbf c= {\boldsymbol{\mu}}\)</span>. Therefore
<span class="math display">\[{\mathbb{E}}(\mathbf y) = \boldsymbol{\Sigma}^{1/2} \boldsymbol 0_p + {\boldsymbol{\mu}}= {\boldsymbol{\mu}}\quad \mbox{and}\quad {\mathbb{V}\operatorname{ar}}(\mathbf y) = \boldsymbol{\Sigma}^{1/2} \mathbf I_p \boldsymbol{\Sigma}^{1/2} = \boldsymbol{\Sigma}.\]</span></p>
</div>
<p>The second corollary says that any MVN random variable can be transformed into standard form.</p>

<div class="corollary">
<span id="cor:csix3" class="corollary"><strong>Corollary 7.2  </strong></span>Suppose <span class="math inline">\(\mathbf x\sim N_p({\boldsymbol{\mu}},\boldsymbol{\Sigma})\)</span>, where <span class="math inline">\(\boldsymbol{\Sigma}\)</span> has full rank. Then<br />
<span class="math display">\[\mathbf y= \boldsymbol{\Sigma}^{-1/2}(\mathbf x- {\boldsymbol{\mu}}) \sim N_p(\boldsymbol 0,\mathbf I_p).\]</span>
</div>
<div class="proof">
<p><span id="unlabeled-div-26" class="proof"><em>Proof</em>. </span>Apply Proposition <a href="7.1-definition-and-properties-of-the-mvn.html#prp:six2">7.1</a> with <span class="math inline">\(\mathbf A= \boldsymbol{\Sigma}^{-1/2}\)</span> and <span class="math inline">\(\mathbf c= - \boldsymbol{\Sigma}^{-1/2} {\boldsymbol{\mu}}\)</span>.</p>
</div>
</div>
<div id="independence" class="section level3" number="7.1.3">
<h3><span class="header-section-number">7.1.3</span> Independence</h3>

<div class="proposition">
<span id="prp:six4" class="proposition"><strong>Proposition 7.2  </strong></span>Two vectors <span class="math inline">\(\mathbf x\in\mathbb{R}^p\)</span> and <span class="math inline">\(\mathbf y\in\mathbb{R}^q\)</span> which are jointly multivariate normal are independent if and only if they are uncorrelated, i.e. <span class="math inline">\({\mathbb{C}\operatorname{ov}}(\mathbf x,\mathbf y) = \boldsymbol 0_{p,q}\)</span>.
</div>
<div class="proof">
<p><span id="unlabeled-div-27" class="proof"><em>Proof</em>. </span>The joint distribution of <span class="math inline">\(\mathbf x\)</span> and <span class="math inline">\(\mathbf y\)</span> can always be factorized as
<span class="math display">\[p(\mathbf x, \mathbf y)= p(\mathbf x)p(\mathbf y|\mathbf x).\]</span>
If the conditional distribution for <span class="math inline">\(\mathbf y\)</span> given <span class="math inline">\(\mathbf x\)</span> does not depend upon <span class="math inline">\(\mathbf x\)</span>, i.e., if <span class="math inline">\(p(\mathbf y| \mathbf x)=p(\mathbf y)\)</span>, then <span class="math inline">\(\mathbf y\)</span> and <span class="math inline">\(\mathbf x\)</span> are independent.</p>
<p>Suppose <span class="math inline">\(\mathbf x\sim N_p({\boldsymbol{\mu}}_\mathbf x, \boldsymbol{\Sigma}_{\mathbf x,\mathbf x})\)</span> and <span class="math inline">\(\mathbf y\sim N_p({\boldsymbol{\mu}}_\mathbf y, \boldsymbol{\Sigma}_{\mathbf y,\mathbf y})\)</span> are jointly normally distributed and that they are uncorrelated. Thus we can write
<span class="math display">\[\begin{pmatrix}\mathbf x\\ \mathbf y\end{pmatrix}\sim N_{p+q}\left({\boldsymbol{\mu}}, \boldsymbol{\Sigma}\right)
\]</span>
where
<span class="math display">\[{\boldsymbol{\mu}}= \begin{pmatrix}{\boldsymbol{\mu}}_\mathbf x\\ {\boldsymbol{\mu}}_{\mathbf y}\end{pmatrix}\quad \boldsymbol{\Sigma}= \begin{pmatrix} \boldsymbol{\Sigma}_{\mathbf x, \mathbf x} &amp;\boldsymbol 0_{p,q}\\
\boldsymbol 0_{q,p} &amp; \boldsymbol{\Sigma}_{\mathbf z, \mathbf z}\end{pmatrix}.\]</span></p>
<p>Now
<span class="math display">\[\begin{align*}
p(\mathbf y|\mathbf x)&amp;=\frac{p(\mathbf x, \mathbf y)}{p(\mathbf x)}\\
  &amp;\propto \frac{\exp\left(-\frac{1}{2}(\mathbf x-{\boldsymbol{\mu}}_\mathbf x)^\top \boldsymbol{\Sigma}_{\mathbf x,\mathbf x}(\mathbf x-{\boldsymbol{\mu}}_\mathbf x) -\frac{1}{2}(\mathbf y-{\boldsymbol{\mu}}_\mathbf y)^\top \boldsymbol{\Sigma}_{\mathbf y,\mathbf y}(\mathbf y-{\boldsymbol{\mu}}_\mathbf y)\right)}{\exp\left(-\frac{1}{2}(\mathbf x-{\boldsymbol{\mu}}_\mathbf x)^\top \boldsymbol{\Sigma}_{\mathbf x,\mathbf x}(\mathbf x-{\boldsymbol{\mu}}_\mathbf x)\right)} \\
&amp;\propto \exp\left(-\frac{1}{2}(\mathbf y-{\boldsymbol{\mu}}_\mathbf y)^\top \boldsymbol{\Sigma}_{\mathbf y,\mathbf y}(\mathbf y-{\boldsymbol{\mu}}_\mathbf y)\right) \\
&amp;\propto p(\mathbf y)
\end{align*}\]</span>
So <span class="math inline">\(p(\mathbf y|\mathbf x)=p(\mathbf y)\)</span>, i.e. <span class="math inline">\(p(\mathbf y|\mathbf x)\)</span> is not a function of <span class="math inline">\(\mathbf x\)</span>, and thus <span class="math inline">\(\mathbf x\)</span> and <span class="math inline">\(\mathbf y\)</span> are independent.</p>
</div>
<p><br></br></p>
<p>Proposition <a href="7.1-definition-and-properties-of-the-mvn.html#prp:six4">7.2</a> means that zero correlation implies independence for the MVN distribution. This is not true in general for other distributions.</p>
<p><strong>Note:</strong> It is important that <span class="math inline">\(\mathbf x\)</span> and <span class="math inline">\(\mathbf y\)</span> are <strong>jointly</strong> multivariate normal. For example, suppose <span class="math inline">\(x \sim N(0, 1)\)</span>. Let
<span class="math display">\[z=\begin{cases}
1 \mbox{ with probability } \frac{1}{2}\\
-1 \mbox{ otherwise}
\end{cases}
\]</span>
and let <span class="math inline">\(y=zx\)</span>. Then clearly <span class="math inline">\(y\)</span> is also a normal random variable: <span class="math inline">\(y \sim N(0,1)\)</span>. In addition, note that
<span class="math display">\[{\mathbb{C}\operatorname{ov}}(x,y)= {\mathbb{E}}(xy)= {\mathbb{E}}(x^2){\mathbb{E}}(z)=0\]</span>
so that <span class="math inline">\(x\)</span> and <span class="math inline">\(y\)</span> are uncorrelated.</p>
<p>However, <span class="math inline">\(x\)</span> and <span class="math inline">\(y\)</span> are clearly not independent: if you tell me <span class="math inline">\(x\)</span>, then I know <span class="math inline">\(y=x\)</span> or <span class="math inline">\(y=-x\)</span>.</p>
<!--
### Moment Generating Functions

\BeginKnitrBlock{definition}<div class="definition"><span class="definition" id="def:mgf"><strong>(\#def:mgf) </strong></span>The **moment generating function** of a random vector $\bx\in \mathbb{R}^p$  is given by
$$
M(\bt)=\BE[e^{\bt^\top \bx}],
$$
and is defined for all $\bt \in \mathbb{R}^p$ for which $M(\bt)$ is finite.
</div>\EndKnitrBlock{definition}
\BeginKnitrBlock{proposition}<div class="proposition"><span class="proposition" id="prp:six3"><strong>(\#prp:six3) </strong></span> The moment generating function of $\bx \sim N_p(\bmu , \bSigma)$ is given by
\begin{equation}
M(\bt)=\exp \left (\bmu^\top  \bt + \frac{1}{2} \bt^\top \bSigma \bt \right).
(\#eq:Mt)
\end{equation}</div>\EndKnitrBlock{proposition}

\BeginKnitrBlock{proof}<div class="proof">\iffalse{} <span class="proof"><em>Proof. </em></span>  \fi{}For fixed $\bt$, define the random variable $Y=\bx^\top \bt$.  From Proposition \@ref(prp:six2), 
$Y \sim N(\mu_{\bt}, {\sigma_\bt}^2)$, 
where $\mu_\bt=\bmu^\top \bt$ and $\sigma^2_\bt =\bt^\top \bSigma \bt$.  

If $\sigma_\bt=0$ then $Y=\bmu^\top \bt$ with probability one, and  $M(\bt)=e^{\bmu^\top \bt}$ which agrees with \@ref(eq:Mt).  

Assume $\sigma_{\bt}>0$.   Then
\begin{align*}
M(\bt)&=\BE[e^{\bx^\top \bt}]\\
&=\BE[e^{Y}]=\int_{-\infty}^\infty \exp(y) \frac{1}{\sqrt{2\pi \sigma_\bt^2}}
\exp\left (-\frac{1}{2}\frac{(y-\mu_\bt)^2}{\sigma_\bt^2} \right )dy.
\end{align*}

The integral above can be evaluated by completing the square in the exponent, using the identity
$$
y-\frac{1}{2}\frac{(y-\mu_\bt)^2}{\sigma_\bt^2}=\mu_\bt
+\frac{1}{2}\sigma_\bt^2-\frac{1}{2}\frac{(y-\mu_\bt-\sigma_\bt^2)^2}{\sigma_\bt^2}.
$$

Consequently
\begin{align*}
M(\bt)&=\int_{-\infty}^\infty \exp \left\{\mu_\bt +\frac{1}{2}\sigma_\bt^2 \right\}
\frac{1}{\sqrt{2 \pi \sigma_\bt^2}}\exp \left\{ -\frac{1}{2} \frac{(y-\mu_\bt-\sigma_\bt^2)^2}
{\sigma_\bt^2}\right\}dy\\
&=\exp\left( \mu_\bt + \frac{1}{2}\sigma_\bt^2  \right)\\
&=\exp\left(
\bmu^\top \bt + \frac{1}{2}\bt^\top \bSigma \bt\right),
\end{align*}
as required.</div>\EndKnitrBlock{proof}

\BeginKnitrBlock{proposition}<div class="proposition"><span class="proposition" id="prp:six4old"><strong>(\#prp:six4old) </strong></span>Two vectors $\bx\in\mathbb{R}^p$  and $\by\in\mathbb{R}^q$  which are jointly multivariate normal are independent if and only if they are uncorrelated, i.e. $\cov(\bx,\by) = \bzero_{p,q}$.</div>\EndKnitrBlock{proposition}

\BeginKnitrBlock{proof}<div class="proof">\iffalse{} <span class="proof"><em>Proof. </em></span>  \fi{}We prove this result using the factorisation theorem for moment generating functions (MGFs), which is now stated.
Let 
$$\bt=\begin{pmatrix}\bt_1\\ \bt_2\end{pmatrix}$$ 
  where $\bt_1 \in \mathbb{R}^p$, $\bt_2 \in \mathbb{R}^q$ and $\bt \in \mathbb{R}^{p+q}$.  The joint MGF of two arbitrary random vectors $\stackrel{p \times 1}{\bx}$ and $\stackrel{q \times 1}{\by}$ is   $$
 M(\bt_1, \bt_2)=\BE[e^{\bt_1^\top \bx + \bt_2^\top \by}].
  $$
The factorisation theorem for MGFs states that
$\bx$ and $\by$ are independent if and only if $M(\bt_1 , \bt_2)$ factorises, i.e.,
$$
M(\bt_1 , \bt_2)=M_1(\bt_1)M_2(\bt_2)
$$
for some functions $M_1$ and $M_2$, in which case $M_1$ and $M_2$ are the marginal MGFs of $\bx$ and $\by$.  

Now suppose $\bx$ and $\by$ are multivariate normal random variables with 
\begin{equation}
\BE[\bx]=\bmu_{\bx}, \qquad \qquad \BE[\by]=\bmu_{\by}, \quad  \var(\bx)=\bSigma_{\bx \bx},
\quad  \var(\by)=\bSigma_{\by \by},
(\#eq:def1)
\end{equation}
 and
\begin{equation}
\cov(\bx,\by)=\bSigma_{\bx \by}=\bSigma_{\by \bx}^\top = \cov(\by, \bx)^\top.
(\#eq:def2)
\end{equation}
Using Proposition \@ref(prp:six3) and definitions \@ref(eq:def1) and \@ref(eq:def2),
\begin{align*}
M(\bt_1, \bt_2)&=\exp\left ( \bmu^\top \bt + \frac{1}{2}\bt^\top \bSigma \bt \right )\\
&=\exp\bigg (\bmu_{\bx}^\top \bt_1 +\bmu_{\by}^\top \bt_2+\frac{1}{2}\bt_1^\top \bSigma_{\bx \bx}{\bt_1}\\
& \qquad \qquad +\frac{1}{2}\bt_2^\top  \bSigma_{\by \by}\bt_2+\frac{1}{2} 2\bt_1^\top \bSigma_{\bx \by}\bt_2 \bigg)\\
&=M_1(\bt_1)M_2(\bt_2)M_3(\bt_1, \bt_2),
\end{align*}
where $M_1(\bt_1)$ and $M_2(\bt_2)$ are the marginal MGFs of $\bx$ and $\by$ respectively, and
$$
M_3(\bt_1, \bt_2)=\exp\left (\bt_1^\top \bSigma_{\bx \by}\bt_2 \right ).
$$
Thus, by the factorisation theorem, $\bx$ and $\by$ are independent if and only if $M_3(\bt_1, \bt_2)$ is constant with respect to
$\bt_1$ and $\bt_2$, which is the case if and only if $\bSigma_{\bx \by}={\mathbf 0}_{p,q}$. </div>\EndKnitrBlock{proof}
-->
<!--**Note**: Propositions \@ref(prp:six1)  -   \@ref(prp:six4)  holds regardless irregardless of whether the covariance matrix $\bSigma$ is invertible.
-->
</div>
<div id="confidence-ellipses" class="section level3" number="7.1.4">
<h3><span class="header-section-number">7.1.4</span> Confidence ellipses</h3>
<p>The contours in the plots of the bivariate normal samples shown above are ellipses.
They were defined to lines of consant density, i.e., by <span class="math inline">\(f(\mathbf x)=c\)</span>, which implies</p>
<p><span class="math display" id="eq:mvnellipse">\[\begin{equation}
(\mathbf x- {\boldsymbol{\mu}})^\top \boldsymbol{\Sigma}^{-1} (\mathbf x- {\boldsymbol{\mu}})=c&#39; \tag{7.1}
\end{equation}\]</span>
for some constant <span class="math inline">\(c&#39;\)</span>.
To see that this is the equation of an ellipse, note that a standard ellipse in <span class="math inline">\(\mathbb{R}^2\)</span> is given by the equation
<span class="math display" id="eq:ellipse">\[\begin{equation}
\frac{x^2}{a^2}+\frac{y^2}{b^2}=1 \quad (a, b&gt;0) \tag{7.2}
\end{equation}\]</span>
and recall that a standard ellipse has axes of symmetry given by the <span class="math inline">\(x\)</span>-axis and <span class="math inline">\(y\)</span>-axis
(if <span class="math inline">\(a&gt;b\)</span>, the <span class="math inline">\(x\)</span>-axis is the major axis, and the <span class="math inline">\(y\)</span>-axis the minor axis). For example, <span class="math inline">\(a=10, b=3\)</span> gives the ellipse:</p>
<p><img src="07-mvn_files/figure-html/unnamed-chunk-7-1.png" width="432" /></p>
<p>If we define
<span class="math inline">\({\mathbf A}=\left( \begin{array}{cc} a^2&amp;0\\ 0&amp;b^2 \end{array} \right)\)</span> and write <span class="math inline">\({\mathbf x}=\binom{x}{y}\)</span>,
then Equation <a href="7.1-definition-and-properties-of-the-mvn.html#eq:ellipse">(7.2)</a> can be written in the form
<span class="math display">\[ \mathbf x^\top {\mathbf A}^{-1}\mathbf x=c&#39;. \]</span>
To shift the centre of the ellipse from the origin to the point <span class="math inline">\({\boldsymbol{\mu}}\)</span> we modify the equation to be
<span class="math display">\[ (\mathbf x-{\boldsymbol{\mu}})^\top {\mathbf A}^{-1}(\mathbf x-{\boldsymbol{\mu}}) =c&#39;.\]</span></p>
<p>What if instead of using a diagonal matrix <span class="math inline">\(\mathbf A\)</span>, we use a non-diagonal matrix <span class="math inline">\(\boldsymbol{\Sigma}\)</span> as in Equation <a href="7.1-definition-and-properties-of-the-mvn.html#eq:mvnellipse">(7.1)</a>? If <span class="math inline">\(\boldsymbol{\Sigma}\)</span> has spectral decomposition
<span class="math inline">\(\boldsymbol{\Sigma}= \mathbf V\boldsymbol \Lambda\mathbf V^\top\)</span>, then
<span class="math display">\[\begin{align*}
(\mathbf x-{\boldsymbol{\mu}})^\top {\boldsymbol{\Sigma}}^{-1}(\mathbf x-{\boldsymbol{\mu}}) &amp;= (\mathbf x-{\boldsymbol{\mu}})^\top \mathbf V\boldsymbol \Lambda^{-1}\mathbf V^\top(\mathbf x-{\boldsymbol{\mu}}) \\
&amp;= \mathbf y^\top \boldsymbol \Lambda^{-1}\mathbf y\\
&amp;= \frac{y_1^2}{\lambda_1}+\frac{y_2^2}{\lambda_2}
\end{align*}\]</span>
where <span class="math inline">\(\boldsymbol \Lambda=\operatorname{diag}(\lambda_1, \lambda_2)\)</span> is a diagonal matrix of eigenvalues, and <span class="math inline">\(\mathbf y= \mathbf V^\top (\mathbf x-\mathbf u)\)</span>. Because <span class="math inline">\(\mathbf V\)</span> is an orthogonal matrix (a rotation), we can see that this is the equation of a standard ellipse when using the eigenvectors as the coordinate system. Or in other words, it is an ellipse
with major axis given by the first eigenvector, and minor axis given by the second eigenvector, centered around <span class="math inline">\({\boldsymbol{\mu}}\)</span>.</p>
<p>Analogous results for ellipsoids and quadratic forms hold in three and higher dimensions.</p>
<p>The term <span class="math inline">\((\mathbf x-{\boldsymbol{\mu}})^\top \boldsymbol{\Sigma}^{-1} (\mathbf x-{\boldsymbol{\mu}})\)</span>
will be important later in the chapter. The following proposition gives its distribution:</p>

<div class="proposition">
<span id="prp:six5" class="proposition"><strong>Proposition 7.3  </strong></span>If <span class="math inline">\(\mathbf x\sim N_p({\boldsymbol{\mu}}, \boldsymbol{\Sigma})\)</span> and <span class="math inline">\(\boldsymbol{\Sigma}\)</span> is positive definite then
<span class="math display">\[(\mathbf x-{\boldsymbol{\mu}})^\top \boldsymbol{\Sigma}^{-1} (\mathbf x-{\boldsymbol{\mu}}) \sim \chi_p^2.\]</span>
</div>
<div class="proof">
<p><span id="unlabeled-div-28" class="proof"><em>Proof</em>. </span>Define <span class="math inline">\(\mathbf y= \boldsymbol{\Sigma}^{-1/2} (\mathbf x-{\boldsymbol{\mu}})\)</span> so
<span class="math display">\[\begin{align*}
(\mathbf x-{\boldsymbol{\mu}})^\top \boldsymbol{\Sigma}^{-1} (\mathbf x-{\boldsymbol{\mu}}) &amp;= \left(\boldsymbol{\Sigma}^{-1/2} (\mathbf x-{\boldsymbol{\mu}}) \right)^\top \left(\boldsymbol{\Sigma}^{-1/2} (\mathbf x-{\boldsymbol{\mu}}) \right)\\
&amp;= \mathbf y^\top \mathbf y= \sum_{i=1}^p y_i^2
\end{align*}\]</span>
By Corollary <a href="7.1-definition-and-properties-of-the-mvn.html#cor:csix3">7.2</a>, <span class="math inline">\(\mathbf y\sim N_p (\boldsymbol 0, \mathbf I_p)\)</span>, and so the components of <span class="math inline">\(\mathbf y\)</span> have independent univariate normal distributions with mean 0 and variance 1. Recall from univariate statistics that if <span class="math inline">\(z \sim N(0,1)\)</span> then <span class="math inline">\(z^2 \sim \chi^2_1\)</span> and if <span class="math inline">\(z_1, \ldots, z_n\)</span> are iid <span class="math inline">\(N(0,1)\)</span> then <span class="math inline">\(\sum_{i=1}^n z_i^2 \sim \chi_n^2\)</span>. It therefore follows that <span class="math display">\[\sum_{i=1}^p y_i^2 \sim \chi^2_p.\]</span></p>
</div>
<p><br></br>
<!--We saw earlier  that the MVN distribution in $p$ dimensions has constant density on ellipses or ellipsoids given by $f(\bx) = c$ for some constant $c > 0$, and that we can rearrange this equation to be of the form
$$U(\bx) = (\bx-\bmu)^\top \bSigma^{-1} (\bx-\bmu) = k$$
where $k = - 2 \log(c) - \log |2 \pi \bSigma| > 0$ is a combination of the constant, $c$,  and the normalising constant in the pdf.   --></p>
<p>Proposition <a href="7.1-definition-and-properties-of-the-mvn.html#prp:six5">7.3</a> means we can calculate the probability <span class="math display">\[{\mathbb{P}}\left((\mathbf x-{\boldsymbol{\mu}})^\top \boldsymbol{\Sigma}^{-1} (\mathbf x-{\boldsymbol{\mu}})&lt;k\right),\]</span> which is the probability of <span class="math inline">\(\mathbf x\)</span> lying within a particular ellipsoid. We often use this to draw confidence ellipses, which are ellipses that we expect to contain some specified proportion of the random samples (95% say).</p>
</div>
<div id="sampling-results-for-the-mvn" class="section level3" number="7.1.5">
<h3><span class="header-section-number">7.1.5</span> Sampling results for the MVN</h3>
<p>In this section we present two important results which are natural
generalisations of what happens in the univariate case.</p>

<div class="proposition">
<span id="prp:six6" class="proposition"><strong>Proposition 7.4  </strong></span>If <span class="math inline">\(\mathbf x_1, \ldots, \mathbf x_n\)</span> is an IID random sample from <span class="math inline">\(N_p({\boldsymbol{\mu}}, \boldsymbol{\Sigma})\)</span>, then the sample mean and sample variance matrix
<span class="math display">\[\bar{\mathbf x} = \frac{1}{n} \sum_{i=1}^n \mathbf x_i, \quad \mathbf S= \frac{1}{n} \sum_{i=1}^n (\mathbf x_i - \bar{\mathbf x})(\mathbf x_i-\bar{\mathbf x})^\top\]</span> are independent.
</div>
<div class="proof">
<p><span id="unlabeled-div-29" class="proof"><em>Proof</em>. </span>From Proposition <a href="7.1-definition-and-properties-of-the-mvn.html#prp:six2">7.1</a> we can see that if <span class="math inline">\(\mathbf x_1, \ldots, \mathbf x_n \sim N_p({\boldsymbol{\mu}}, \boldsymbol{\Sigma})\)</span> then <span class="math inline">\(\bar{\mathbf x} \sim N_p ({\boldsymbol{\mu}}, n^{-1}\boldsymbol{\Sigma})\)</span>. Let <span class="math inline">\(\mathbf y_i = \mathbf x_i -\bar{\mathbf x}\)</span>. Then
<span class="math display">\[\begin{align*}
{\mathbb{C}\operatorname{ov}}(\bar{\mathbf x},\mathbf y_i)&amp;={\mathbb{C}\operatorname{ov}}(\bar{\mathbf x}, \mathbf x_i -\bar{\mathbf x})\\
&amp;={\mathbb{C}\operatorname{ov}}(\bar{\mathbf x}, \mathbf x_i) - {\mathbb{C}\operatorname{ov}}(\bar{\mathbf x}, \bar{\mathbf x})\\
&amp;=n^{-1}\sum_{j=1}^n \left \{{\mathbb{E}}[(\mathbf x_j -{\boldsymbol{\mu}})(\mathbf x_i-{\boldsymbol{\mu}})^\top]\right \}\\
&amp; \qquad \qquad -{\mathbb{E}}[(\bar{\mathbf x}-{\boldsymbol{\mu}})(\bar{\mathbf x}-{\boldsymbol{\mu}})^\top]\\
&amp;=n^{-1}\boldsymbol{\Sigma}- n^{-1}\boldsymbol{\Sigma}\\
&amp;= {\mathbf 0}_{p,p}.
\end{align*}\]</span>
Thus Proposition <a href="7.1-definition-and-properties-of-the-mvn.html#prp:six4">7.2</a> gives that <span class="math inline">\(\bar{\mathbf x}\)</span> and <span class="math inline">\(\mathbf y_i\)</span> are independent, and therefore <span class="math inline">\(\bar{\mathbf x}\)</span> and
<span class="math display">\[
\mathbf S=\frac{1}{n}\sum_{i=1}^n \mathbf y_i \mathbf y_i^\top =n^{-1}\sum_{i=1}^n (\mathbf x_i -\bar{\mathbf x})(\mathbf x_i -\bar{\mathbf x})^\top
\]</span>
are independent.</p>
</div>
<p><br></br></p>
<p>Recall from above that if <span class="math inline">\(\mathbf x_1, \ldots, \mathbf x_n\)</span> is a random sample from <span class="math inline">\(N_p({\boldsymbol{\mu}}, \boldsymbol{\Sigma})\)</span> then <span class="math display">\[\bar{\mathbf x} \sim N_p ({\boldsymbol{\mu}}, \frac{1}{n}\boldsymbol{\Sigma}).\]</span> This result is also approximately true for large samples from non-normal distributions, as is now stated in the multivariate central limit theorem.</p>

<div class="proposition">
<span id="prp:clt" class="proposition"><strong>Proposition 7.5  </strong></span><strong>Central limit theorem</strong> Let <span class="math inline">\(\mathbf x_1, \mathbf x_2, \ldots \in \mathbb{R}^p\)</span> be a sample of independent and identically distributed random vectors from a distribution with mean <span class="math inline">\({\boldsymbol{\mu}}\)</span> and finite variance matrix <span class="math inline">\(\boldsymbol{\Sigma}\)</span>. Then asymptotically as <span class="math inline">\(n \rightarrow \infty\)</span>, <span class="math inline">\(\sqrt{n}(\bar{\mathbf x}-{\boldsymbol{\mu}})\)</span> converges in distribution to <span class="math inline">\(N_p ({\mathbf 0}_p, \boldsymbol{\Sigma})\)</span>.
</div>
<div class="proof">
<p><span id="unlabeled-div-30" class="proof"><em>Proof</em>. </span>Beyond the scope of this module.</p>
</div>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="7-multinormal.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="7.2-the-wishart-distribution.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["MultivariateStatistics.pdf"],
"search": {
"engine": "fuse",
"options": null
},
"toc": {
"collapse": "section"
},
"pandoc_args": "--top-level-division=[chapter|part]"
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
