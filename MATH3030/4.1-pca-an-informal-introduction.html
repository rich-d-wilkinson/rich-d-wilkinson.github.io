<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>4.1 PCA: an informal introduction | Multivariate Statistics</title>
  <meta name="description" content="The lecture notes for MATH3030/4068: Multivariate Analysis / Applied Multivariate Statistics" />
  <meta name="generator" content="bookdown 0.24.4 and GitBook 2.6.7" />

  <meta property="og:title" content="4.1 PCA: an informal introduction | Multivariate Statistics" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="The lecture notes for MATH3030/4068: Multivariate Analysis / Applied Multivariate Statistics" />
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="4.1 PCA: an informal introduction | Multivariate Statistics" />
  
  <meta name="twitter:description" content="The lecture notes for MATH3030/4068: Multivariate Analysis / Applied Multivariate Statistics" />
  

<meta name="author" content="Dr Katie Severn" />



  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="4-pca.html"/>
<link rel="next" href="4.2-pca-a-formal-description-with-proofs.html"/>
<script src="libs/header-attrs-2.11/header-attrs.js"></script>
<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections-1.0.1/anchor-sections.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.0.1/anchor-sections.js"></script>
<script src="libs/kePrint-0.0.1/kePrint.js"></script>
<link href="libs/lightable-0.0.1/lightable.css" rel="stylesheet" />


<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { color: #008000; } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { color: #008000; font-weight: bold; } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>


<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Applied multivariate statistics</a></li>

<li class="divider"></li>
<li><a href="index.html#introduction" id="toc-introduction">Introduction</a></li>
<li><a href="part-i-prerequisites.html#part-i-prerequisites" id="toc-part-i-prerequisites">PART I: Prerequisites</a></li>
<li><a href="1-stat-prelim.html#stat-prelim" id="toc-stat-prelim"><span class="toc-section-number">1</span> Statistical Preliminaries</a>
<ul>
<li><a href="1.1-notation.html#notation" id="toc-notation"><span class="toc-section-number">1.1</span> Notation</a>
<ul>
<li><a href="1.1-notation.html#example-datasets" id="toc-example-datasets"><span class="toc-section-number">1.1.1</span> Example datasets</a></li>
<li><a href="1.1-notation.html#aims-of-multivariate-data-analysis" id="toc-aims-of-multivariate-data-analysis"><span class="toc-section-number">1.1.2</span> Aims of multivariate data analysis</a></li>
</ul></li>
<li><a href="1.2-exploratory-data-analysis-eda.html#exploratory-data-analysis-eda" id="toc-exploratory-data-analysis-eda"><span class="toc-section-number">1.2</span> Exploratory data analysis (EDA)</a>
<ul>
<li><a href="1.2-exploratory-data-analysis-eda.html#data-visualization" id="toc-data-visualization"><span class="toc-section-number">1.2.1</span> Data visualization</a></li>
<li><a href="1.2-exploratory-data-analysis-eda.html#summary-statistics" id="toc-summary-statistics"><span class="toc-section-number">1.2.2</span> Summary statistics</a></li>
</ul></li>
<li><a href="1.3-randvec.html#randvec" id="toc-randvec"><span class="toc-section-number">1.3</span> Random vectors and matrices</a>
<ul>
<li><a href="1.3-randvec.html#estimators" id="toc-estimators"><span class="toc-section-number">1.3.1</span> Estimators</a></li>
</ul></li>
<li><a href="1.4-computer-tasks.html#computer-tasks" id="toc-computer-tasks"><span class="toc-section-number">1.4</span> Computer tasks</a></li>
<li><a href="1.5-exercises.html#exercises" id="toc-exercises"><span class="toc-section-number">1.5</span> Exercises</a></li>
</ul></li>
<li><a href="2-linalg-prelim.html#linalg-prelim" id="toc-linalg-prelim"><span class="toc-section-number">2</span> Review of linear algebra</a>
<ul>
<li><a href="2.1-linalg-basics.html#linalg-basics" id="toc-linalg-basics"><span class="toc-section-number">2.1</span> Basics</a>
<ul>
<li><a href="2.1-linalg-basics.html#notation-1" id="toc-notation-1"><span class="toc-section-number">2.1.1</span> Notation</a></li>
<li><a href="2.1-linalg-basics.html#elementary-matrix-operations" id="toc-elementary-matrix-operations"><span class="toc-section-number">2.1.2</span> Elementary matrix operations</a></li>
<li><a href="2.1-linalg-basics.html#special-matrices" id="toc-special-matrices"><span class="toc-section-number">2.1.3</span> Special matrices</a></li>
<li><a href="2.1-linalg-basics.html#vectordiff" id="toc-vectordiff"><span class="toc-section-number">2.1.4</span> Vector Differentiation</a></li>
</ul></li>
<li><a href="2.2-linalg-vecspaces.html#linalg-vecspaces" id="toc-linalg-vecspaces"><span class="toc-section-number">2.2</span> Vector spaces</a>
<ul>
<li><a href="2.2-linalg-vecspaces.html#linear-independence" id="toc-linear-independence"><span class="toc-section-number">2.2.1</span> Linear independence</a></li>
<li><a href="2.2-linalg-vecspaces.html#colsspace" id="toc-colsspace"><span class="toc-section-number">2.2.2</span> Row and column spaces</a></li>
<li><a href="2.2-linalg-vecspaces.html#linear-transformations" id="toc-linear-transformations"><span class="toc-section-number">2.2.3</span> Linear transformations</a></li>
</ul></li>
<li><a href="2.3-linalg-innerprod.html#linalg-innerprod" id="toc-linalg-innerprod"><span class="toc-section-number">2.3</span> Inner product spaces</a>
<ul>
<li><a href="2.3-linalg-innerprod.html#normed" id="toc-normed"><span class="toc-section-number">2.3.1</span> Distances, and angles</a></li>
<li><a href="2.3-linalg-innerprod.html#orthogonal-matrices" id="toc-orthogonal-matrices"><span class="toc-section-number">2.3.2</span> Orthogonal matrices</a></li>
<li><a href="2.3-linalg-innerprod.html#projection-matrix" id="toc-projection-matrix"><span class="toc-section-number">2.3.3</span> Projections</a></li>
</ul></li>
<li><a href="2.4-centering-matrix.html#centering-matrix" id="toc-centering-matrix"><span class="toc-section-number">2.4</span> The Centering Matrix</a></li>
<li><a href="2.5-tasks-ch2.html#tasks-ch2" id="toc-tasks-ch2"><span class="toc-section-number">2.5</span> Computer tasks</a></li>
<li><a href="2.6-exercises-ch2.html#exercises-ch2" id="toc-exercises-ch2"><span class="toc-section-number">2.6</span> Exercises</a></li>
</ul></li>
<li><a href="3-linalg-decomp.html#linalg-decomp" id="toc-linalg-decomp"><span class="toc-section-number">3</span> Matrix decompositions</a>
<ul>
<li><a href="3.1-matrix-matrix.html#matrix-matrix" id="toc-matrix-matrix"><span class="toc-section-number">3.1</span> Matrix-matrix products</a></li>
<li><a href="3.2-spectraleigen-decomposition.html#spectraleigen-decomposition" id="toc-spectraleigen-decomposition"><span class="toc-section-number">3.2</span> Spectral/eigen decomposition</a>
<ul>
<li><a href="3.2-spectraleigen-decomposition.html#eigenvalues-and-eigenvectors" id="toc-eigenvalues-and-eigenvectors"><span class="toc-section-number">3.2.1</span> Eigenvalues and eigenvectors</a></li>
<li><a href="3.2-spectraleigen-decomposition.html#spectral-decomposition" id="toc-spectral-decomposition"><span class="toc-section-number">3.2.2</span> Spectral decomposition</a></li>
<li><a href="3.2-spectraleigen-decomposition.html#matrixroots" id="toc-matrixroots"><span class="toc-section-number">3.2.3</span> Matrix square roots</a></li>
</ul></li>
<li><a href="3.3-linalg-SVD.html#linalg-SVD" id="toc-linalg-SVD"><span class="toc-section-number">3.3</span> Singular Value Decomposition (SVD)</a>
<ul>
<li><a href="3.3-linalg-SVD.html#examples" id="toc-examples"><span class="toc-section-number">3.3.1</span> Examples</a></li>
</ul></li>
<li><a href="3.4-svdopt.html#svdopt" id="toc-svdopt"><span class="toc-section-number">3.4</span> SVD optimization results</a></li>
<li><a href="3.5-lowrank.html#lowrank" id="toc-lowrank"><span class="toc-section-number">3.5</span> Low-rank approximation</a>
<ul>
<li><a href="3.5-lowrank.html#matrix-norms" id="toc-matrix-norms"><span class="toc-section-number">3.5.1</span> Matrix norms</a></li>
<li><a href="3.5-lowrank.html#eckart-young-mirsky-theorem" id="toc-eckart-young-mirsky-theorem"><span class="toc-section-number">3.5.2</span> Eckart-Young-Mirsky Theorem</a></li>
<li><a href="3.5-lowrank.html#example-image-compression" id="toc-example-image-compression"><span class="toc-section-number">3.5.3</span> Example: image compression</a></li>
</ul></li>
<li><a href="3.6-tasks-ch3.html#tasks-ch3" id="toc-tasks-ch3"><span class="toc-section-number">3.6</span> Computer tasks</a></li>
<li><a href="3.7-exercises-ch3.html#exercises-ch3" id="toc-exercises-ch3"><span class="toc-section-number">3.7</span> Exercises</a></li>
</ul></li>
<li><a href="part-ii-dimension-reduction-methods.html#part-ii-dimension-reduction-methods" id="toc-part-ii-dimension-reduction-methods">PART II: Dimension reduction methods</a>
<ul>
<li><a href="part-ii-dimension-reduction-methods.html#a-warning" id="toc-a-warning">A warning</a></li>
</ul></li>
<li><a href="4-pca.html#pca" id="toc-pca"><span class="toc-section-number">4</span> Principal Component Analysis (PCA)</a>
<ul>
<li><a href="4.1-pca-an-informal-introduction.html#pca-an-informal-introduction" id="toc-pca-an-informal-introduction"><span class="toc-section-number">4.1</span> PCA: an informal introduction</a>
<ul>
<li><a href="4.1-pca-an-informal-introduction.html#notation-recap" id="toc-notation-recap"><span class="toc-section-number">4.1.1</span> Notation recap</a></li>
<li><a href="4.1-pca-an-informal-introduction.html#first-principal-component" id="toc-first-principal-component"><span class="toc-section-number">4.1.2</span> First principal component</a></li>
<li><a href="4.1-pca-an-informal-introduction.html#second-principal-component" id="toc-second-principal-component"><span class="toc-section-number">4.1.3</span> Second principal component</a></li>
<li><a href="4.1-pca-an-informal-introduction.html#geometric-interpretation-1" id="toc-geometric-interpretation-1"><span class="toc-section-number">4.1.4</span> Geometric interpretation</a></li>
<li><a href="4.1-pca-an-informal-introduction.html#example" id="toc-example"><span class="toc-section-number">4.1.5</span> Example</a></li>
<li><a href="4.1-pca-an-informal-introduction.html#example-iris" id="toc-example-iris"><span class="toc-section-number">4.1.6</span> Example: Iris</a></li>
</ul></li>
<li><a href="4.2-pca-a-formal-description-with-proofs.html#pca-a-formal-description-with-proofs" id="toc-pca-a-formal-description-with-proofs"><span class="toc-section-number">4.2</span> PCA: a formal description with proofs</a>
<ul>
<li><a href="4.2-pca-a-formal-description-with-proofs.html#properties-of-principal-components" id="toc-properties-of-principal-components"><span class="toc-section-number">4.2.1</span> Properties of principal components</a></li>
<li><a href="4.2-pca-a-formal-description-with-proofs.html#pca:football" id="toc-pca:football"><span class="toc-section-number">4.2.2</span> Example: Football</a></li>
<li><a href="4.2-pca-a-formal-description-with-proofs.html#pcawithR" id="toc-pcawithR"><span class="toc-section-number">4.2.3</span> PCA based on <span class="math inline">\(\mathbf R\)</span> versus PCA based on <span class="math inline">\(\mathbf S\)</span></a></li>
<li><a href="4.2-pca-a-formal-description-with-proofs.html#population-pca" id="toc-population-pca"><span class="toc-section-number">4.2.4</span> Population PCA</a></li>
<li><a href="4.2-pca-a-formal-description-with-proofs.html#pca-under-transformations-of-variables" id="toc-pca-under-transformations-of-variables"><span class="toc-section-number">4.2.5</span> PCA under transformations of variables</a></li>
</ul></li>
<li><a href="4.3-an-alternative-view-of-pca.html#an-alternative-view-of-pca" id="toc-an-alternative-view-of-pca"><span class="toc-section-number">4.3</span> An alternative view of PCA</a>
<ul>
<li><a href="4.3-an-alternative-view-of-pca.html#pca-mnist" id="toc-pca-mnist"><span class="toc-section-number">4.3.1</span> Example: MNIST handwritten digits</a></li>
</ul></li>
<li><a href="4.4-pca-comptask.html#pca-comptask" id="toc-pca-comptask"><span class="toc-section-number">4.4</span> Computer tasks</a></li>
<li><a href="4.5-exercises-1.html#exercises-1" id="toc-exercises-1"><span class="toc-section-number">4.5</span> Exercises</a></li>
</ul></li>
<li><a href="5-cca.html#cca" id="toc-cca"><span class="toc-section-number">5</span> Canonical Correlation Analysis (CCA)</a>
<ul>
<li><a href="5.1-cca1.html#cca1" id="toc-cca1"><span class="toc-section-number">5.1</span> The first pair of canonical variables</a>
<ul>
<li><a href="5.1-cca1.html#the-first-canonical-components" id="toc-the-first-canonical-components"><span class="toc-section-number">5.1.1</span> The first canonical components</a></li>
<li><a href="5.1-cca1.html#premcca" id="toc-premcca"><span class="toc-section-number">5.1.2</span> Example: Premier league football</a></li>
</ul></li>
<li><a href="5.2-the-full-set-of-canonical-correlations.html#the-full-set-of-canonical-correlations" id="toc-the-full-set-of-canonical-correlations"><span class="toc-section-number">5.2</span> The full set of canonical correlations</a>
<ul>
<li><a href="5.2-the-full-set-of-canonical-correlations.html#example-continued" id="toc-example-continued"><span class="toc-section-number">5.2.1</span> Example continued</a></li>
</ul></li>
<li><a href="5.3-properties.html#properties" id="toc-properties"><span class="toc-section-number">5.3</span> Properties</a>
<ul>
<li><a href="5.3-properties.html#connection-with-linear-regression-when-q1" id="toc-connection-with-linear-regression-when-q1"><span class="toc-section-number">5.3.1</span> Connection with linear regression when <span class="math inline">\(q=1\)</span></a></li>
<li><a href="5.3-properties.html#invarianceequivariance-properties-of-cca" id="toc-invarianceequivariance-properties-of-cca"><span class="toc-section-number">5.3.2</span> Invariance/equivariance properties of CCA</a></li>
</ul></li>
<li><a href="5.4-computer-tasks-1.html#computer-tasks-1" id="toc-computer-tasks-1"><span class="toc-section-number">5.4</span> Computer tasks</a></li>
<li><a href="5.5-exercises-2.html#exercises-2" id="toc-exercises-2"><span class="toc-section-number">5.5</span> Exercises</a></li>
</ul></li>
<li><a href="6-mds.html#mds" id="toc-mds"><span class="toc-section-number">6</span> Multidimensional Scaling (MDS)</a>
<ul>
<li><a href="6.1-classical-mds.html#classical-mds" id="toc-classical-mds"><span class="toc-section-number">6.1</span> Classical MDS</a>
<ul>
<li><a href="6.1-classical-mds.html#non-euclidean-distance-matrices" id="toc-non-euclidean-distance-matrices"><span class="toc-section-number">6.1.1</span> Non-Euclidean distance matrices</a></li>
<li><a href="6.1-classical-mds.html#principal-coordinate-analysis" id="toc-principal-coordinate-analysis"><span class="toc-section-number">6.1.2</span> Principal Coordinate Analysis</a></li>
</ul></li>
<li><a href="6.2-similarity.html#similarity" id="toc-similarity"><span class="toc-section-number">6.2</span> Similarity measures</a>
<ul>
<li><a href="6.2-similarity.html#binary-attributes" id="toc-binary-attributes"><span class="toc-section-number">6.2.1</span> Binary attributes</a></li>
<li><a href="6.2-similarity.html#example-classical-mds-with-the-mnist-data" id="toc-example-classical-mds-with-the-mnist-data"><span class="toc-section-number">6.2.2</span> Example: Classical MDS with the MNIST data</a></li>
</ul></li>
<li><a href="6.3-non-metric-mds.html#non-metric-mds" id="toc-non-metric-mds"><span class="toc-section-number">6.3</span> Non-metric MDS</a></li>
<li><a href="6.4-exercises-3.html#exercises-3" id="toc-exercises-3"><span class="toc-section-number">6.4</span> Exercises</a></li>
<li><a href="6.5-computer-tasks-2.html#computer-tasks-2" id="toc-computer-tasks-2"><span class="toc-section-number">6.5</span> Computer Tasks</a></li>
</ul></li>
<li><a href="part-iii-inference-using-the-multivariate-normal-distribution-mvn.html#part-iii-inference-using-the-multivariate-normal-distribution-mvn" id="toc-part-iii-inference-using-the-multivariate-normal-distribution-mvn">Part III: Inference using the Multivariate Normal Distribution (MVN)</a></li>
<li><a href="7-multinormal.html#multinormal" id="toc-multinormal"><span class="toc-section-number">7</span> The Multivariate Normal Distribution</a>
<ul>
<li><a href="7.1-definition-and-properties-of-the-mvn.html#definition-and-properties-of-the-mvn" id="toc-definition-and-properties-of-the-mvn"><span class="toc-section-number">7.1</span> Definition and Properties of the MVN</a>
<ul>
<li><a href="7.1-definition-and-properties-of-the-mvn.html#basics" id="toc-basics"><span class="toc-section-number">7.1.1</span> Basics</a></li>
<li><a href="7.1-definition-and-properties-of-the-mvn.html#transformations" id="toc-transformations"><span class="toc-section-number">7.1.2</span> Transformations</a></li>
<li><a href="7.1-definition-and-properties-of-the-mvn.html#independence" id="toc-independence"><span class="toc-section-number">7.1.3</span> Independence</a></li>
<li><a href="7.1-definition-and-properties-of-the-mvn.html#confidence-ellipses" id="toc-confidence-ellipses"><span class="toc-section-number">7.1.4</span> Confidence ellipses</a></li>
<li><a href="7.1-definition-and-properties-of-the-mvn.html#sampling-results-for-the-mvn" id="toc-sampling-results-for-the-mvn"><span class="toc-section-number">7.1.5</span> Sampling results for the MVN</a></li>
</ul></li>
<li><a href="7.2-the-wishart-distribution.html#the-wishart-distribution" id="toc-the-wishart-distribution"><span class="toc-section-number">7.2</span> The Wishart distribution</a>
<ul>
<li><a href="7.2-the-wishart-distribution.html#properties-1" id="toc-properties-1"><span class="toc-section-number">7.2.1</span> Properties</a></li>
<li><a href="7.2-the-wishart-distribution.html#cochrans-theorem" id="toc-cochrans-theorem"><span class="toc-section-number">7.2.2</span> Cochran’s theorem</a></li>
</ul></li>
<li><a href="7.3-hotellings-t2-distribution.html#hotellings-t2-distribution" id="toc-hotellings-t2-distribution"><span class="toc-section-number">7.3</span> Hotelling’s <span class="math inline">\(T^2\)</span> distribution</a></li>
<li><a href="7.4-inference-based-on-the-mvn.html#inference-based-on-the-mvn" id="toc-inference-based-on-the-mvn"><span class="toc-section-number">7.4</span> Inference based on the MVN</a>
<ul>
<li><a href="7.4-inference-based-on-the-mvn.html#onesampleSigma" id="toc-onesampleSigma"><span class="toc-section-number">7.4.1</span> <span class="math inline">\(\boldsymbol{\Sigma}\)</span> known</a></li>
<li><a href="7.4-inference-based-on-the-mvn.html#onesample" id="toc-onesample"><span class="toc-section-number">7.4.2</span> <span class="math inline">\(\boldsymbol{\Sigma}\)</span> unknown: 1 sample</a></li>
<li><a href="7.4-inference-based-on-the-mvn.html#boldsymbolsigma-unknown-2-samples" id="toc-boldsymbolsigma-unknown-2-samples"><span class="toc-section-number">7.4.3</span> <span class="math inline">\(\boldsymbol{\Sigma}\)</span> unknown: 2 samples</a></li>
</ul></li>
<li><a href="7.5-exercises-4.html#exercises-4" id="toc-exercises-4"><span class="toc-section-number">7.5</span> Exercises</a></li>
<li><a href="7.6-computer-tasks-3.html#computer-tasks-3" id="toc-computer-tasks-3"><span class="toc-section-number">7.6</span> Computer tasks</a></li>
</ul></li>
<li><a href="part-iv-classification-and-clustering.html#part-iv-classification-and-clustering" id="toc-part-iv-classification-and-clustering">Part IV: Classification and Clustering</a></li>
<li><a href="8-lda.html#lda" id="toc-lda"><span class="toc-section-number">8</span> Discriminant analysis</a>
<ul>
<li><a href="8-lda.html#linear-discriminant-analysis" id="toc-linear-discriminant-analysis">Linear discriminant analysis</a></li>
<li><a href="8.1-lda-ML.html#lda-ML" id="toc-lda-ML"><span class="toc-section-number">8.1</span> Maximum likelihood (ML) discriminant rule</a>
<ul>
<li><a href="8.1-lda-ML.html#multivariate-normal-populations" id="toc-multivariate-normal-populations"><span class="toc-section-number">8.1.1</span> Multivariate normal populations</a></li>
<li><a href="8.1-lda-ML.html#sample-lda" id="toc-sample-lda"><span class="toc-section-number">8.1.2</span> The sample ML discriminant rule</a></li>
<li><a href="8.1-lda-ML.html#two-populations" id="toc-two-populations"><span class="toc-section-number">8.1.3</span> Two populations</a></li>
<li><a href="8.1-lda-ML.html#more-than-two-populations" id="toc-more-than-two-populations"><span class="toc-section-number">8.1.4</span> More than two populations</a></li>
</ul></li>
<li><a href="8.2-lda-Bayes.html#lda-Bayes" id="toc-lda-Bayes"><span class="toc-section-number">8.2</span> Bayes discriminant rule</a>
<ul>
<li><a href="8.2-lda-Bayes.html#example-lda-using-the-iris-data" id="toc-example-lda-using-the-iris-data"><span class="toc-section-number">8.2.1</span> Example: LDA using the Iris data</a></li>
<li><a href="8.2-lda-Bayes.html#quadratic-discriminant-analysis-qda" id="toc-quadratic-discriminant-analysis-qda"><span class="toc-section-number">8.2.2</span> Quadratic Discriminant Analysis (QDA)</a></li>
<li><a href="8.2-lda-Bayes.html#prediction-accuracy" id="toc-prediction-accuracy"><span class="toc-section-number">8.2.3</span> Prediction accuracy</a></li>
</ul></li>
<li><a href="8.3-FLDA.html#FLDA" id="toc-FLDA"><span class="toc-section-number">8.3</span> Fisher’s linear discriminant rule</a>
<ul>
<li><a href="8.3-FLDA.html#iris-example-continued-1" id="toc-iris-example-continued-1"><span class="toc-section-number">8.3.1</span> Iris example continued</a></li>
<li><a href="8.3-FLDA.html#links-between-methods" id="toc-links-between-methods"><span class="toc-section-number">8.3.2</span> Links between methods</a></li>
</ul></li>
<li><a href="8.4-computer-tasks-4.html#computer-tasks-4" id="toc-computer-tasks-4"><span class="toc-section-number">8.4</span> Computer tasks</a></li>
<li><a href="8.5-exercises-5.html#exercises-5" id="toc-exercises-5"><span class="toc-section-number">8.5</span> Exercises</a></li>
</ul></li>
<li><a href="9-cluster.html#cluster" id="toc-cluster"><span class="toc-section-number">9</span> Cluster Analysis</a>
<ul>
<li><a href="9.1-k-means-clustering.html#k-means-clustering" id="toc-k-means-clustering"><span class="toc-section-number">9.1</span> K-means clustering</a>
<ul>
<li><a href="9.1-k-means-clustering.html#estimating-boldsymbol-delta" id="toc-estimating-boldsymbol-delta"><span class="toc-section-number">9.1.1</span> Estimating <span class="math inline">\(\boldsymbol \delta\)</span></a></li>
<li><a href="9.1-k-means-clustering.html#k-means" id="toc-k-means"><span class="toc-section-number">9.1.2</span> K-means</a></li>
<li><a href="9.1-k-means-clustering.html#example-iris-data" id="toc-example-iris-data"><span class="toc-section-number">9.1.3</span> Example: Iris data</a></li>
<li><a href="9.1-k-means-clustering.html#choosing-k" id="toc-choosing-k"><span class="toc-section-number">9.1.4</span> Choosing <span class="math inline">\(K\)</span></a></li>
</ul></li>
<li><a href="9.2-model-based-clustering.html#model-based-clustering" id="toc-model-based-clustering"><span class="toc-section-number">9.2</span> Model-based clustering</a>
<ul>
<li><a href="9.2-model-based-clustering.html#maximum-likelihood-estimation" id="toc-maximum-likelihood-estimation"><span class="toc-section-number">9.2.1</span> Maximum-likelihood estimation</a></li>
<li><a href="9.2-model-based-clustering.html#multivariate-gaussian-clusters" id="toc-multivariate-gaussian-clusters"><span class="toc-section-number">9.2.2</span> Multivariate Gaussian clusters</a></li>
<li><a href="9.2-model-based-clustering.html#example-iris-1" id="toc-example-iris-1"><span class="toc-section-number">9.2.3</span> Example: Iris</a></li>
</ul></li>
<li><a href="9.3-hierarchical-clustering-methods.html#hierarchical-clustering-methods" id="toc-hierarchical-clustering-methods"><span class="toc-section-number">9.3</span> Hierarchical clustering methods</a>
<ul>
<li><a href="9.3-hierarchical-clustering-methods.html#distance-measures" id="toc-distance-measures"><span class="toc-section-number">9.3.1</span> Distance measures</a></li>
<li><a href="9.3-hierarchical-clustering-methods.html#toy-example" id="toc-toy-example"><span class="toc-section-number">9.3.2</span> Toy Example</a></li>
<li><a href="9.3-hierarchical-clustering-methods.html#comparison-of-methods" id="toc-comparison-of-methods"><span class="toc-section-number">9.3.3</span> Comparison of methods</a></li>
</ul></li>
<li><a href="9.4-summary.html#summary" id="toc-summary"><span class="toc-section-number">9.4</span> Summary</a></li>
<li><a href="9.5-computer-tasks-5.html#computer-tasks-5" id="toc-computer-tasks-5"><span class="toc-section-number">9.5</span> Computer tasks</a></li>
<li><a href="9.6-exercises-6.html#exercises-6" id="toc-exercises-6"><span class="toc-section-number">9.6</span> Exercises</a></li>
</ul></li>
<li><a href="10-lm.html#lm" id="toc-lm"><span class="toc-section-number">10</span> Linear Models</a>
<ul>
<li><a href="10-lm.html#notation-3" id="toc-notation-3">Notation</a></li>
<li><a href="10.1-ordinary-least-squares-ols.html#ordinary-least-squares-ols" id="toc-ordinary-least-squares-ols"><span class="toc-section-number">10.1</span> Ordinary least squares (OLS)</a>
<ul>
<li><a href="10.1-ordinary-least-squares-ols.html#geometry" id="toc-geometry"><span class="toc-section-number">10.1.1</span> Geometry</a></li>
<li><a href="10.1-ordinary-least-squares-ols.html#normal-linear-model" id="toc-normal-linear-model"><span class="toc-section-number">10.1.2</span> Normal linear model</a></li>
<li><a href="10.1-ordinary-least-squares-ols.html#linear-models-in-r" id="toc-linear-models-in-r"><span class="toc-section-number">10.1.3</span> Linear models in R</a></li>
<li><a href="10.1-ordinary-least-squares-ols.html#problems-with-ols" id="toc-problems-with-ols"><span class="toc-section-number">10.1.4</span> Problems with OLS</a></li>
</ul></li>
<li><a href="10.2-principal-component-regression-pcr.html#principal-component-regression-pcr" id="toc-principal-component-regression-pcr"><span class="toc-section-number">10.2</span> Principal component regression (PCR)</a>
<ul>
<li><a href="10.2-principal-component-regression-pcr.html#pcr-in-r" id="toc-pcr-in-r"><span class="toc-section-number">10.2.1</span> PCR in R</a></li>
</ul></li>
<li><a href="10.3-shrinkage-methods.html#shrinkage-methods" id="toc-shrinkage-methods"><span class="toc-section-number">10.3</span> Shrinkage methods</a>
<ul>
<li><a href="10.3-shrinkage-methods.html#ridge-regression-in-r" id="toc-ridge-regression-in-r"><span class="toc-section-number">10.3.1</span> Ridge regression in R</a></li>
</ul></li>
<li><a href="10.4-multi-output-linear-model.html#multi-output-linear-model" id="toc-multi-output-linear-model"><span class="toc-section-number">10.4</span> Multi-output Linear Model</a>
<ul>
<li><a href="10.4-multi-output-linear-model.html#normal-linear-model-1" id="toc-normal-linear-model-1"><span class="toc-section-number">10.4.1</span> Normal linear model</a></li>
<li><a href="10.4-multi-output-linear-model.html#reduced-rank-regression" id="toc-reduced-rank-regression"><span class="toc-section-number">10.4.2</span> Reduced rank regression</a></li>
</ul></li>
<li><a href="10.5-computer-tasks-6.html#computer-tasks-6" id="toc-computer-tasks-6"><span class="toc-section-number">10.5</span> Computer tasks</a></li>
<li><a href="10.6-exercises-7.html#exercises-7" id="toc-exercises-7"><span class="toc-section-number">10.6</span> Exercises</a></li>
</ul></li>
<li class="divider"></li>
<li> <a href="https://rich-d-wilkinson.github.io/teaching.html" target="blank">University of Nottingham</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Multivariate Statistics</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="pca-an-informal-introduction" class="section level2" number="4.1">
<h2><span class="header-section-number">4.1</span> PCA: an informal introduction</h2>
<p>There are two different ways of motivating
principal component analysis (PCA), which may in part explain why PCA is so widely used.</p>
<p>The first motivation, and the topic of this section, is to introduce PCA as method for maximizing the variance of the transformed variables <span class="math inline">\(\mathbf y\)</span>. We start by choosing <span class="math inline">\(\mathbf u_1\)</span> so that <span class="math inline">\(y_1=\mathbf u_1^\top \mathbf x\)</span> has maximum variance. We then choose <span class="math inline">\(\mathbf u_2\)</span> so that <span class="math inline">\(y_2=\mathbf u_2^\top \mathbf x\)</span> has maximum variance subject to being uncorrelated with <span class="math inline">\(y_1\)</span>, and so on.</p>
<p>The idea is to produce a set of variables <span class="math inline">\(y_1, y_2, \ldots, y_r\)</span> that are uncorrelated, but which are most informative about the data. The thinking is that if a variable has large variance it must be informative/important.</p>
<p>The name <strong>principal component analysis</strong> comes from thinking of this as splitting the data <span class="math inline">\(\mathbf X\)</span> into its most important parts. It therefore won’t surprise you to find that this involves the matrix decompositions we studied in Chapter <a href="3-linalg-decomp.html#linalg-decomp">3</a>.</p>
<p><a href="https://twitter.com/allison_horst/status/1288904459490213888?lang=en">Allison Horst (@allison_horst)</a> gave a great illustration of how to think about PCA on Twitter. Imagine you are a whale shark with a wide mouth</p>
<p><img src="figs/WideMouthShark1.png" /></p>
<p>and that you’re swimming towards a delicious swarm of krill.</p>
<p><img src="figs/WideMouthShark2.png" /></p>
<p>What way should you tilt your shark head in order to eat as many krill as possible? The answer is given by the first principal component of the data!</p>
<div id="notation-recap" class="section level3" number="4.1.1">
<h3><span class="header-section-number">4.1.1</span> Notation recap</h3>
<p>As before, let <span class="math inline">\(\mathbf x_1,\ldots,\mathbf x_n\)</span> be <span class="math inline">\(p \times 1\)</span> vectors of measurements on <span class="math inline">\(n\)</span> experimental units and write
<span class="math display">\[\mathbf X=\left( \begin{array}{ccc}
- &amp;\mathbf x_1^\top&amp;-\\
- &amp;\mathbf x_2^\top&amp;-\\
- &amp;..&amp;-\\
- &amp;\mathbf x_n^\top&amp;-
\end{array}\right)
\]</span></p>
<p><strong>IMPORTANT NOTE:</strong>
In this section we will assume that <span class="math inline">\(\mathbf X\)</span> has been column centered so that the mean of each column is <span class="math inline">\(0\)</span> (i.e., the sample mean of <span class="math inline">\(\mathbf x_1,\ldots,\mathbf x_n\)</span> is the zero vector <span class="math inline">\(\boldsymbol 0\in \mathbb{R}^p\)</span>). If <span class="math inline">\(\mathbf X\)</span> has not been column centered, replace <span class="math inline">\(\mathbf X\)</span> by
<span class="math display">\[\mathbf H\mathbf X\]</span> where <span class="math inline">\(\mathbf H\)</span> is the centering matrix (see <a href="2.4-centering-matrix.html#centering-matrix">2.4</a>), or equivalently, replace <span class="math inline">\(\mathbf x_i\)</span> by <span class="math inline">\(\mathbf x_i - \bar{\mathbf x}\)</span>. It is possible to write out the details of PCA replacing <span class="math inline">\(\mathbf X\)</span> by <span class="math inline">\(\mathbf H\mathbf X\)</span> throughout, but this gets messy and obscures the important detail. Most software implementations (and in particular <code>prcomp</code> in R), automatically centre your data for you, and so in practice you don’t need to worry about doing this when using a software package.</p>
<p>The sample covariance matrix for <span class="math inline">\(\mathbf X\)</span> (assuming it has been column centered) is
<span class="math display">\[\mathbf S= \frac{1}{n}\mathbf X^\top \mathbf X= \frac{1}{n}\sum \mathbf x_i\mathbf x_i^\top\]</span></p>
<p>Given some vector <span class="math inline">\(\mathbf u\)</span>, the transformed variables
<span class="math display">\[y_i = \mathbf u^\top \mathbf x_i\]</span>
have</p>
<ul>
<li><p><strong>mean <span class="math inline">\(0\)</span></strong>:
<span class="math display">\[\bar{y}= \frac{1}{n}\sum_{i=1}^n y_i = \frac{1}{n}\sum_{i=1}^n \mathbf u^\top \mathbf x_i =\frac{1}{n} \mathbf u^\top \sum_{i=1}^n  \mathbf x_i = 0\]</span>
as the mean of the <span class="math inline">\(\mathbf x_i\)</span> is <span class="math inline">\(\boldsymbol 0\)</span>.</p></li>
<li><p><strong>sample covariance matrix</strong> <span class="math display">\[\mathbf u^\top \mathbf S\mathbf u\]</span>
as
<span class="math display">\[\frac{1}{n} \sum_{i=1}^n y_i^2 = \frac{1}{n} \sum_{i=1}^n \mathbf u^\top \mathbf x_i \mathbf x_i^\top\mathbf u= \frac{1}{n}\mathbf u^\top \sum_{i=1}^n  \mathbf x_i \mathbf x_i^\top \mathbf u= \mathbf u^\top \mathbf S\mathbf u
\]</span></p></li>
</ul>
</div>
<div id="first-principal-component" class="section level3" number="4.1.2">
<h3><span class="header-section-number">4.1.2</span> First principal component</h3>
<p>We would like to find the <span class="math inline">\(\mathbf u\)</span> which maximises the sample variance, <span class="math inline">\(\mathbf u^\top \mathbf S\mathbf u\)</span> over unit vectors <span class="math inline">\(\mathbf u\)</span>, i.e., vectors with <span class="math inline">\(||\mathbf u||=1\)</span>. Why do we focus on unit vectors? If we don’t, we could make the variance as large as we like, e.g., if we replace <span class="math inline">\(\mathbf u\)</span> by <span class="math inline">\(10\mathbf u\)</span> it would increase the variance by a factor of 100. Thus, we constrain the problem and only consider unit vectors for <span class="math inline">\(\mathbf u\)</span>.</p>
<p>We know from Proposition <a href="3.4-svdopt.html#prp:two8">3.7</a> in Section <a href="3.4-svdopt.html#svdopt">3.4</a> that <span class="math inline">\(\mathbf v_1\)</span>, the first eigenvector of <span class="math inline">\(\mathbf S\)</span> (also the first right singular vector of <span class="math inline">\(\mathbf X\)</span>), maximizes <span class="math inline">\(\mathbf u^\top \mathbf S\mathbf u\)</span> with
<span class="math display">\[  \max_{\mathbf u: ||\mathbf u||=1} \mathbf u^\top \mathbf S\mathbf u= \mathbf v_1 \mathbf S\mathbf v_1 =\lambda_1\]</span>
where <span class="math inline">\(\lambda_1\)</span> is the largest eigenvalue of <span class="math inline">\(\mathbf S\)</span>.</p>
<p>So the first principal component of <span class="math inline">\(\mathbf X\)</span> is <span class="math inline">\(\mathbf v_1\)</span>, and the first transformed variable (sometimes called a principal component score) is <span class="math inline">\(y_1 = \mathbf v_1 ^\top \mathbf x\)</span>.
Applying this to each data point we get <span class="math inline">\(n\)</span> instances of this new variable
<span class="math display">\[y_{i1} = \mathbf v_1 ^\top \mathbf x_i.\]</span></p>
<p><strong>A note on singular values</strong>: We know <span class="math inline">\(\mathbf S= \frac{1}{n}\mathbf X^\top\mathbf X\)</span> and so the eigenvalues of <span class="math inline">\(\mathbf S\)</span> are the same as the squared singular values of <span class="math inline">\(\frac{1}{\sqrt{n}} \mathbf X\)</span>:</p>
<p><span class="math display">\[\sqrt{\lambda_1} = \sigma_1\left(\frac{1}{\sqrt{n}} \mathbf X\right)\]</span></p>
<p>If we scale <span class="math inline">\(\mathbf X\)</span> by a factor <span class="math inline">\(c\)</span>, then the singular values are scaled by the same amount, i.e.,
<span class="math display">\[\sigma_i(c\mathbf X)=c\sigma_i(\mathbf X)\]</span>
and in particular
<span class="math display">\[ \sigma_i\left(\frac{1}{\sqrt{n}} \mathbf X\right) = \frac{1}{\sqrt{n}} \sigma_i(\mathbf X)\]</span>
We will need to remember this scaling if we use the SVD of <span class="math inline">\(\mathbf X\)</span> to do PCA. Note that scaling <span class="math inline">\(\mathbf X\)</span> does not change the singular vectors/principal components.</p>
</div>
<div id="second-principal-component" class="section level3" number="4.1.3">
<h3><span class="header-section-number">4.1.3</span> Second principal component</h3>
<p><span class="math inline">\(y_1\)</span> is the transformed variable that has maximum variance. What should we choose to be our next transformed variable, i.e., what <span class="math inline">\(\mathbf u_2\)</span> should we choose for <span class="math inline">\(y_2 = \mathbf u_2^\top \mathbf x\)</span>? It makes sense to choose <span class="math inline">\(y_2\)</span> to be uncorrelated with <span class="math inline">\(y_1\)</span>, as otherwise it contains some of the same information given by <span class="math inline">\(y_1\)</span>. The sample covariance between <span class="math inline">\(y_1\)</span> and <span class="math inline">\(\mathbf u_2^\top \mathbf x\)</span> is
<span class="math display">\[\begin{align*}
s_{y_2y_1} &amp;=\frac{1}{n}\sum_{i=1}^n \mathbf u_2^\top \mathbf x_i \mathbf x_i^\top \mathbf v_1\\
&amp;= \mathbf u_2^\top \mathbf S\mathbf v_1\\
&amp; = \lambda_1 \mathbf u_2^\top \mathbf v_1 \mbox{ as } \mathbf v_1 \mbox{ is an eigenvector of } S
\end{align*}\]</span>
So to make <span class="math inline">\(y_2\)</span> uncorrelated with <span class="math inline">\(y_1\)</span> we have to choose <span class="math inline">\(\mathbf u_2\)</span> to be orthogonal to <span class="math inline">\(\mathbf v_1\)</span>, i.e., <span class="math inline">\(\mathbf u_2^\top \mathbf v_1=0\)</span>. So we choose <span class="math inline">\(\mathbf u_2\)</span> to be the solution to the optimization problem</p>
<p><span class="math display">\[\max_{\mathbf u} \mathbf u^\top \mathbf S\mathbf u\mbox{ subject to } \mathbf u^\top \mathbf v_1=0.\]</span>
The solution to this problem is to take <span class="math inline">\(\mathbf u_2 = \mathbf v_2\)</span>, i.e., the second eigenvector of <span class="math inline">\(\mathbf S\)</span> (or second right singular vector of <span class="math inline">\(\mathbf X\)</span>), and then <span class="math display">\[\mathbf v_2^\top \mathbf S\mathbf v_2=\lambda_2.\]</span>
We’ll prove this result in the next section.</p>
<div id="later-principal-components" class="section level4 unnumbered">
<h4>Later principal components</h4>
<p>Our first transformed variable is
<span class="math display">\[y_{i1}= \mathbf v_1^\top \mathbf x_i\]</span>
and our second transformed variable is
<span class="math display">\[y_{i2}= \mathbf v_2^\top \mathbf x_i.\]</span>
At this point, you can probably guess that the <span class="math inline">\(j^{th}\)</span> transformed variable is going to be
<span class="math display">\[y_{ij}= \mathbf v_j^\top \mathbf x_i.\]</span>
where <span class="math inline">\(\mathbf v_j\)</span> is the <span class="math inline">\(j^{th}\)</span> eigenvector of <span class="math inline">\(\mathbf S\)</span>.</p>
<ul>
<li>The transformed variables <span class="math inline">\(y_{i}\)</span> are the <strong>principal component scores</strong>. <span class="math inline">\(y_1\)</span> is the first score etc.</li>
<li>The eigenvectors/right singular vectors are sometimes refered to as the <strong>loadings</strong> or simply as the <strong>principal components</strong>.</li>
</ul>
</div>
</div>
<div id="geometric-interpretation-1" class="section level3" number="4.1.4">
<h3><span class="header-section-number">4.1.4</span> Geometric interpretation</h3>
<p>We think of PCA as projecting the data points <span class="math inline">\(\mathbf x\)</span> onto a subspace <span class="math inline">\(V\)</span>. The basis vectors for this subspace are the eigenvectors of <span class="math inline">\(\mathbf S\)</span>, which are the same as the right singular vectors of <span class="math inline">\(\mathbf X\)</span> (the loadings):
<span class="math display">\[V=\operatorname{span}\{\mathbf v_1, \ldots, \mathbf v_r\}.\]</span>
The orthogonal projection matrix (see Section <a href="2.3-linalg-innerprod.html#orthogproj">2.3.3.1</a>) for projecting onto <span class="math inline">\(V\)</span> is
<span class="math display">\[\mathbf P_V = \mathbf V\mathbf V^\top\]</span>
as <span class="math inline">\(\mathbf V^\top \mathbf V=\mathbf I\)</span>.<br />
The coordinates of the data points projected onto <span class="math inline">\(V\)</span> (with respect to the basis for <span class="math inline">\(V\)</span>) are the <strong>principal component scores</strong>:</p>
<p><span class="math display">\[\mathbf y_i= \left(\begin{array}{c}y_{i1}\\\vdots\\y_{ir}\end{array}\right)= \mathbf V^\top \mathbf x_i\]</span>
where <span class="math display">\[\mathbf V= \left(\begin{array}{ccc} | &amp;&amp;|\\\mathbf v_1&amp;\ldots&amp; \mathbf v_r\\  | &amp;&amp;|\end{array}\right)\]</span>
is the matrix of right singular vectors from the SVD of <span class="math inline">\(\mathbf X\)</span>.
The transformed variables are</p>
<p><span class="math display">\[\mathbf Y= \left( \begin{array}{ccc}
- &amp;\mathbf y_1^\top&amp;-\\
- &amp;..&amp;-\\
- &amp;\mathbf y_n^\top&amp;-
\end{array}\right ) = \mathbf X\mathbf V.
\]</span>
Substituting the SVD for <span class="math inline">\(\mathbf X= \mathbf U\boldsymbol{\Sigma}\mathbf V^\top\)</span> we can see the transformed variable matrix/principal component scores are
<span class="math display">\[\mathbf Y= \mathbf U\boldsymbol{\Sigma}.\]</span></p>
<p><span class="math inline">\(\mathbf Y\)</span> is a <span class="math inline">\(n \times r\)</span> matrix, and so if <span class="math inline">\(r&lt;p\)</span> we have reduced the dimension of <span class="math inline">\(\mathbf X\)</span>, keeping the most important parts of the data</p>
</div>
<div id="example" class="section level3" number="4.1.5">
<h3><span class="header-section-number">4.1.5</span> Example</h3>
<p>We consider the marks of <span class="math inline">\(n=10\)</span> students who studied G11PRB and G11STA.</p>
<table class="table" style="width: auto !important; margin-left: auto; margin-right: auto;">
<thead>
<tr>
<th style="text-align:right;">
student
</th>
<th style="text-align:right;">
PRB
</th>
<th style="text-align:right;">
STA
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:right;">
1
</td>
<td style="text-align:right;">
81
</td>
<td style="text-align:right;">
75
</td>
</tr>
<tr>
<td style="text-align:right;">
2
</td>
<td style="text-align:right;">
79
</td>
<td style="text-align:right;">
73
</td>
</tr>
<tr>
<td style="text-align:right;">
3
</td>
<td style="text-align:right;">
66
</td>
<td style="text-align:right;">
79
</td>
</tr>
<tr>
<td style="text-align:right;">
4
</td>
<td style="text-align:right;">
53
</td>
<td style="text-align:right;">
55
</td>
</tr>
<tr>
<td style="text-align:right;">
5
</td>
<td style="text-align:right;">
43
</td>
<td style="text-align:right;">
53
</td>
</tr>
<tr>
<td style="text-align:right;">
6
</td>
<td style="text-align:right;">
59
</td>
<td style="text-align:right;">
49
</td>
</tr>
<tr>
<td style="text-align:right;">
7
</td>
<td style="text-align:right;">
62
</td>
<td style="text-align:right;">
72
</td>
</tr>
<tr>
<td style="text-align:right;">
8
</td>
<td style="text-align:right;">
79
</td>
<td style="text-align:right;">
92
</td>
</tr>
<tr>
<td style="text-align:right;">
9
</td>
<td style="text-align:right;">
49
</td>
<td style="text-align:right;">
58
</td>
</tr>
<tr>
<td style="text-align:right;">
10
</td>
<td style="text-align:right;">
55
</td>
<td style="text-align:right;">
56
</td>
</tr>
</tbody>
</table>
<p>These data haven’t been column centered, so let’s do that in R. You can do it using the centering matrix as previously, but here is a different approach:</p>
<div class="sourceCode" id="cb73"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb73-1"><a href="4.1-pca-an-informal-introduction.html#cb73-1" aria-hidden="true" tabindex="-1"></a>secondyr <span class="ot">&lt;-</span> <span class="fu">data.frame</span>(</span>
<span id="cb73-2"><a href="4.1-pca-an-informal-introduction.html#cb73-2" aria-hidden="true" tabindex="-1"></a>  <span class="at">student =</span> <span class="dv">1</span><span class="sc">:</span><span class="dv">10</span>,</span>
<span id="cb73-3"><a href="4.1-pca-an-informal-introduction.html#cb73-3" aria-hidden="true" tabindex="-1"></a><span class="at">PRB=</span><span class="fu">c</span>(<span class="dv">81</span> , <span class="dv">79</span> , <span class="dv">66</span> , <span class="dv">53</span> , <span class="dv">43</span> , <span class="dv">59</span> , <span class="dv">62</span> , <span class="dv">79</span> , <span class="dv">49</span> , <span class="dv">55</span>),</span>
<span id="cb73-4"><a href="4.1-pca-an-informal-introduction.html#cb73-4" aria-hidden="true" tabindex="-1"></a><span class="at">STA =</span><span class="fu">c</span>(<span class="dv">75</span> , <span class="dv">73</span> , <span class="dv">79</span> , <span class="dv">55</span> , <span class="dv">53</span> , <span class="dv">49</span> , <span class="dv">72</span> , <span class="dv">92</span> , <span class="dv">58</span> , <span class="dv">56</span>)</span>
<span id="cb73-5"><a href="4.1-pca-an-informal-introduction.html#cb73-5" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb73-6"><a href="4.1-pca-an-informal-introduction.html#cb73-6" aria-hidden="true" tabindex="-1"></a>xbar <span class="ot">&lt;-</span> <span class="fu">colMeans</span>(secondyr[,<span class="dv">2</span><span class="sc">:</span><span class="dv">3</span>]) <span class="co">#only columns 2 and 3 are data</span></span>
<span id="cb73-7"><a href="4.1-pca-an-informal-introduction.html#cb73-7" aria-hidden="true" tabindex="-1"></a>X <span class="ot">&lt;-</span> <span class="fu">as.matrix</span>(<span class="fu">sweep</span>(secondyr[,<span class="dv">2</span><span class="sc">:</span><span class="dv">3</span>], <span class="dv">2</span>, xbar) ) </span></code></pre></div>
<table class="table" style="width: auto !important; margin-left: auto; margin-right: auto;">
<thead>
<tr>
<th style="text-align:right;">
PRB
</th>
<th style="text-align:right;">
STA
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:right;">
18.4
</td>
<td style="text-align:right;">
8.8
</td>
</tr>
<tr>
<td style="text-align:right;">
16.4
</td>
<td style="text-align:right;">
6.8
</td>
</tr>
<tr>
<td style="text-align:right;">
3.4
</td>
<td style="text-align:right;">
12.8
</td>
</tr>
<tr>
<td style="text-align:right;">
-9.6
</td>
<td style="text-align:right;">
-11.2
</td>
</tr>
<tr>
<td style="text-align:right;">
-19.6
</td>
<td style="text-align:right;">
-13.2
</td>
</tr>
<tr>
<td style="text-align:right;">
-3.6
</td>
<td style="text-align:right;">
-17.2
</td>
</tr>
<tr>
<td style="text-align:right;">
-0.6
</td>
<td style="text-align:right;">
5.8
</td>
</tr>
<tr>
<td style="text-align:right;">
16.4
</td>
<td style="text-align:right;">
25.8
</td>
</tr>
<tr>
<td style="text-align:right;">
-13.6
</td>
<td style="text-align:right;">
-8.2
</td>
</tr>
<tr>
<td style="text-align:right;">
-7.6
</td>
<td style="text-align:right;">
-10.2
</td>
</tr>
</tbody>
</table>
<p>The sample covariance matrix can be computed in two ways:</p>
<div class="sourceCode" id="cb74"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb74-1"><a href="4.1-pca-an-informal-introduction.html#cb74-1" aria-hidden="true" tabindex="-1"></a><span class="dv">1</span><span class="sc">/</span><span class="dv">10</span><span class="sc">*</span> <span class="fu">t</span>(X)<span class="sc">%*%</span>X</span></code></pre></div>
<pre><code>##        PRB    STA
## PRB 162.04 135.38
## STA 135.38 175.36</code></pre>
<div class="sourceCode" id="cb76"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb76-1"><a href="4.1-pca-an-informal-introduction.html#cb76-1" aria-hidden="true" tabindex="-1"></a><span class="fu">cov</span>(X)<span class="sc">*</span><span class="dv">9</span><span class="sc">/</span><span class="dv">10</span> </span></code></pre></div>
<pre><code>##        PRB    STA
## PRB 162.04 135.38
## STA 135.38 175.36</code></pre>
<div class="sourceCode" id="cb78"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb78-1"><a href="4.1-pca-an-informal-introduction.html#cb78-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Remember R uses the unbiased factor 1/(n-1), </span></span>
<span id="cb78-2"><a href="4.1-pca-an-informal-introduction.html#cb78-2" aria-hidden="true" tabindex="-1"></a><span class="co"># so the 9/10=(n-1)/n changes this to 1/n </span></span>
<span id="cb78-3"><a href="4.1-pca-an-informal-introduction.html#cb78-3" aria-hidden="true" tabindex="-1"></a><span class="co"># to match the notes</span></span></code></pre></div>
<p>We can find the singular value decomposition of <span class="math inline">\(\mathbf X\)</span> using R</p>
<div class="sourceCode" id="cb79"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb79-1"><a href="4.1-pca-an-informal-introduction.html#cb79-1" aria-hidden="true" tabindex="-1"></a>(<span class="at">X_svd =</span> <span class="fu">svd</span>(X))</span></code></pre></div>
<pre><code>## $d
## [1] 55.15829 18.20887
## 
## $u
##              [,1]        [,2]
##  [1,] -0.34556317 -0.39864295
##  [2,] -0.29430029 -0.39482564
##  [3,] -0.21057607  0.34946080
##  [4,]  0.26707104 -0.04226416
##  [5,]  0.41833934  0.27975879
##  [6,]  0.27085156 -0.50812066
##  [7,] -0.06865802  0.24349429
##  [8,] -0.54378479  0.32464825
##  [9,]  0.27768146  0.23043980
## [10,]  0.22893893 -0.08394852
## 
## $v
##            [,1]       [,2]
## [1,] -0.6895160 -0.7242705
## [2,] -0.7242705  0.6895160</code></pre>
<p>So we can see that the eigenvectors/right singular vectors/loadings are</p>
<p><span class="math display">\[\mathbf v_1=\begin{pmatrix} -0.69 \\ -0.724 \end{pmatrix},\qquad \mathbf v_2=\begin{pmatrix} -0.724 \\ 0.69 \end{pmatrix}\]</span></p>
<p>Sometimes the new variables have an obvious interpretation. In this case the first PC gives approximately equal weight to PRB and STA and thus represents some form of negative ‘’average’’ mark. Note that the singular vectors are only determined upto multiplication by <span class="math inline">\(\pm 1\)</span>. In this case, R has chosen <span class="math inline">\(\mathbf v_1\)</span> to have negative entries, but we could multiply <span class="math inline">\(\mathbf v_1\)</span> by <span class="math inline">\(-1\)</span> so that the first PC was more like the avearge.
As it is, a student that has a high mark on PRB and STA will have a low negative value for <span class="math inline">\(y_1\)</span>. The second PC, meanwhile, represents a contrast between PRB and STA. For example, a large positive value for <span class="math inline">\(y_2\)</span> implies the student did much better on STA than PRB, and a large negative value implies the opposite.</p>
<p>If we plot the data along with the principal components. The two lines, centred on <span class="math inline">\(\bar{\mathbf x}\)</span>, are in the direction of the principal components/eigenvectors, and their lengths are <span class="math inline">\(2 \sqrt{\lambda_j}\)</span>, <span class="math inline">\(j=1,2\)</span>.
We can see that the first PC is in the direction of greatest variation (shown in red), and that the second PC (shown in green) is orthogonal to the first PC.</p>
<p><img src="04-pca_files/figure-html/unnamed-chunk-8-1.png" width="576" /></p>
<p>We can find the transformed variables by computing either <span class="math inline">\(\mathbf X\mathbf V\)</span> or <span class="math inline">\(\mathbf U\boldsymbol{\Sigma}\)</span></p>
<div class="sourceCode" id="cb81"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb81-1"><a href="4.1-pca-an-informal-introduction.html#cb81-1" aria-hidden="true" tabindex="-1"></a>X <span class="sc">%*%</span> X_svd<span class="sc">$</span>v</span></code></pre></div>
<pre><code>##             [,1]       [,2]
##  [1,] -19.060674 -7.2588361
##  [2,] -16.233101 -7.1893271
##  [3,] -11.615016  6.3632849
##  [4,]  14.731183 -0.7695824
##  [5,]  23.074883  5.0940904
##  [6,]  14.939710 -9.2523011
##  [7,]  -3.787059  4.4337549
##  [8,] -29.994240  5.9114764
##  [9,]  15.316435  4.1960474
## [10,]  12.627880 -1.5286074</code></pre>
<div class="sourceCode" id="cb83"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb83-1"><a href="4.1-pca-an-informal-introduction.html#cb83-1" aria-hidden="true" tabindex="-1"></a>X_svd<span class="sc">$</span>u <span class="sc">%*%</span> <span class="fu">diag</span>(X_svd<span class="sc">$</span>d)</span></code></pre></div>
<pre><code>##             [,1]       [,2]
##  [1,] -19.060674 -7.2588361
##  [2,] -16.233101 -7.1893271
##  [3,] -11.615016  6.3632849
##  [4,]  14.731183 -0.7695824
##  [5,]  23.074883  5.0940904
##  [6,]  14.939710 -9.2523011
##  [7,]  -3.787059  4.4337549
##  [8,] -29.994240  5.9114764
##  [9,]  15.316435  4.1960474
## [10,]  12.627880 -1.5286074</code></pre>
<p>If we plot the PC scores we can see that the variation is now in line with the new coordinate axes:</p>
<p><img src="04-pca_files/figure-html/unnamed-chunk-10-1.png" width="672" /></p>
<p>R also has a built-in function for doing PCA.</p>
<div class="sourceCode" id="cb85"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb85-1"><a href="4.1-pca-an-informal-introduction.html#cb85-1" aria-hidden="true" tabindex="-1"></a>pca <span class="ot">&lt;-</span> <span class="fu">prcomp</span>(secondyr[,<span class="dv">2</span><span class="sc">:</span><span class="dv">3</span>]) <span class="co"># prcomp will automatocally remove the column mean</span></span>
<span id="cb85-2"><a href="4.1-pca-an-informal-introduction.html#cb85-2" aria-hidden="true" tabindex="-1"></a>pca<span class="sc">$</span>rotation <span class="co"># the loadings</span></span></code></pre></div>
<pre><code>##            PC1        PC2
## PRB -0.6895160 -0.7242705
## STA -0.7242705  0.6895160</code></pre>
<div class="sourceCode" id="cb87"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb87-1"><a href="4.1-pca-an-informal-introduction.html#cb87-1" aria-hidden="true" tabindex="-1"></a>pca<span class="sc">$</span>x <span class="co"># the scores</span></span></code></pre></div>
<pre><code>##              PC1        PC2
##  [1,] -19.060674 -7.2588361
##  [2,] -16.233101 -7.1893271
##  [3,] -11.615016  6.3632849
##  [4,]  14.731183 -0.7695824
##  [5,]  23.074883  5.0940904
##  [6,]  14.939710 -9.2523011
##  [7,]  -3.787059  4.4337549
##  [8,] -29.994240  5.9114764
##  [9,]  15.316435  4.1960474
## [10,]  12.627880 -1.5286074</code></pre>
<p>Note that the new variables have sample mean <span class="math inline">\(\bar{\mathbf y}=\boldsymbol 0\)</span>. The sample covariance matrix is a diagonal with entries given by the eigenvalues (see part 4. of Proposition <a href="4.2-pca-a-formal-description-with-proofs.html#prp:pca2">4.2</a>). Note that there is always some numerical error (so quantities are never 0, and instead are just very small numnbers).</p>
<p><span class="math display">\[
\boldsymbol \Lambda= \text{diag}(\lambda_1,\lambda_2) =  \begin{pmatrix} \lambda_1 &amp; 0 \\ 0 &amp; \lambda_2 \end{pmatrix}.
\]</span></p>
<div class="sourceCode" id="cb89"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb89-1"><a href="4.1-pca-an-informal-introduction.html#cb89-1" aria-hidden="true" tabindex="-1"></a><span class="fu">colMeans</span>(pca<span class="sc">$</span>x)</span></code></pre></div>
<pre><code>##           PC1           PC2 
##  2.842171e-15 -9.769963e-16</code></pre>
<div class="sourceCode" id="cb91"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb91-1"><a href="4.1-pca-an-informal-introduction.html#cb91-1" aria-hidden="true" tabindex="-1"></a><span class="fu">cov</span>(pca<span class="sc">$</span>x)<span class="sc">*</span><span class="dv">9</span><span class="sc">/</span><span class="dv">10</span> <span class="co"># to convert to using 1/n as the denominator </span></span></code></pre></div>
<pre><code>##              PC1          PC2
## PC1 3.042437e+02 1.974167e-14
## PC2 1.974167e-14 3.315628e+01</code></pre>
<p>Finally, note that we did the singular value decomposition for <span class="math inline">\(\mathbf X\)</span> above not <span class="math inline">\(\frac{1}{\sqrt{10}}\mathbf X\)</span>, and so we’d need to square and scale the singular values to find the eigenvalues. Let’s check:</p>
<div class="sourceCode" id="cb93"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb93-1"><a href="4.1-pca-an-informal-introduction.html#cb93-1" aria-hidden="true" tabindex="-1"></a>X_svd<span class="sc">$</span>d<span class="sc">^</span><span class="dv">2</span><span class="sc">/</span><span class="dv">10</span> <span class="co"># square and scale the singular values</span></span></code></pre></div>
<pre><code>## [1] 304.24372  33.15628</code></pre>
<div class="sourceCode" id="cb95"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb95-1"><a href="4.1-pca-an-informal-introduction.html#cb95-1" aria-hidden="true" tabindex="-1"></a><span class="fu">eigen</span>(<span class="fu">t</span>(X) <span class="sc">%*%</span> X<span class="sc">/</span><span class="dv">10</span>)<span class="sc">$</span>values  </span></code></pre></div>
<pre><code>## [1] 304.24372  33.15628</code></pre>
<div class="sourceCode" id="cb97"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb97-1"><a href="4.1-pca-an-informal-introduction.html#cb97-1" aria-hidden="true" tabindex="-1"></a><span class="co"># compute the eigenvalues of the covariance matrix</span></span>
<span id="cb97-2"><a href="4.1-pca-an-informal-introduction.html#cb97-2" aria-hidden="true" tabindex="-1"></a><span class="fu">svd</span>(X<span class="sc">/</span><span class="fu">sqrt</span>(<span class="dv">10</span>))<span class="sc">$</span>d<span class="sc">^</span><span class="dv">2</span> </span></code></pre></div>
<pre><code>## [1] 304.24372  33.15628</code></pre>
<div class="sourceCode" id="cb99"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb99-1"><a href="4.1-pca-an-informal-introduction.html#cb99-1" aria-hidden="true" tabindex="-1"></a><span class="co"># compute the singular values of X/sqrt(10) and square</span></span></code></pre></div>
</div>
<div id="example-iris" class="section level3" number="4.1.6">
<h3><span class="header-section-number">4.1.6</span> Example: Iris</h3>
<p>In general when using R to do PCA, we don’t need to compute the SVD and then do the projections, as there is an R command <code>prcomp</code> that will do it all for us. The <code>princomp</code> will also do PCA, but is less stable than <code>prcomp</code>, and it is recommended that you use <code>prcomp</code> in preference.</p>
<p>Let’s do PCA on the iris dataset discussed in Chapter <a href="1-stat-prelim.html#stat-prelim">1</a>. The <code>prcomp</code> returns the square root of the eigenvalues (the standard devaiation of the PC scores), and the PC scores.</p>
<div class="sourceCode" id="cb100"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb100-1"><a href="4.1-pca-an-informal-introduction.html#cb100-1" aria-hidden="true" tabindex="-1"></a>iris.pca <span class="ot">=</span> <span class="fu">prcomp</span>(iris[,<span class="dv">1</span><span class="sc">:</span><span class="dv">4</span>])</span>
<span id="cb100-2"><a href="4.1-pca-an-informal-introduction.html#cb100-2" aria-hidden="true" tabindex="-1"></a>iris.pca<span class="sc">$</span>sdev <span class="co"># the square root of the eigenvalues</span></span></code></pre></div>
<pre><code>## [1] 2.0562689 0.4926162 0.2796596 0.1543862</code></pre>
<div class="sourceCode" id="cb102"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb102-1"><a href="4.1-pca-an-informal-introduction.html#cb102-1" aria-hidden="true" tabindex="-1"></a><span class="fu">head</span>(iris.pca<span class="sc">$</span>x)  <span class="co">#the PC scores</span></span></code></pre></div>
<pre><code>##            PC1        PC2         PC3          PC4
## [1,] -2.684126 -0.3193972  0.02791483  0.002262437
## [2,] -2.714142  0.1770012  0.21046427  0.099026550
## [3,] -2.888991  0.1449494 -0.01790026  0.019968390
## [4,] -2.745343  0.3182990 -0.03155937 -0.075575817
## [5,] -2.728717 -0.3267545 -0.09007924 -0.061258593
## [6,] -2.280860 -0.7413304 -0.16867766 -0.024200858</code></pre>
<p>The PC loadings/eigenvectors can also be accessed, as can the sample mean</p>
<div class="sourceCode" id="cb104"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb104-1"><a href="4.1-pca-an-informal-introduction.html#cb104-1" aria-hidden="true" tabindex="-1"></a>iris.pca<span class="sc">$</span>rotation <span class="co">#the eigenvecstors</span></span></code></pre></div>
<pre><code>##                      PC1         PC2         PC3        PC4
## Sepal.Length  0.36138659 -0.65658877  0.58202985  0.3154872
## Sepal.Width  -0.08452251 -0.73016143 -0.59791083 -0.3197231
## Petal.Length  0.85667061  0.17337266 -0.07623608 -0.4798390
## Petal.Width   0.35828920  0.07548102 -0.54583143  0.7536574</code></pre>
<div class="sourceCode" id="cb106"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb106-1"><a href="4.1-pca-an-informal-introduction.html#cb106-1" aria-hidden="true" tabindex="-1"></a>iris.pca<span class="sc">$</span>center <span class="co"># the sample mean of the data</span></span></code></pre></div>
<pre><code>## Sepal.Length  Sepal.Width Petal.Length  Petal.Width 
##     5.843333     3.057333     3.758000     1.199333</code></pre>
<p>A scree plot can be obtained simply by using the <code>plot</code> command. The summary command also gives useful information about the importance of each PC.</p>
<div class="sourceCode" id="cb108"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb108-1"><a href="4.1-pca-an-informal-introduction.html#cb108-1" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(iris.pca)</span></code></pre></div>
<p><img src="04-pca_files/figure-html/unnamed-chunk-16-1.png" width="672" /></p>
<div class="sourceCode" id="cb109"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb109-1"><a href="4.1-pca-an-informal-introduction.html#cb109-1" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(iris.pca)</span></code></pre></div>
<pre><code>## Importance of components:
##                           PC1     PC2    PC3     PC4
## Standard deviation     2.0563 0.49262 0.2797 0.15439
## Proportion of Variance 0.9246 0.05307 0.0171 0.00521
## Cumulative Proportion  0.9246 0.97769 0.9948 1.00000</code></pre>
<p>To plot the PC scores, you can either manually create a plot or use the <code>ggfortify</code> package. For example, here is a plot of the first two PC scores coloured according to the species of iris.</p>
<div class="sourceCode" id="cb111"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb111-1"><a href="4.1-pca-an-informal-introduction.html#cb111-1" aria-hidden="true" tabindex="-1"></a>iris<span class="sc">$</span>PC1<span class="ot">=</span>iris.pca<span class="sc">$</span>x[,<span class="dv">1</span>]</span>
<span id="cb111-2"><a href="4.1-pca-an-informal-introduction.html#cb111-2" aria-hidden="true" tabindex="-1"></a>iris<span class="sc">$</span>PC2<span class="ot">=</span>iris.pca<span class="sc">$</span>x[,<span class="dv">2</span>]</span>
<span id="cb111-3"><a href="4.1-pca-an-informal-introduction.html#cb111-3" aria-hidden="true" tabindex="-1"></a><span class="fu">qplot</span>(PC1, PC2, <span class="at">colour=</span>Species, <span class="at">data=</span>iris)</span></code></pre></div>
<p><img src="04-pca_files/figure-html/unnamed-chunk-17-1.png" width="672" /></p>
<p>The <code>ggfortify</code> package provides a nice wrapper for some of this functionality.</p>
<div class="sourceCode" id="cb112"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb112-1"><a href="4.1-pca-an-informal-introduction.html#cb112-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(ggfortify)</span>
<span id="cb112-2"><a href="4.1-pca-an-informal-introduction.html#cb112-2" aria-hidden="true" tabindex="-1"></a><span class="fu">autoplot</span>(iris.pca, <span class="at">data =</span> iris, <span class="at">colour =</span> <span class="st">&#39;Species&#39;</span>, <span class="at">scale=</span><span class="cn">FALSE</span>)</span></code></pre></div>
<p><img src="04-pca_files/figure-html/unnamed-chunk-18-1.png" width="672" /></p>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="4-pca.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="4.2-pca-a-formal-description-with-proofs.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["MultivariateStatistics.pdf"],
"search": {
"engine": "fuse",
"options": null
},
"toc": {
"collapse": "section"
},
"pandoc_args": "--top-level-division=[chapter|part]"
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
