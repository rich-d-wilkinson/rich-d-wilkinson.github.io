<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>7.4 Inference based on the MVN | Multivariate Statistics</title>
  <meta name="description" content="The lecture notes for MATH3030/4068: Multivariate Analysis / Applied Multivariate Statistics" />
  <meta name="generator" content="bookdown 0.20.6 and GitBook 2.6.7" />

  <meta property="og:title" content="7.4 Inference based on the MVN | Multivariate Statistics" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="The lecture notes for MATH3030/4068: Multivariate Analysis / Applied Multivariate Statistics" />
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="7.4 Inference based on the MVN | Multivariate Statistics" />
  
  <meta name="twitter:description" content="The lecture notes for MATH3030/4068: Multivariate Analysis / Applied Multivariate Statistics" />
  

<meta name="author" content="Prof. Richard Wilkinson" />



  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="7-3-hotellings-t2-distribution.html"/>

<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />









<script src="libs/kePrint-0.0.1/kePrint.js"></script>
<link href="libs/lightable-0.0.1/lightable.css" rel="stylesheet" />


<style type="text/css">
a.sourceLine { display: inline-block; line-height: 1.25; }
a.sourceLine { pointer-events: none; color: inherit; text-decoration: inherit; }
a.sourceLine:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode { white-space: pre; position: relative; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
code.sourceCode { white-space: pre-wrap; }
a.sourceLine { text-indent: -1em; padding-left: 1em; }
}
pre.numberSource a.sourceLine
  { position: relative; left: -4em; }
pre.numberSource a.sourceLine::before
  { content: attr(data-line-number);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; pointer-events: all; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {  }
@media screen {
a.sourceLine::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Applied multivariate statistics</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Introduction</a></li>
<li class="chapter" data-level="" data-path="part-i-prerequisites.html"><a href="part-i-prerequisites.html"><i class="fa fa-check"></i>PART I: Prerequisites</a></li>
<li class="chapter" data-level="1" data-path="1-stat-prelim.html"><a href="1-stat-prelim.html"><i class="fa fa-check"></i><b>1</b> Statistical Preliminaries</a><ul>
<li class="chapter" data-level="1.1" data-path="1-1-notation.html"><a href="1-1-notation.html"><i class="fa fa-check"></i><b>1.1</b> Notation</a><ul>
<li class="chapter" data-level="1.1.1" data-path="1-1-notation.html"><a href="1-1-notation.html#example-datasets"><i class="fa fa-check"></i><b>1.1.1</b> Example datasets</a></li>
<li class="chapter" data-level="1.1.2" data-path="1-1-notation.html"><a href="1-1-notation.html#aims-of-multivariate-data-analysis"><i class="fa fa-check"></i><b>1.1.2</b> Aims of multivariate data analysis</a></li>
</ul></li>
<li class="chapter" data-level="1.2" data-path="1-2-exploratory-data-analysis-eda.html"><a href="1-2-exploratory-data-analysis-eda.html"><i class="fa fa-check"></i><b>1.2</b> Exploratory data analysis (EDA)</a><ul>
<li class="chapter" data-level="1.2.1" data-path="1-2-exploratory-data-analysis-eda.html"><a href="1-2-exploratory-data-analysis-eda.html#data-visualization"><i class="fa fa-check"></i><b>1.2.1</b> Data visualization</a></li>
<li class="chapter" data-level="1.2.2" data-path="1-2-exploratory-data-analysis-eda.html"><a href="1-2-exploratory-data-analysis-eda.html#summary-statistics"><i class="fa fa-check"></i><b>1.2.2</b> Summary statistics</a></li>
</ul></li>
<li class="chapter" data-level="1.3" data-path="1-3-random-vectors-and-matrices.html"><a href="1-3-random-vectors-and-matrices.html"><i class="fa fa-check"></i><b>1.3</b> Random vectors and matrices</a><ul>
<li class="chapter" data-level="1.3.1" data-path="1-3-random-vectors-and-matrices.html"><a href="1-3-random-vectors-and-matrices.html#estimators"><i class="fa fa-check"></i><b>1.3.1</b> Estimators</a></li>
</ul></li>
<li class="chapter" data-level="1.4" data-path="1-4-computer-tasks.html"><a href="1-4-computer-tasks.html"><i class="fa fa-check"></i><b>1.4</b> Computer tasks</a></li>
<li class="chapter" data-level="1.5" data-path="1-5-exercises.html"><a href="1-5-exercises.html"><i class="fa fa-check"></i><b>1.5</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="2-linalg-prelim.html"><a href="2-linalg-prelim.html"><i class="fa fa-check"></i><b>2</b> Review of linear algebra</a><ul>
<li class="chapter" data-level="2.1" data-path="2-1-linalg-basics.html"><a href="2-1-linalg-basics.html"><i class="fa fa-check"></i><b>2.1</b> Basics</a><ul>
<li class="chapter" data-level="2.1.1" data-path="2-1-linalg-basics.html"><a href="2-1-linalg-basics.html#notation-1"><i class="fa fa-check"></i><b>2.1.1</b> Notation</a></li>
<li class="chapter" data-level="2.1.2" data-path="2-1-linalg-basics.html"><a href="2-1-linalg-basics.html#elementary-matrix-operations"><i class="fa fa-check"></i><b>2.1.2</b> Elementary matrix operations</a></li>
<li class="chapter" data-level="2.1.3" data-path="2-1-linalg-basics.html"><a href="2-1-linalg-basics.html#special-matrices"><i class="fa fa-check"></i><b>2.1.3</b> Special matrices</a></li>
<li class="chapter" data-level="2.1.4" data-path="2-1-linalg-basics.html"><a href="2-1-linalg-basics.html#vectordiff"><i class="fa fa-check"></i><b>2.1.4</b> Vector Differentiation</a></li>
</ul></li>
<li class="chapter" data-level="2.2" data-path="2-2-linalg-vecspaces.html"><a href="2-2-linalg-vecspaces.html"><i class="fa fa-check"></i><b>2.2</b> Vector spaces</a><ul>
<li class="chapter" data-level="2.2.1" data-path="2-2-linalg-vecspaces.html"><a href="2-2-linalg-vecspaces.html#linear-independence"><i class="fa fa-check"></i><b>2.2.1</b> Linear independence</a></li>
<li class="chapter" data-level="2.2.2" data-path="2-2-linalg-vecspaces.html"><a href="2-2-linalg-vecspaces.html#colsspace"><i class="fa fa-check"></i><b>2.2.2</b> Row and column spaces</a></li>
<li class="chapter" data-level="2.2.3" data-path="2-2-linalg-vecspaces.html"><a href="2-2-linalg-vecspaces.html#linear-transformations"><i class="fa fa-check"></i><b>2.2.3</b> Linear transformations</a></li>
</ul></li>
<li class="chapter" data-level="2.3" data-path="2-3-linalg-innerprod.html"><a href="2-3-linalg-innerprod.html"><i class="fa fa-check"></i><b>2.3</b> Inner product spaces</a><ul>
<li class="chapter" data-level="2.3.1" data-path="2-3-linalg-innerprod.html"><a href="2-3-linalg-innerprod.html#normed"><i class="fa fa-check"></i><b>2.3.1</b> Distances, and angles</a></li>
<li class="chapter" data-level="2.3.2" data-path="2-3-linalg-innerprod.html"><a href="2-3-linalg-innerprod.html#orthogonal-matrices"><i class="fa fa-check"></i><b>2.3.2</b> Orthogonal matrices</a></li>
<li class="chapter" data-level="2.3.3" data-path="2-3-linalg-innerprod.html"><a href="2-3-linalg-innerprod.html#projection-matrix"><i class="fa fa-check"></i><b>2.3.3</b> Projections</a></li>
</ul></li>
<li class="chapter" data-level="2.4" data-path="2-4-centering-matrix.html"><a href="2-4-centering-matrix.html"><i class="fa fa-check"></i><b>2.4</b> The Centering Matrix</a></li>
<li class="chapter" data-level="2.5" data-path="2-5-tasks-ch2.html"><a href="2-5-tasks-ch2.html"><i class="fa fa-check"></i><b>2.5</b> Computer tasks</a></li>
<li class="chapter" data-level="2.6" data-path="2-6-exercises-ch2.html"><a href="2-6-exercises-ch2.html"><i class="fa fa-check"></i><b>2.6</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="3-linalg-decomp.html"><a href="3-linalg-decomp.html"><i class="fa fa-check"></i><b>3</b> Matrix decompositions</a><ul>
<li class="chapter" data-level="3.1" data-path="3-1-matrix-matrix.html"><a href="3-1-matrix-matrix.html"><i class="fa fa-check"></i><b>3.1</b> Matrix-matrix products</a></li>
<li class="chapter" data-level="3.2" data-path="3-2-spectraleigen-decomposition.html"><a href="3-2-spectraleigen-decomposition.html"><i class="fa fa-check"></i><b>3.2</b> Spectral/eigen decomposition</a><ul>
<li class="chapter" data-level="3.2.1" data-path="3-2-spectraleigen-decomposition.html"><a href="3-2-spectraleigen-decomposition.html#eigenvalues-and-eigenvectors"><i class="fa fa-check"></i><b>3.2.1</b> Eigenvalues and eigenvectors</a></li>
<li class="chapter" data-level="3.2.2" data-path="3-2-spectraleigen-decomposition.html"><a href="3-2-spectraleigen-decomposition.html#spectral-decomposition"><i class="fa fa-check"></i><b>3.2.2</b> Spectral decomposition</a></li>
<li class="chapter" data-level="3.2.3" data-path="3-2-spectraleigen-decomposition.html"><a href="3-2-spectraleigen-decomposition.html#matrixroots"><i class="fa fa-check"></i><b>3.2.3</b> Matrix square roots</a></li>
</ul></li>
<li class="chapter" data-level="3.3" data-path="3-3-linalg-SVD.html"><a href="3-3-linalg-SVD.html"><i class="fa fa-check"></i><b>3.3</b> Singular Value Decomposition (SVD)</a><ul>
<li class="chapter" data-level="3.3.1" data-path="3-3-linalg-SVD.html"><a href="3-3-linalg-SVD.html#examples"><i class="fa fa-check"></i><b>3.3.1</b> Examples</a></li>
</ul></li>
<li class="chapter" data-level="3.4" data-path="3-4-svdopt.html"><a href="3-4-svdopt.html"><i class="fa fa-check"></i><b>3.4</b> SVD optimization results</a></li>
<li class="chapter" data-level="3.5" data-path="3-5-low-rank-approximation.html"><a href="3-5-low-rank-approximation.html"><i class="fa fa-check"></i><b>3.5</b> Low-rank approximation</a><ul>
<li class="chapter" data-level="3.5.1" data-path="3-5-low-rank-approximation.html"><a href="3-5-low-rank-approximation.html#matrix-norms"><i class="fa fa-check"></i><b>3.5.1</b> Matrix norms</a></li>
<li class="chapter" data-level="3.5.2" data-path="3-5-low-rank-approximation.html"><a href="3-5-low-rank-approximation.html#eckart-young-mirsky-theorem"><i class="fa fa-check"></i><b>3.5.2</b> Eckart-Young-Mirsky Theorem</a></li>
<li class="chapter" data-level="3.5.3" data-path="3-5-low-rank-approximation.html"><a href="3-5-low-rank-approximation.html#example-image-compression"><i class="fa fa-check"></i><b>3.5.3</b> Example: image compression</a></li>
</ul></li>
<li class="chapter" data-level="3.6" data-path="3-6-tasks-ch3.html"><a href="3-6-tasks-ch3.html"><i class="fa fa-check"></i><b>3.6</b> Computer tasks</a></li>
<li class="chapter" data-level="3.7" data-path="3-7-exercises-ch3.html"><a href="3-7-exercises-ch3.html"><i class="fa fa-check"></i><b>3.7</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="part-ii-dimension-reduction-methods.html"><a href="part-ii-dimension-reduction-methods.html"><i class="fa fa-check"></i>PART II: Dimension reduction methods</a><ul>
<li class="chapter" data-level="" data-path="part-ii-dimension-reduction-methods.html"><a href="part-ii-dimension-reduction-methods.html#a-warning"><i class="fa fa-check"></i>A warning</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="4-pca.html"><a href="4-pca.html"><i class="fa fa-check"></i><b>4</b> Principal component analysis (PCA)</a><ul>
<li class="chapter" data-level="4.1" data-path="4-1-pca-an-informal-introduction.html"><a href="4-1-pca-an-informal-introduction.html"><i class="fa fa-check"></i><b>4.1</b> PCA: an informal introduction</a><ul>
<li class="chapter" data-level="4.1.1" data-path="4-1-pca-an-informal-introduction.html"><a href="4-1-pca-an-informal-introduction.html#notation-recap"><i class="fa fa-check"></i><b>4.1.1</b> Notation recap</a></li>
<li class="chapter" data-level="4.1.2" data-path="4-1-pca-an-informal-introduction.html"><a href="4-1-pca-an-informal-introduction.html#first-principal-component"><i class="fa fa-check"></i><b>4.1.2</b> First principal component</a></li>
<li class="chapter" data-level="4.1.3" data-path="4-1-pca-an-informal-introduction.html"><a href="4-1-pca-an-informal-introduction.html#second-principal-component"><i class="fa fa-check"></i><b>4.1.3</b> Second principal component</a></li>
<li class="chapter" data-level="4.1.4" data-path="4-1-pca-an-informal-introduction.html"><a href="4-1-pca-an-informal-introduction.html#geometric-interpretation-1"><i class="fa fa-check"></i><b>4.1.4</b> Geometric interpretation</a></li>
<li class="chapter" data-level="4.1.5" data-path="4-1-pca-an-informal-introduction.html"><a href="4-1-pca-an-informal-introduction.html#example"><i class="fa fa-check"></i><b>4.1.5</b> Example</a></li>
<li class="chapter" data-level="4.1.6" data-path="4-1-pca-an-informal-introduction.html"><a href="4-1-pca-an-informal-introduction.html#example-iris"><i class="fa fa-check"></i><b>4.1.6</b> Example: Iris</a></li>
</ul></li>
<li class="chapter" data-level="4.2" data-path="4-2-pca-a-formal-description-with-proofs.html"><a href="4-2-pca-a-formal-description-with-proofs.html"><i class="fa fa-check"></i><b>4.2</b> PCA: a formal description with proofs</a><ul>
<li class="chapter" data-level="4.2.1" data-path="4-2-pca-a-formal-description-with-proofs.html"><a href="4-2-pca-a-formal-description-with-proofs.html#properties-of-principal-components"><i class="fa fa-check"></i><b>4.2.1</b> Properties of principal components</a></li>
<li class="chapter" data-level="4.2.2" data-path="4-2-pca-a-formal-description-with-proofs.html"><a href="4-2-pca-a-formal-description-with-proofs.html#example-football"><i class="fa fa-check"></i><b>4.2.2</b> Example: Football</a></li>
<li class="chapter" data-level="4.2.3" data-path="4-2-pca-a-formal-description-with-proofs.html"><a href="4-2-pca-a-formal-description-with-proofs.html#pcawithR"><i class="fa fa-check"></i><b>4.2.3</b> PCA based on <span class="math inline">\(\boldsymbol R\)</span> versus PCA based on <span class="math inline">\(\boldsymbol S\)</span></a></li>
<li class="chapter" data-level="4.2.4" data-path="4-2-pca-a-formal-description-with-proofs.html"><a href="4-2-pca-a-formal-description-with-proofs.html#population-pca"><i class="fa fa-check"></i><b>4.2.4</b> Population PCA</a></li>
<li class="chapter" data-level="4.2.5" data-path="4-2-pca-a-formal-description-with-proofs.html"><a href="4-2-pca-a-formal-description-with-proofs.html#pca-under-transformations-of-variables"><i class="fa fa-check"></i><b>4.2.5</b> PCA under transformations of variables</a></li>
</ul></li>
<li class="chapter" data-level="4.3" data-path="4-3-an-alternative-view-of-pca.html"><a href="4-3-an-alternative-view-of-pca.html"><i class="fa fa-check"></i><b>4.3</b> An alternative view of PCA</a><ul>
<li class="chapter" data-level="4.3.1" data-path="4-3-an-alternative-view-of-pca.html"><a href="4-3-an-alternative-view-of-pca.html#example-mnist-handwritten-digits"><i class="fa fa-check"></i><b>4.3.1</b> Example: MNIST handwritten digits</a></li>
</ul></li>
<li class="chapter" data-level="4.4" data-path="4-4-pca-comptask.html"><a href="4-4-pca-comptask.html"><i class="fa fa-check"></i><b>4.4</b> Computer tasks</a></li>
<li class="chapter" data-level="4.5" data-path="4-5-exercises-1.html"><a href="4-5-exercises-1.html"><i class="fa fa-check"></i><b>4.5</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="5-cca.html"><a href="5-cca.html"><i class="fa fa-check"></i><b>5</b> Canonical Correlation Analysis</a><ul>
<li class="chapter" data-level="5.1" data-path="5-1-cca1.html"><a href="5-1-cca1.html"><i class="fa fa-check"></i><b>5.1</b> The first pair of canonical variables</a><ul>
<li class="chapter" data-level="5.1.1" data-path="5-1-cca1.html"><a href="5-1-cca1.html#example-premier-league-football"><i class="fa fa-check"></i><b>5.1.1</b> Example: Premier league football</a></li>
</ul></li>
<li class="chapter" data-level="5.2" data-path="5-2-the-full-set-of-canonical-correlations.html"><a href="5-2-the-full-set-of-canonical-correlations.html"><i class="fa fa-check"></i><b>5.2</b> The full set of canonical correlations</a></li>
<li class="chapter" data-level="5.3" data-path="5-3-properties.html"><a href="5-3-properties.html"><i class="fa fa-check"></i><b>5.3</b> Properties</a><ul>
<li class="chapter" data-level="5.3.1" data-path="5-3-properties.html"><a href="5-3-properties.html#connection-with-linear-regression-when-q1"><i class="fa fa-check"></i><b>5.3.1</b> Connection with linear regression when <span class="math inline">\(q=1\)</span></a></li>
<li class="chapter" data-level="5.3.2" data-path="5-3-properties.html"><a href="5-3-properties.html#invarianceequivariance-properties-of-cca"><i class="fa fa-check"></i><b>5.3.2</b> Invariance/equivariance properties of CCA</a></li>
</ul></li>
<li class="chapter" data-level="5.4" data-path="5-4-exercises-2.html"><a href="5-4-exercises-2.html"><i class="fa fa-check"></i><b>5.4</b> Exercises</a></li>
<li class="chapter" data-level="5.5" data-path="5-5-computer-tasks-1.html"><a href="5-5-computer-tasks-1.html"><i class="fa fa-check"></i><b>5.5</b> Computer tasks</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="6-mds.html"><a href="6-mds.html"><i class="fa fa-check"></i><b>6</b> Multidimensional Scaling</a><ul>
<li class="chapter" data-level="6.1" data-path="6-1-classical-multidimensional-scaling.html"><a href="6-1-classical-multidimensional-scaling.html"><i class="fa fa-check"></i><b>6.1</b> Classical Multidimensional Scaling</a><ul>
<li class="chapter" data-level="6.1.1" data-path="6-1-classical-multidimensional-scaling.html"><a href="6-1-classical-multidimensional-scaling.html#example-1"><i class="fa fa-check"></i><b>6.1.1</b> Example</a></li>
</ul></li>
<li class="chapter" data-level="6.2" data-path="6-2-principal-coordinates.html"><a href="6-2-principal-coordinates.html"><i class="fa fa-check"></i><b>6.2</b> Principal Coordinates</a></li>
<li class="chapter" data-level="6.3" data-path="6-3-similarity-measures.html"><a href="6-3-similarity-measures.html"><i class="fa fa-check"></i><b>6.3</b> Similarity measures</a></li>
<li class="chapter" data-level="6.4" data-path="6-4-exercises-3.html"><a href="6-4-exercises-3.html"><i class="fa fa-check"></i><b>6.4</b> Exercises</a></li>
<li class="chapter" data-level="6.5" data-path="6-5-computer-tasks-2.html"><a href="6-5-computer-tasks-2.html"><i class="fa fa-check"></i><b>6.5</b> Computer Tasks</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="part-iii-inference-using-the-multivariate-normal-distribution-mvn.html"><a href="part-iii-inference-using-the-multivariate-normal-distribution-mvn.html"><i class="fa fa-check"></i>Part III: Inference using the Multivariate Normal Distribution (MVN)</a></li>
<li class="chapter" data-level="7" data-path="7-multinormal.html"><a href="7-multinormal.html"><i class="fa fa-check"></i><b>7</b> The Multivariate Normal Distribution</a><ul>
<li class="chapter" data-level="7.1" data-path="7-1-definition-and-properties-of-the-mvn.html"><a href="7-1-definition-and-properties-of-the-mvn.html"><i class="fa fa-check"></i><b>7.1</b> Definition and Properties of the MVN</a><ul>
<li class="chapter" data-level="7.1.1" data-path="7-1-definition-and-properties-of-the-mvn.html"><a href="7-1-definition-and-properties-of-the-mvn.html#transformations"><i class="fa fa-check"></i><b>7.1.1</b> Transformations</a></li>
<li class="chapter" data-level="7.1.2" data-path="7-1-definition-and-properties-of-the-mvn.html"><a href="7-1-definition-and-properties-of-the-mvn.html#moment-generating-functions"><i class="fa fa-check"></i><b>7.1.2</b> Moment Generating Functions</a></li>
<li class="chapter" data-level="7.1.3" data-path="7-1-definition-and-properties-of-the-mvn.html"><a href="7-1-definition-and-properties-of-the-mvn.html#sampling-results-for-the-mvn"><i class="fa fa-check"></i><b>7.1.3</b> Sampling results for the MVN</a></li>
</ul></li>
<li class="chapter" data-level="7.2" data-path="7-2-the-wishart-distribution.html"><a href="7-2-the-wishart-distribution.html"><i class="fa fa-check"></i><b>7.2</b> The Wishart distribution</a><ul>
<li class="chapter" data-level="7.2.1" data-path="7-2-the-wishart-distribution.html"><a href="7-2-the-wishart-distribution.html#properties-1"><i class="fa fa-check"></i><b>7.2.1</b> Properties</a></li>
<li class="chapter" data-level="7.2.2" data-path="7-2-the-wishart-distribution.html"><a href="7-2-the-wishart-distribution.html#cochrans-theorem"><i class="fa fa-check"></i><b>7.2.2</b> Cochran’s theorem</a></li>
</ul></li>
<li class="chapter" data-level="7.3" data-path="7-3-hotellings-t2-distribution.html"><a href="7-3-hotellings-t2-distribution.html"><i class="fa fa-check"></i><b>7.3</b> Hotelling’s <span class="math inline">\(T^2\)</span> distribution</a></li>
<li class="chapter" data-level="7.4" data-path="7-4-inference-based-on-the-mvn.html"><a href="7-4-inference-based-on-the-mvn.html"><i class="fa fa-check"></i><b>7.4</b> Inference based on the MVN</a><ul>
<li class="chapter" data-level="7.4.1" data-path="7-4-inference-based-on-the-mvn.html"><a href="7-4-inference-based-on-the-mvn.html#onesampleSigma"><i class="fa fa-check"></i><b>7.4.1</b> <span class="math inline">\(\boldsymbol \Sigma\)</span> known</a></li>
<li class="chapter" data-level="7.4.2" data-path="7-4-inference-based-on-the-mvn.html"><a href="7-4-inference-based-on-the-mvn.html#onesample"><i class="fa fa-check"></i><b>7.4.2</b> <span class="math inline">\(\boldsymbol \Sigma\)</span> unknown: 1 sample</a></li>
<li class="chapter" data-level="7.4.3" data-path="7-4-inference-based-on-the-mvn.html"><a href="7-4-inference-based-on-the-mvn.html#boldsymbol-sigma-unknown-2-samples"><i class="fa fa-check"></i><b>7.4.3</b> <span class="math inline">\(\boldsymbol \Sigma\)</span> unknown: 2 samples</a></li>
</ul></li>
</ul></li>
<li class="divider"></li>
<li> <a href="https://rich-d-wilkinson.github.io/teaching.html" target="blank">University of Nottingham</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Multivariate Statistics</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="inference-based-on-the-mvn" class="section level2">
<h2><span class="header-section-number">7.4</span> Inference based on the MVN</h2>
<p>In univariate statistical analysis, you will have seen how to do hypothesis testing for the mean of a population.</p>
<ol style="list-style-type: decimal">
<li><p>In the case where you have a single sample <span class="math inline">\(x_1, \ldots, x_n\)</span> which come from a population with known variance <span class="math inline">\(\sigma^2\)</span> we use a z-test when testing hypotheses such as
<span class="math inline">\(H_0: \mu=\mu_1\)</span>.</p></li>
<li><p>When the variance of the population, <span class="math inline">\(\sigma^2\)</span>, is unknown, then we have to use a t-test.</p></li>
<li><p>When we have two samples, <span class="math inline">\(x_1, \ldots, x_n\)</span> and <span class="math inline">\(y_1, \ldots, y_m\)</span>, we use either a paired or an unpaired t-test.</p></li>
</ol>
<p>We now develop analogous results in the multivariate case. The role of the Student <span class="math inline">\(t\)</span> distribution will be played by Hotelling’s <span class="math inline">\(T^2\)</span>, and the role of the <span class="math inline">\(\chi^2\)</span> is played by the Wishart distribution.</p>
<p>The next three subsections deal with the multivariate equivalent of the three situations listed above. Before we do, lets quickly recap how hypothesis testing works:</p>
<div id="recap-of-hypothesis-testing-framework" class="section level4 unnumbered">
<h4>Recap of hypothesis testing framework</h4>
<p>Suppose that we have a null hypothesis <span class="math inline">\(H_0\)</span> represented by a completely specified model and that we wish to test this hypothesis using data <span class="math inline">\(x_1, \ldots, x_n\)</span>. We proceed as follows</p>
<ol style="list-style-type: decimal">
<li><p>Assume <span class="math inline">\(H_0\)</span> is true.</p></li>
<li><p>Find a test statistic <span class="math inline">\(T(x_1, \ldots, x_n)\)</span> for which large values indicate departure from <span class="math inline">\(H_0\)</span>.</p></li>
<li><p>Calculate the theoretical sampling distribution of <span class="math inline">\(T\)</span> under <span class="math inline">\(H_0\)</span>.</p></li>
<li>The observed value <span class="math inline">\(T_{obs}=T(x_1, \ldots, x_n)\)</span> of the test statistic is compared with the distribution of <span class="math inline">\(T\)</span> under <span class="math inline">\(H_0\)</span>. Either
<ul>
<li>(Neyman-Pearson) reject <span class="math inline">\(H_0\)</span> if <span class="math inline">\(T_{obs}&gt;c\)</span>. Here <span class="math inline">\(c\)</span> is chosen so that <span class="math inline">\(\mathbb{P}(T\geq c| H_0)=\alpha\)</span> where <span class="math inline">\(\alpha\)</span> is the <strong>size</strong> of the test, i.e., <span class="math inline">\(\mathbb{P}(\mbox{reject } H_0 | H_0 \mbox{ true})=\alpha\)</span>.</li>
<li>(Fisherian) compute the p-value <span class="math inline">\(p=\mathbb{P}(T\geq T_{obs}|H_0)\)</span> and report it. This represents the strength of evidence against <span class="math inline">\(H_0\)</span>.</li>
</ul></li>
</ol>
</div>
<div id="onesampleSigma" class="section level3">
<h3><span class="header-section-number">7.4.1</span> <span class="math inline">\(\boldsymbol \Sigma\)</span> known</h3>
<p>Let <span class="math inline">\(\boldsymbol x_1,\ldots,\boldsymbol x_n\)</span> be a random sample from <span class="math inline">\(N_p(\boldsymbol \mu,\boldsymbol \Sigma)\)</span> where
<span class="math inline">\(\boldsymbol \mu= (\mu_1,\ldots,\mu_p)^\top\)</span>. Suppose we wish to conduct the following hypothesis test <span class="math display">\[H_0: \boldsymbol \mu= \boldsymbol a\mbox{ vs } H_1: \boldsymbol \mu\neq \boldsymbol a\]</span> where <span class="math inline">\(\boldsymbol a\)</span> is fixed and pre-specified.
Let’s first <strong>assume that <span class="math inline">\(\boldsymbol \Sigma\)</span> is known</strong>. This will result in the multivariate analogue of the z-test.</p>
<p>One approach would be to conduct <span class="math inline">\(p\)</span> separate univariate z-tests tests with null hypotheses
<span class="math display">\[H_0: \mu_i = a_i \qquad \text{vs.} \qquad H_1: \mu_i \neq a_i, \quad \mbox{for } i=1,\ldots,p.\]</span><br />
However, this ignores possible correlations between the variables - see the example for a situation in which this can make a difference.</p>
<p>A better approach is to conduct a single (multivariate) hypothesis test using the test statistic
<span class="math display">\[
\zeta^2 = n(\bar{\boldsymbol x}-\boldsymbol a)^\top \boldsymbol \Sigma^{-1} (\bar{\boldsymbol x}-\boldsymbol a).
\]</span></p>
<p>We need to compute the distribution of <span class="math inline">\(\zeta^2\)</span> when <span class="math inline">\(H_0\)</span> is true. Note that</p>
<p><span class="math display">\[
\zeta^2 = (n^{1/2}\bar{\boldsymbol x}-n^{1/2}\boldsymbol \mu)^\top \boldsymbol \Sigma^{-1} (n^{1/2}\bar{\boldsymbol x}-n^{1/2}\boldsymbol \mu).
\]</span>
Recall that <span class="math inline">\(\bar{\boldsymbol x} \sim N_p(\boldsymbol \mu, \frac{1}{n} \boldsymbol \Sigma)\)</span>, and that therefore <span class="math inline">\(n^{1/2} \bar{\boldsymbol x} \sim N_p(n^{1/2} \boldsymbol \mu, \boldsymbol \Sigma)\)</span>. Applying Proposition<br />
Proposition <a href="7-1-definition-and-properties-of-the-mvn.html#prp:six5">7.5</a> we thus see that
<span class="math display">\[\zeta^2\sim \chi_p^2\]</span>
when <span class="math inline">\(H_0\)</span> is true.</p>
<p>Thus to conduct the hypothesis test, we compute <span class="math inline">\(\zeta^2\)</span> for the observed data, and if the value is large compared to a <span class="math inline">\(\chi^2_p\)</span> distribution, we reject <span class="math inline">\(H_0\)</span>.</p>
<ul>
<li><p>The Neyman-Pearson approach is to define a critical region and reject <span class="math inline">\(H_0\)</span> if <span class="math inline">\(\zeta^2 &gt; \chi^2_{p,\alpha}\)</span>, where <span class="math inline">\(\chi^2_{p,\alpha}\)</span> is the upper <span class="math inline">\(\alpha\)</span> quantile of the <span class="math inline">\(\chi^2_p\)</span> distribution, i.e., <span class="math inline">\(\mathbb{P}(\chi^2_p &gt; \chi^2_{p,\alpha})=\alpha\)</span>.</p></li>
<li><p>The Fisherian approach is to state the result as a <span class="math inline">\(p\)</span>-value where <span class="math inline">\(p = \mathbb{P}(\chi^2_p &gt; \zeta_{\text{obs}}^2)\)</span>, and
<span class="math inline">\(\zeta_{\text{obs}}^2\)</span> is the observed value of the statistic <span class="math inline">\(\zeta^2\)</span>.</p></li>
</ul>
<p>The multivariate equivalent of a confidence interval is a <strong>confidence region</strong> and the <span class="math inline">\(100(1-\alpha)\)</span>% confidence region for <span class="math inline">\(\boldsymbol \mu\)</span> is <span class="math inline">\(\{ \boldsymbol a: \zeta^2 \leq \chi^2_{p,\alpha} \}\)</span>.
This confidence region will be the interior of an ellipse or ellipsoid.</p>
<div id="example-2" class="section level4 unnumbered">
<h4>Example</h4>
<p>Consider again the exam marks for first year maths students in the two modules PRB and STA. The scatterplot below shows the module marks for <span class="math inline">\(n=203\)</span> students on probability (PRB, <span class="math inline">\(x_1\)</span>) and statistics (STA, <span class="math inline">\(x_2\)</span>), with the sample mean vector <span class="math inline">\(\begin{pmatrix}62.1 \\62.7 \\\end{pmatrix}\)</span> marked on as a red ‘+’.</p>
<p><img src="07-mvn_files/figure-html/unnamed-chunk-28-1.png" width="672" /></p>
<p>The target for the module mean for a large population of students should be exactly 60 for both modules. We now conduct a hypothesis test to see if the lecturers have missed the target and made the exam too difficult. We will test the hypotheses <span class="math inline">\(H_0\)</span> versus <span class="math inline">\(H_1\)</span> at the 5% level where
<span class="math display">\[
H_0: \boldsymbol \mu= \begin{pmatrix} 60 \\ 60 \end{pmatrix} \qquad \text{and} \qquad H_1: \boldsymbol \mu\neq \begin{pmatrix} 60 \\ 60 \end{pmatrix}.
\]</span></p>
<p>Let’s assume to begin with that observations <span class="math inline">\(\boldsymbol x_1,\ldots,\boldsymbol x_{203}\)</span> are a random sample from <span class="math inline">\(N_2(\boldsymbol \mu,\boldsymbol \Sigma)\)</span> where
<span class="math display">\[
\boldsymbol \Sigma= \begin{pmatrix} 200 &amp; 150 \\ 150 &amp; 300 \end{pmatrix}
\]</span>
is assumed known.</p>
<p>The test statistic is
<span class="math display">\[\begin{align*}
\zeta^2 &amp;= 203 \begin{pmatrix} 62.1 - 60 \\ 62.7 - 60 \end{pmatrix}^\top \begin{pmatrix} 200 &amp; 150 \\ 150 &amp; 300 \end{pmatrix}^{-1} \begin{pmatrix} 62.1 - 60 \\ 62.7 - 60 \end{pmatrix}\\
&amp;=5.84
\end{align*}\]</span></p>
<p>Under <span class="math inline">\(H_0\)</span>, <span class="math inline">\(\zeta^2\sim \chi^2_2\)</span>. and so the critical value is <span class="math inline">\(\chi^2_{2,0.05} = 5.991\)</span>.</p>
<div class="sourceCode" id="cb168"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb168-1" data-line-number="1"><span class="kw">qchisq</span>(<span class="fl">0.95</span>, <span class="dv">2</span>)</a></code></pre></div>
<pre><code>## [1] 5.991465</code></pre>
<p>The plot below shows the density of a <span class="math inline">\(\chi^2_2\)</span> random variable. The vertical red line shows the critical value, the vertical black line the observed value, and the shaded region shows the critical region. As <span class="math inline">\(\zeta^2 &lt; \chi^2_{p,0.05}\)</span> we can see that we do not reject the null hypothesis at the 5% level.</p>
<p><img src="07-mvn_files/figure-html/unnamed-chunk-31-1.png" width="672" /></p>
<p>The <span class="math inline">\(p\)</span>-value is</p>
<div class="sourceCode" id="cb170"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb170-1" data-line-number="1"><span class="dv">1</span><span class="op">-</span><span class="kw">pchisq</span>(zeta2, <span class="dv">2</span>)</a></code></pre></div>
<pre><code>##            [,1]
## [1,] 0.05389049</code></pre>
<p>which is area of the red shaded region.</p>
<p>Note that if we had conducted separate univariate hypothesis tests of <span class="math inline">\(H_0: \mu_1 = 60\)</span> and <span class="math inline">\(H_0: \mu_2 = 60\)</span> then the test statistics would have been:
<span class="math display">\[\begin{eqnarray*}
z_1 &amp;=&amp; \frac{\bar{x}_1 - \mu_1}{\sqrt{\sigma_1^2/n}} = \frac{62-60}{\sqrt{200/203}} = 2.11  \\
z_2 &amp;=&amp; \frac{\bar{x}_2 - \mu_2}{\sqrt{\sigma_2^2/n}} = \frac{62-60}{\sqrt{300/203}} = 2.22.
\end{eqnarray*}\]</span>
The critical value would have been <span class="math inline">\(Z_{0.025} = 1.960\)</span> and both null hypotheses would have been rejected. Therefore we see that a multivariate hypothesis can be accepted when each of univariate components is rejected and vice-versa.</p>
<p>The <span class="math inline">\(95\)</span>% confidence region is the interior of an ellipse, centred on <span class="math inline">\(\bar{\boldsymbol x}\)</span>, with the angle of the major-axis governed by <span class="math inline">\(\boldsymbol \Sigma\)</span> (given by the eigenvectors of <span class="math inline">\(\boldsymbol \Sigma\)</span>). We can see from the plot below that <span class="math inline">\((60,60)^\top\)</span>, marked with a cross, lies just inside the confidence region.`</p>
<p><img src="07-mvn_files/figure-html/unnamed-chunk-33-1.png" width="672" /></p>
</div>
</div>
<div id="onesample" class="section level3">
<h3><span class="header-section-number">7.4.2</span> <span class="math inline">\(\boldsymbol \Sigma\)</span> unknown: 1 sample</h3>
<p>In the previous section we considered a hypothesis test of
<span class="math display">\[H_0: \boldsymbol \mu= \boldsymbol a\mbox{ vs } H_1: \boldsymbol \mu\neq \boldsymbol a\]</span> based on an IID sample from <span class="math inline">\(N_p(\boldsymbol \mu,\boldsymbol \Sigma)\)</span> when <span class="math inline">\(\boldsymbol \Sigma\)</span> was <strong>known</strong>. In reality, we rarely know <span class="math inline">\(\boldsymbol \Sigma\)</span>, so we <strong>replace it with the sample covariance matrix</strong>, <span class="math inline">\(\boldsymbol S\)</span>. Corollary <a href="7-3-hotellings-t2-distribution.html#cor:csix6">7.6</a> tells us that the distribution is then <span class="math inline">\(F_{p,n-p}\)</span> rather than <span class="math inline">\(\chi^2_p\)</span> as was the case when <span class="math inline">\(\boldsymbol \Sigma\)</span> was known.</p>
<p>More specifically, we use the test statistic:
<span class="math display">\[\gamma^2 = \frac{n-p}{p} (\bar{\boldsymbol x}-\boldsymbol a)^\top \boldsymbol S^{-1} (\bar{\boldsymbol x}-\boldsymbol a),\]</span>
Corollary <a href="7-3-hotellings-t2-distribution.html#cor:csix6">7.6</a> tellsus that when <span class="math inline">\(H_0\)</span> is true,
<span class="math display">\[\gamma^2 \sim F_{p,n-p}.\]</span>
As before, depending upon our approach we either</p>
<ul>
<li><p>(Neyman-Pearson approach) reject <span class="math inline">\(H_0\)</span> if <span class="math inline">\(\gamma^2 &gt; F_{p,n-p,\alpha}\)</span>, where <span class="math inline">\(\alpha\)</span> is the significance level.</p></li>
<li><p>(Fisherian approach) compute the p-value <span class="math inline">\(p = \mathbb{P}(F_{p,n-p} &gt; \gamma^2_{obs})\)</span>.</p></li>
</ul>
<p>The <span class="math inline">\(100(1-\alpha)\)</span>% confidence region for <span class="math inline">\(\boldsymbol \mu\)</span> is <span class="math inline">\(\{ \boldsymbol a: \gamma^2 \leq F_{p,n-p,\alpha} \}\)</span>, which will again be the interior of an ellipse or ellipsoid, but the confidence region is now determined by <span class="math inline">\(\boldsymbol S\)</span> rather than <span class="math inline">\(\boldsymbol \Sigma\)</span>.</p>
<div id="example-continued" class="section level4 unnumbered">
<h4>Example continued</h4>
<p>We return to the example with the module marks for <span class="math inline">\(n=203\)</span> students on probability (PRB, <span class="math inline">\(x_1\)</span>) and statistics (STA, <span class="math inline">\(x_2\)</span>), but now we assume that <span class="math inline">\(\boldsymbol \Sigma\)</span> is unknown.</p>
<p>The sample mean and sample covariance matrix are
<span class="math display">\[\bar{\boldsymbol x} = \begin{pmatrix}62.1 \\62.7 \\\end{pmatrix} \qquad \boldsymbol S= \begin{pmatrix}191&amp;155.6 \\155.6&amp;313.5 \\\end{pmatrix}\]</span>
We conduct a hypothesis test at the 5% level of:
<span class="math display">\[H_0: \boldsymbol \mu= \begin{pmatrix} 60 \\ 60 \end{pmatrix} \qquad \text{vs.} \qquad H_1: \boldsymbol \mu\neq \begin{pmatrix} 60 \\ 60 \end{pmatrix}.\]</span></p>
<p>The test statistic is
<span class="math display">\[\begin{align*}
\gamma^2 &amp;= \frac{203-2}{2} \begin{pmatrix} 62.1 - 60 \\ 62.7 - 60 \end{pmatrix}^\top \begin{pmatrix}191&amp;155.6 \\155.6&amp;313.5 \\\end{pmatrix}^{-1} \begin{pmatrix} 62.1 - 60 \\ 62.7 - 60 \end{pmatrix} \\
 &amp;= 2.84.
\end{align*}\]</span>
The critical value is <span class="math inline">\(F_{2,201,0.05}\)</span></p>
<div class="sourceCode" id="cb172"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb172-1" data-line-number="1"> <span class="kw">qf</span>(<span class="fl">0.95</span>, <span class="dv">2</span>,<span class="dv">201</span>)</a></code></pre></div>
<pre><code>## [1] 3.040828</code></pre>
<p>so <span class="math inline">\(\gamma^2 &lt; F_{p,n-p,0.05}\)</span> and we do not reject the null hypothesis at the 5% level.</p>
<p>The <span class="math inline">\(p\)</span>-value is</p>
<div class="sourceCode" id="cb174"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb174-1" data-line-number="1"><span class="dv">1</span><span class="op">-</span><span class="kw">pf</span>(gamma2, <span class="dv">2</span>, <span class="dv">201</span>)</a></code></pre></div>
<pre><code>##            [,1]
## [1,] 0.06051421</code></pre>
<p>Thankfully, we don’t need to do all the computation ourselves whenever we want to do a test, as there are several R packages that will do the work for us:</p>
<div class="sourceCode" id="cb176"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb176-1" data-line-number="1"><span class="kw">library</span>(ICSNP) <span class="co"># you&#39;ll need to install this package the first time</span></a>
<a class="sourceLine" id="cb176-2" data-line-number="2"><span class="kw">HotellingsT2</span>(X,  <span class="dt">mu =</span> mu)</a></code></pre></div>
<pre><code>## 
##  Hotelling&#39;s one sample T2-test
## 
## data:  X
## T.2 = 2.8444, df1 = 2, df2 = 201, p-value = 0.06051
## alternative hypothesis: true location is not equal to c(60,60)</code></pre>
<p>Notice again the difference to the two univariate tests</p>
<div class="sourceCode" id="cb178"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb178-1" data-line-number="1"><span class="kw">t.test</span>(X[,<span class="dv">1</span>], <span class="dt">mu=</span><span class="dv">60</span>)</a></code></pre></div>
<pre><code>## 
##  One Sample t-test
## 
## data:  X[, 1]
## t = 2.1581, df = 202, p-value = 0.0321
## alternative hypothesis: true mean is not equal to 60
## 95 percent confidence interval:
##  60.18121 64.01584
## sample estimates:
## mean of x 
##  62.09852</code></pre>
<div class="sourceCode" id="cb180"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb180-1" data-line-number="1"><span class="kw">t.test</span>(X[,<span class="dv">2</span>], <span class="dt">mu=</span><span class="dv">60</span>)</a></code></pre></div>
<pre><code>## 
##  One Sample t-test
## 
## data:  X[, 2]
## t = 2.1669, df = 202, p-value = 0.03141
## alternative hypothesis: true mean is not equal to 60
## 95 percent confidence interval:
##  60.24305 65.15596
## sample estimates:
## mean of x 
##  62.69951</code></pre>
<p>The <span class="math inline">\(95\)</span>% confidence region is the interior of an ellipse, centred on <span class="math inline">\(\bar{\boldsymbol x}\)</span>, with the angle of the major-axis governed by <span class="math inline">\(\boldsymbol S\)</span>. The confidence region is very slightly larger than when <span class="math inline">\(\boldsymbol \Sigma\)</span> was known.</p>
<p><img src="07-mvn_files/figure-html/unnamed-chunk-39-1.png" width="672" /></p>
</div>
</div>
<div id="boldsymbol-sigma-unknown-2-samples" class="section level3">
<h3><span class="header-section-number">7.4.3</span> <span class="math inline">\(\boldsymbol \Sigma\)</span> unknown: 2 samples</h3>
<p>Suppose now we have data from two populations <span class="math inline">\(\boldsymbol x_1, \ldots, \boldsymbol x_n\)</span> and <span class="math inline">\(\boldsymbol y_1,\ldots, \boldsymbol y_m\)</span>, and that we wish to test the difference between the two population means. As with the univariate case, there are two cases to consider:</p>
<!-- PROBLEM IS ... HERE-->
<div id="paired-case" class="section level4 unnumbered">
<h4>Paired case</h4>
<p>If <span class="math inline">\(m=n\)</span> and there exists some experimental link between <span class="math inline">\(\boldsymbol x_i\)</span> and <span class="math inline">\(\boldsymbol y_i\)</span> then we can look at the differences <span class="math inline">\(\boldsymbol z_i = \boldsymbol y_i - \boldsymbol x_i\)</span> for <span class="math inline">\(i=1,\ldots,n\)</span>. For example, <span class="math inline">\(\boldsymbol x_i\)</span> and <span class="math inline">\(\boldsymbol y_i\)</span> could be vectors of pre-treatment and post-treatment measurements, respectively, of the same variables. The crucial assumption is that the differences <span class="math inline">\(\boldsymbol z_i\)</span> are IID <span class="math inline">\(N_p(\boldsymbol \mu, \boldsymbol \Sigma)\)</span>. To examine the null hypothesis of no difference between the means we would test</p>
<!-- PROBLEM IS DEFINITELY BELOW HERE-->
<p><span class="math display">\[H_0: \boldsymbol \mu={\mathbf 0}_p \mbox{ vs } H_1: \boldsymbol \mu\neq {\mathbf 0}_p.\]</span></p>
<p>We then base our inference on <span class="math inline">\(\bar{\boldsymbol z} = \frac{1}{n} \sum_{i=1}^n \boldsymbol z_i = \bar{\boldsymbol y} - \bar{\boldsymbol x}\)</span>, and proceed exactly as in the 1 sample case, using the test in Secion <a href="7-4-inference-based-on-the-mvn.html#onesampleSigma">7.4.1</a> if <span class="math inline">\(\boldsymbol \Sigma\)</span> is known, or else the test in Section <a href="7-4-inference-based-on-the-mvn.html#onesample">7.4.2</a> if with <span class="math inline">\(\boldsymbol S= \frac{1}{n} \sum_{i=1}^n (\boldsymbol z_i - \bar{\boldsymbol z})(\boldsymbol z_i - \bar{\boldsymbol z})^\top\)</span>.</p>
<!-- PROBLEM IS ABOVE HERE-->
</div>
<div id="unpaired-case" class="section level4 unnumbered">
<h4>Unpaired case</h4>
<p>The unpaired case is where <span class="math inline">\(\boldsymbol x_i\)</span> and <span class="math inline">\(\boldsymbol y_i\)</span> are independent and not connected to each other. For example, in a clinical trial we may have two separate groups of patients, where one group receives a placebo and the other group receives an active treatment. Let <span class="math inline">\(\boldsymbol x_1,\ldots,\boldsymbol x_n\)</span> be an IID sample from <span class="math inline">\(N_p(\boldsymbol \mu_1,\boldsymbol \Sigma)\)</span> and let <span class="math inline">\(\boldsymbol y_1,\ldots,\boldsymbol y_m\)</span> be an IID sample from <span class="math inline">\(N_p(\boldsymbol \mu_2,\boldsymbol \Sigma)\)</span>. In this case, we can base our inference on the following result.</p>
<!-- PROBLEM IS ABOVE HERE-->

<div class="proposition">
<span id="prp:seven1" class="proposition"><strong>Proposition 7.15  </strong></span>Suppose
<span class="math display">\[\begin{align*}
\boldsymbol x_1,\ldots,\boldsymbol x_n  &amp;\sim N_p(\boldsymbol \mu_1,\boldsymbol \Sigma_1)\\ 
\boldsymbol y_1,\ldots,\boldsymbol y_m  &amp;\sim N_p(\boldsymbol \mu_2,\boldsymbol \Sigma_2). 
\end{align*}\]</span>
Then when <span class="math inline">\(\boldsymbol \mu_1 = \boldsymbol \mu_2\)</span> and <span class="math inline">\(\boldsymbol \Sigma_1 = \boldsymbol \Sigma_2\)</span> (i.e. when the two populations have the same distribution),
<span class="math display">\[\frac{nm}{n+m} (\bar{\boldsymbol y} - \bar{\boldsymbol x})^\top \boldsymbol S_u^{-1} (\bar{\boldsymbol y} - \bar{\boldsymbol x}) \sim T^2(p,n+m-2),\]</span>
where
<span class="math display">\[\boldsymbol S_u = \frac{n\boldsymbol S_1 + m\boldsymbol S_2}{n+m-2}\]</span>
is the pooled unbiased variance matrix estimator
and <span class="math inline">\(\boldsymbol S_j\)</span> is the sample covariance matrix for group <span class="math inline">\(j\)</span>.
</div>


<div class="proof">
<p> <span class="proof"><em>Proof. </em></span> We know that <span class="math inline">\(\bar{\boldsymbol x} \sim N_p \left(\boldsymbol \mu_1,n^{-1}\boldsymbol \Sigma_1 \right)\)</span> and <span class="math inline">\(\bar{\boldsymbol y} \sim N_p \left(\boldsymbol \mu_2,m^{-1}\boldsymbol \Sigma_2 \right)\)</span>, and <span class="math inline">\(\bar{\boldsymbol x}\)</span> and <span class="math inline">\(\bar{\boldsymbol y}\)</span> are independent, so
<span class="math display">\[\bar{\boldsymbol y} - \bar{\boldsymbol x} \sim N_p \left(\boldsymbol \mu_2 - \boldsymbol \mu_1, \frac{1}{n}\boldsymbol \Sigma_1 + \frac{1}{m} \boldsymbol \Sigma_2 \right).\]</span>
If <span class="math inline">\(\boldsymbol \mu_1 = \boldsymbol \mu_2\)</span> and <span class="math inline">\(\boldsymbol \Sigma_1 = \boldsymbol \Sigma_2 = \boldsymbol \Sigma\)</span>, then <span class="math inline">\(\bar{\boldsymbol y} - \bar{\boldsymbol x} \sim N_p \left(\boldsymbol 0_p, \left(\frac{1}{n} + \frac{1}{m} \right)\boldsymbol \Sigma\right)\)</span> and
<span class="math display">\[\boldsymbol z= \left(\frac{1}{n} + \frac{1}{m} \right)^{-1/2} (\bar{\boldsymbol y} - \bar{\boldsymbol x}) \sim N_p(\boldsymbol 0_p,\boldsymbol \Sigma).\]</span></p>
<p>From Proposition <a href="7-2-the-wishart-distribution.html#prp:six12">7.12</a> we know that <span class="math inline">\(n\boldsymbol S_1 \sim W_p(\Sigma_1,n-1)\)</span> and <span class="math inline">\(m\boldsymbol S_2 \sim W_p(\Sigma_2,m-1)\)</span>. Therefore when <span class="math inline">\(\boldsymbol \Sigma_1 = \boldsymbol \Sigma_2 = \boldsymbol \Sigma\)</span>,
<span class="math display">\[\begin{eqnarray*}
\boldsymbol M= (n+m-2)\boldsymbol S_u &amp;=&amp; (n+m-2)\frac{n\boldsymbol S_1 + m\boldsymbol S_2}{n+m-2} \\
&amp;=&amp; n\boldsymbol S_1 + m\boldsymbol S_2 \sim W_p(\boldsymbol \Sigma,n+m-2)
\end{eqnarray*}\]</span>
by Proposition <a href="7-2-the-wishart-distribution.html#prp:six10">7.11</a>, using the fact that <span class="math inline">\(\boldsymbol S_1\)</span> and <span class="math inline">\(\boldsymbol S_2\)</span> are independent.</p>
Now <span class="math inline">\(\boldsymbol z\)</span> is independent of <span class="math inline">\(\boldsymbol M\)</span>, since <span class="math inline">\(\bar{\boldsymbol x}\)</span> and <span class="math inline">\(\bar{\boldsymbol y}\)</span> are independent of <span class="math inline">\(\boldsymbol S_1\)</span> and <span class="math inline">\(\boldsymbol S_2\)</span>, respectively, by Proposition <a href="7-1-definition-and-properties-of-the-mvn.html#prp:six6">7.6</a>. Therefore, applying Proposition <a href="7-3-hotellings-t2-distribution.html#prp:six13">7.13</a> with <span class="math inline">\(\boldsymbol x= \boldsymbol z\)</span> and <span class="math inline">\(\boldsymbol M= (n+m-2)\boldsymbol S_u\)</span>, we have
<span class="math display">\[\begin{align*}
(n+m-2) \boldsymbol z^\top ((n+m-2)\boldsymbol S_u)^{-1} \boldsymbol z&amp;= \boldsymbol z^\top \boldsymbol S_u^{-1} \boldsymbol z\\
&amp;\sim T^2(p,n+m-2)
\end{align*}\]</span>
and
<span class="math display">\[\begin{eqnarray*}
\boldsymbol z^\top \boldsymbol S_u^{-1} \boldsymbol z&amp;=&amp; \left(\frac{1}{n} + \frac{1}{m} \right)^{-1/2} (\bar{\boldsymbol y} - \bar{\boldsymbol x})^\top \boldsymbol S_u^{-1} \left(\frac{1}{n} + \frac{1}{m} \right)^{-1/2} (\bar{\boldsymbol y} - \bar{\boldsymbol x}) \\
&amp;=&amp; \left(\frac{1}{n} + \frac{1}{m} \right)^{-1} (\bar{\boldsymbol y} - \bar{\boldsymbol x})^\top \boldsymbol S_u^{-1} (\bar{\boldsymbol y} - \bar{\boldsymbol x}).
\end{eqnarray*}\]</span>
Finally,
<span class="math display">\[\left(\frac{1}{n} + \frac{1}{m} \right)^{-1} = \left(\frac{m}{nm} + \frac{n}{nm} \right)^{-1} = \left(\frac{n+m}{nm} \right)^{-1} = \frac{nm}{n+m},\]</span>
so Proposition <a href="7-4-inference-based-on-the-mvn.html#prp:seven1">7.15</a> is proved.
</div>

<p>As in the one sample case, we can convert Hotelling’s two-sample <span class="math inline">\(T^2\)</span> statistic to the <span class="math inline">\(F\)</span> distribution using Proposition <a href="7-3-hotellings-t2-distribution.html#prp:six14">7.14</a>.</p>

<div class="corollary">
<span id="cor:cseven1" class="corollary"><strong>Corollary 7.7  </strong></span>Using the notation of Proposition <a href="7-4-inference-based-on-the-mvn.html#prp:seven1">7.15</a>, it follows that
<span class="math display">\[\delta^2 = \frac{(n+m-p-1)}{(n+m-2)p} \frac{nm}{(n+m)} (\bar{\boldsymbol y} - \bar{\boldsymbol x})^\top \boldsymbol S_u^{-1} (\bar{\boldsymbol y} - \bar{\boldsymbol x}) \sim F_{p,n+m-p-1}.\]</span>
</div>


<div class="proof">
 <span class="proof"><em>Proof. </em></span> Simply apply Proposition <a href="7-3-hotellings-t2-distribution.html#prp:six14">7.14</a> to the statistic in Proposition <a href="7-4-inference-based-on-the-mvn.html#prp:seven1">7.15</a> (replace <span class="math inline">\(n\)</span> with <span class="math inline">\(n+m-2\)</span>).
</div>

<!-- PROBLEM IS ABOVE HERE-->
</div>
<div id="example-continued-1" class="section level4 unnumbered">
<h4>Example continued</h4>
<p>There are two different maths undergraduate programmes at the University of Nottingham: a 3-year (G100) and a 4-year (G103) programme. For the exam marks example, is there a significant difference between students registered on the two different programmes? Let <span class="math inline">\(\boldsymbol x_1,\ldots, \boldsymbol x_{131}\)</span> be the exam marks of the G100 students and let <span class="math inline">\(\boldsymbol y_1,\ldots, \boldsymbol y_{72}\)</span> be the exam marks of the G103 students. The data is shown below, with the sample means marked as large ‘+’ signs.</p>
<p><img src="07-mvn_files/figure-html/unnamed-chunk-42-1.png" width="672" /></p>
<p>Let <span class="math inline">\(\boldsymbol \mu_1\)</span> and <span class="math inline">\(\boldsymbol \mu_2\)</span> be the population means for G100 and G103 respectively. Our hypotheses are
<span class="math display">\[H_0: \boldsymbol \mu_1 = \boldsymbol \mu_2 \qquad \text{vs.} \qquad H_1: \boldsymbol \mu_1 \neq \boldsymbol \mu_2.\]</span></p>
<p>We will assume that
<span class="math display">\[\begin{align*}
\boldsymbol x_n, \ldots, \boldsymbol x_{131}&amp;\sim N_2(\boldsymbol \mu_1,\boldsymbol \Sigma_1)\\
\boldsymbol y_1,\ldots,\boldsymbol y_m &amp;\sim  N_2(\boldsymbol \mu_2,\boldsymbol \Sigma_2).
\end{align*}\]</span></p>
<p>The sample summary statistics are:
<span class="math display">\[\begin{eqnarray*}
n = 131 &amp;\quad&amp; m = 72 \\
\bar{\boldsymbol x} = \begin{pmatrix}61.6 \\63.2 \\\end{pmatrix}&amp;\quad&amp; \bar{\boldsymbol y}= \begin{pmatrix}63.1 \\61.9 \\\end{pmatrix}\\
\boldsymbol S_1 = \begin{pmatrix}180.2&amp;158.5 \\158.5&amp;312.8 \\\end{pmatrix} &amp;\qquad&amp; \boldsymbol S_2 = \begin{pmatrix}179.1&amp;157.5 \\157.5&amp;310.8 \\\end{pmatrix}
\end{eqnarray*}\]</span></p>
<p>The assumption <span class="math inline">\(\boldsymbol \Sigma= \boldsymbol \Sigma_1 = \boldsymbol \Sigma_2\)</span> does not look unreasonable given the sample covariance matrices. Using the <code>HotellingT2</code> command from the <code>ICSNP</code> package, we find</p>
<!--
compute
\begin{eqnarray*}
\bS_u &=& \frac{1}{131+72-2} \lb 131 \begin{pmatrix}180.2&158.5 \\158.5&312.8 \\\end{pmatrix} + 72 \begin{pmatrix}209.1&151.7 \\151.7&313.7 \\\end{pmatrix} \rb \\
&=& \begin{pmatrix} 213.21 & 146.76 \\ 146.76 & 332.96 \end{pmatrix}
\end{eqnarray*}
and, therefore, ${\ds \bS_u^{-1} = \begin{pmatrix} 0.0067 & -0.0030 \\ -0.0030 & 0.0043 \end{pmatrix}}.$

The test statistic is
$$\delta^2 = \frac{141}{284} \times \frac{4508}{144} \begin{pmatrix} 4.179 \\ -2.329 \end{pmatrix}^\top \begin{pmatrix} 0.0067 & -0.0030 \\ -0.0030 & 0.0043 \end{pmatrix} \begin{pmatrix} 4.179 \\ -2.329 \end{pmatrix}
= 3.089$$


```r
S100 <- cov(G100)*(n-1)/n
S103 <- cov(G103)*(m-1)/m
Su <- (n*S100+m*S103)/(n+m-2)
delta2 <- (n+m-2-1)/(2*(n+m-2)) *n*m/(n+m)*t((ybar-xbar))%*% solve(Su)%*%(ybar-xbar)
```
-->
<div class="sourceCode" id="cb182"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb182-1" data-line-number="1"><span class="kw">library</span>(ICSNP) <span class="co"># you&#39;ll need to install this</span></a>
<a class="sourceLine" id="cb182-2" data-line-number="2"><span class="kw">HotellingsT2</span>(G100, G103)</a></code></pre></div>
<pre><code>## 
##  Hotelling&#39;s two sample T2-test
## 
## data:  G100 and G103
## T.2 = 1.0696, df1 = 2, df2 = 200, p-value = 0.3451
## alternative hypothesis: true location difference is not equal to c(0,0)</code></pre>
<p>So the test statistic was computed to be <span class="math inline">\(\delta^2 = 1.06962\)</span> and the p-value is <span class="math inline">\(p= 0.345\)</span>.</p>
<p>The critical value for <span class="math inline">\(\alpha=0.05\)</span> is
<span class="math display">\[F_{2,n+m-2-1,\alpha} = F_{2,200,0.05} = 3.041.\]</span></p>
<div class="sourceCode" id="cb184"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb184-1" data-line-number="1"><span class="kw">qf</span>(<span class="fl">0.95</span>,<span class="dv">2</span>, <span class="dv">200</span>)</a></code></pre></div>
<pre><code>## [1] 3.041056</code></pre>
<p>Therefore <span class="math inline">\(\delta^2 &lt; F_{p,n+m-p-1}\)</span>, so we do not reject the null hypothesis at the 5% level.</p>
<!-- PROBLEM IS ABOVE HERE ->

## Exercises

1. If $M \sim W_p(\bSigma,n)$, prove that $\BE \bM = n \bSigma$.


2. If $\bx \sim N_p(\bmu,\bSigma)$ and $\ba$ is any fixed vector of dimension $p$, show that
$$ z = \frac{ \ba^\top (\bx - \bmu) }{ \sqrt{\ba^\top \bSigma \ba} } \sim N(0,1).$$

3. Prove that 
$$ n ( \bar{\bx} - \bmu)^{\rm T} \bSigma^{-1} ( \bar{\bx} - \bmu) \sim \chi_p^2 . $$



4. If $\bx_1, \ldots, \bx_n$ are i.i.d. $N_p(\bmu, \bSigma)$ with sample mean $\bar{\bx}$ and sample covariance matrix $\bS$, show that $\cov(\bar{\bx}, \bx_i - \bar{\bx}) = \bzero$.  Hence deduce that $\bar{\bx}$ and $\bS$ are independent.

    **Hint:** If the random vector $\bx$ is independent of the random vector $\by$, then $\bx$ is also independent of any function $f(\by)$.


5. A survey of $n=25$ families records the head length of the first ($x_1$) and second ($x_2$) sons.  The sample mean is $\bar{\bx} = (185.72, 183.84)^\top$.  Assume that the observations are sampled from $N_2(\bmu,\bSigma)$ where $\bSigma = \tdiag(100,100)$.
    - Conduct a hypothesis test of $\bmu = (182,182)^\top$.
    - Show that the confidence region for $\bmu$ is circular.  Find its centre and radius.
    - Repeat the hypothesis test and sketch the confidence region if we assume that
$$\bSigma = \begin{pmatrix} 100 & 50 \\ 50 & 100 \end{pmatrix}.$$

6.  Let $\bx_1, \ldots, \bx_{20}$ be a random sample of vectors from a $N_3(\bmu,\bSigma)$ population where $\bmu$ and $\bSigma$ are unknown.  The sample mean and sample covariance matrix are given by
$$\bar{\bx} = \begin{pmatrix} 0.358 \\ -1.056 \\ -1.795 \end{pmatrix} \qquad
\bS = \begin{pmatrix} 0.522 & 0.556 & -2.285 \\ 0.556 & 3.258 & -0.765 \\ -2.285 & -0.765 & 14.093 \end{pmatrix}.$$

    - Use Hotelling's $T^2$ distribution to perform a significance test of the hypothesis $H_0: \bmu = (0,-1,-1)^T$.  Note that $\bS = \bQ \bLambda \bQ^T$ where $\bLambda = \tdiag(14.531, 3.253,0.090)$ and
$$\bQ = \begin{pmatrix} -0.163 & -0.121 & -0.979 \\ -0.075 & -0.988 & 0.135 \\ 0.984 & -0.095 & -0.152 \end{pmatrix}.$$
    - Let $\bmu = (\mu_1,\mu_2,\mu_3)^\top$.  Perform separate (univariate) $t$-tests of the following hypotheses: $\mu_1 = 0$; $\mu_2 = -1$; $\mu_3 = -1$.  Compare the results of the individual tests with the combined test based on Hotelling's $T^2$ distribution in (a).  Comment briefly.

7. Two measurements were collected on each of 36 flea-beetles; 18 of the beetles were from a species called Chaetocnema concinna and the other 18 were from another species called Chaetocnema heikertingeri.  The first variable consisted of the sum of widths (in micrometres) of the first joints of the first two tarsi (``feet''); and the second variable consisted of the corresponding sum for the second joints.  It is of interest to know
whether or not the population means of the two species are different.

    The sample means are $\bar{\bx}_1 = (181.50,129.17)^\top$ and $\bar{\bx}_2 = (205.06,120.44)^\top$; and the sample covariance matrices are
$$\bS_1 = \begin{pmatrix} 120.58 & 56.25 \\ 56.25 & 44.63 \end{pmatrix} \qquad
\bS_2 = \begin{pmatrix} 203.94 & 73.42 \\ 73.42 & 47.14 \end{pmatrix}.$$
Conduct a suitable hypothesis test.  State your conclusion in words.  What assumptions have you made in constructing the test?  Do any of these assumptions seem suspect with these data?



## Computer tasks

1. Download the wine dataset from the [UCI Machine Learning Repository](https://archive.ics.uci.edu/ml/datasets/wine)


```r
download.file(url="https://archive.ics.uci.edu/ml/machine-learning-databases/wine/wine.data", destfile="wine.data")
wine <- read.csv("wine.data", header=FALSE)
colnames(wine) <- c("Type", "Alcohol", "Malic",
"Ash",
"Alcalinity",
"Magnesium",
"Phenols",
"Flavanoids",
"Nonflavanoids",
"Proanthocyanins",
"Color",
"Hue",
"Dilution",
"Proline")
```

Let's assume this selection of wines is a random sample from the population of all possible wines. Conduct a multivariate hypothesis test to see whether the average wine has an alcohol content different from 13 and an Malic acide  content different from 2?

Test whether the alcohol and Malic acid content are significantly different between wines of type 1 and wines of type 2.
  
  
2. The command `rWishart` can be used to simulate from a Wishart distribution. Alternatively, to sample from $W_p(\bSigma,n)$, you can sample $\bx_1, \ldots, \bx_n \sim N_p(\bzero, \bSigma)$ and set $\bM = \sum \bx_i \bx_i^\top$.

    - Generate 10,000 samples $M_1, \ldots,$ from a $W_2(\bSigma, 10)$ distribution with $\bSigma = \operatorname{diag}(2,1)$ using these two approaches. Compute the mean and variance of the two samples and check these accord with Proposition \@ref(prp:wishartmean).

    - Set $\ba = (1\; 1)^\top$. Check empirically that $\ba^\top \bM \ba \sim 3 \chi^2_10$. **Hint** plot the theoretical densities on top of a histogram of the sampled quantities.
    
    - Suppose $\bx_1, \ldots, \bx_{10} \sim N_2(\bzero, \operatorname{diag}(2,1))$. Empirically check that  Proposition \@ref(prp:six12) is true by computing the covariance matrix of a large number of such samples, and comparing this to the mean and variance of the Wishart distribution specified in the proposition.
    - Similarly, validate Corollary \@ref(cor:csix6) by comparing the distribution of $\gamma^2$ with a $F_{p, n-p}$ distribution.
    

3. Explain why the confidence regions for the population mean discussed in Section  \@ref(onesample) are ellipses. What are the major and minor axes of these ellipses?

    Download the exam data from Moodle. We will now work through how to plot the confidence regions shown.
    
    - Firstly, let's plot a circle with equation 
    $$x^2+y^2=c^2$$
    or in vector form:
    $$\bx^\top\bx=c^2.$$
    Why must $x$ be in the range $(-c,c)$? Suppose $c=10$. In this case you can plot a circle using the following commands

```r
x <- seq(-10, 10,1/1000) 
y <- -sqrt(10^2-x^2)
x <- c(x, 10, rev(x) )
y <- c(y, 0, -rev(y))
plot(x,y,type='l')
```
 
     Explain why this works. **Note** if your plotting window is not square, your circle will look like an ellipse!
    
    - Secondly, let's now plot the ellipse 
    $$\bx^\top \bS^{-1}\bx=c^2$$
    We can do this by noting that $\bu = \bS^{-1/2}\bx$ obeys the equation
    $$\bu^\top\bu=c^2,$$
    i.e., a circle. Thus you can plot an ellipse by using the code above to generate a circle, and then transforming it to be an ellipse. Plot the ellipse for $\bS$ given by the sample covariance matrix of the data.
    
    - Thirdly, we can plot the ellipse
      $$(\bx-\mu)^\top \bS^{-1}(\bx-\mu)=c^2$$
    by shifting the ellipse to be centered around $\mu$. Thus plot the 95\% confidence region for the population mean for the exam data.
    
<!--```{r, echo=FALSE, warning=FALSE, message=FALSE, cache=TRUE}
const2 <-  qf(0.95, 2,201)*2/(N-2)
u1 <- seq(-sqrt(const2), sqrt(const2),1/1000)
#u1 <- c(u1, )
u2 <- -sqrt(const2-u1^2)
u1 <- c(u1, sqrt(const2), rev(u1) )
u2 <- c(u2, 0, -rev(u2))
u <- rbind(u1,u2)
library(expm)
S <- cov(X)*(N-1)/N
S12 <- sqrtm(S)
a <- S12%*% u+c(xbar)
plot(X)
lines(a[1,], a[2,], lty=2, col=2, cex=2)
points(60,60, col=2, pch="+", cex=2)
```-->

</div>
</div>
</div>
<!-- </div> -->
            </section>

          </div>
        </div>
      </div>
<a href="7-3-hotellings-t2-distribution.html" class="navigation navigation-prev navigation-unique" aria-label="Previous page"><i class="fa fa-angle-left"></i></a>

    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["MultivariateStatistics.pdf", "MultivariateStatistics.epub"],
"toc": {
"collapse": "section"
},
"pandoc_args": "--top-level-division=[chapter|part]"
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
