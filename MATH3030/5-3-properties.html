<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>5.3 Properties | Multivariate Statistics</title>
  <meta name="description" content="The lecture notes for MATH3030/4068: Multivariate Analysis / Applied Multivariate Statistics" />
  <meta name="generator" content="bookdown 0.20.6 and GitBook 2.6.7" />

  <meta property="og:title" content="5.3 Properties | Multivariate Statistics" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="The lecture notes for MATH3030/4068: Multivariate Analysis / Applied Multivariate Statistics" />
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="5.3 Properties | Multivariate Statistics" />
  
  <meta name="twitter:description" content="The lecture notes for MATH3030/4068: Multivariate Analysis / Applied Multivariate Statistics" />
  

<meta name="author" content="Prof. Richard Wilkinson" />



  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="5-2-the-full-set-of-canonical-correlations.html"/>
<link rel="next" href="5-4-exercises-2.html"/>
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />









<script src="libs/kePrint-0.0.1/kePrint.js"></script>
<link href="libs/lightable-0.0.1/lightable.css" rel="stylesheet" />


<style type="text/css">
a.sourceLine { display: inline-block; line-height: 1.25; }
a.sourceLine { pointer-events: none; color: inherit; text-decoration: inherit; }
a.sourceLine:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode { white-space: pre; position: relative; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
code.sourceCode { white-space: pre-wrap; }
a.sourceLine { text-indent: -1em; padding-left: 1em; }
}
pre.numberSource a.sourceLine
  { position: relative; left: -4em; }
pre.numberSource a.sourceLine::before
  { content: attr(data-line-number);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; pointer-events: all; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {  }
@media screen {
a.sourceLine::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Applied multivariate statistics</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Introduction</a></li>
<li class="chapter" data-level="" data-path="part-i-prerequisites.html"><a href="part-i-prerequisites.html"><i class="fa fa-check"></i>PART I: Prerequisites</a></li>
<li class="chapter" data-level="1" data-path="1-stat-prelim.html"><a href="1-stat-prelim.html"><i class="fa fa-check"></i><b>1</b> Statistical Preliminaries</a><ul>
<li class="chapter" data-level="1.1" data-path="1-1-notation.html"><a href="1-1-notation.html"><i class="fa fa-check"></i><b>1.1</b> Notation</a><ul>
<li class="chapter" data-level="1.1.1" data-path="1-1-notation.html"><a href="1-1-notation.html#example-datasets"><i class="fa fa-check"></i><b>1.1.1</b> Example datasets</a></li>
<li class="chapter" data-level="1.1.2" data-path="1-1-notation.html"><a href="1-1-notation.html#aims-of-multivariate-data-analysis"><i class="fa fa-check"></i><b>1.1.2</b> Aims of multivariate data analysis</a></li>
</ul></li>
<li class="chapter" data-level="1.2" data-path="1-2-exploratory-data-analysis-eda.html"><a href="1-2-exploratory-data-analysis-eda.html"><i class="fa fa-check"></i><b>1.2</b> Exploratory data analysis (EDA)</a><ul>
<li class="chapter" data-level="1.2.1" data-path="1-2-exploratory-data-analysis-eda.html"><a href="1-2-exploratory-data-analysis-eda.html#data-visualization"><i class="fa fa-check"></i><b>1.2.1</b> Data visualization</a></li>
<li class="chapter" data-level="1.2.2" data-path="1-2-exploratory-data-analysis-eda.html"><a href="1-2-exploratory-data-analysis-eda.html#summary-statistics"><i class="fa fa-check"></i><b>1.2.2</b> Summary statistics</a></li>
</ul></li>
<li class="chapter" data-level="1.3" data-path="1-3-random-vectors-and-matrices.html"><a href="1-3-random-vectors-and-matrices.html"><i class="fa fa-check"></i><b>1.3</b> Random vectors and matrices</a><ul>
<li class="chapter" data-level="1.3.1" data-path="1-3-random-vectors-and-matrices.html"><a href="1-3-random-vectors-and-matrices.html#estimators"><i class="fa fa-check"></i><b>1.3.1</b> Estimators</a></li>
</ul></li>
<li class="chapter" data-level="1.4" data-path="1-4-computer-tasks.html"><a href="1-4-computer-tasks.html"><i class="fa fa-check"></i><b>1.4</b> Computer tasks</a></li>
<li class="chapter" data-level="1.5" data-path="1-5-exercises.html"><a href="1-5-exercises.html"><i class="fa fa-check"></i><b>1.5</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="2-linalg-prelim.html"><a href="2-linalg-prelim.html"><i class="fa fa-check"></i><b>2</b> Review of linear algebra</a><ul>
<li class="chapter" data-level="2.1" data-path="2-1-linalg-basics.html"><a href="2-1-linalg-basics.html"><i class="fa fa-check"></i><b>2.1</b> Basics</a><ul>
<li class="chapter" data-level="2.1.1" data-path="2-1-linalg-basics.html"><a href="2-1-linalg-basics.html#notation-1"><i class="fa fa-check"></i><b>2.1.1</b> Notation</a></li>
<li class="chapter" data-level="2.1.2" data-path="2-1-linalg-basics.html"><a href="2-1-linalg-basics.html#elementary-matrix-operations"><i class="fa fa-check"></i><b>2.1.2</b> Elementary matrix operations</a></li>
<li class="chapter" data-level="2.1.3" data-path="2-1-linalg-basics.html"><a href="2-1-linalg-basics.html#special-matrices"><i class="fa fa-check"></i><b>2.1.3</b> Special matrices</a></li>
<li class="chapter" data-level="2.1.4" data-path="2-1-linalg-basics.html"><a href="2-1-linalg-basics.html#vectordiff"><i class="fa fa-check"></i><b>2.1.4</b> Vector Differentiation</a></li>
</ul></li>
<li class="chapter" data-level="2.2" data-path="2-2-linalg-vecspaces.html"><a href="2-2-linalg-vecspaces.html"><i class="fa fa-check"></i><b>2.2</b> Vector spaces</a><ul>
<li class="chapter" data-level="2.2.1" data-path="2-2-linalg-vecspaces.html"><a href="2-2-linalg-vecspaces.html#linear-independence"><i class="fa fa-check"></i><b>2.2.1</b> Linear independence</a></li>
<li class="chapter" data-level="2.2.2" data-path="2-2-linalg-vecspaces.html"><a href="2-2-linalg-vecspaces.html#colsspace"><i class="fa fa-check"></i><b>2.2.2</b> Row and column spaces</a></li>
<li class="chapter" data-level="2.2.3" data-path="2-2-linalg-vecspaces.html"><a href="2-2-linalg-vecspaces.html#linear-transformations"><i class="fa fa-check"></i><b>2.2.3</b> Linear transformations</a></li>
</ul></li>
<li class="chapter" data-level="2.3" data-path="2-3-linalg-innerprod.html"><a href="2-3-linalg-innerprod.html"><i class="fa fa-check"></i><b>2.3</b> Inner product spaces</a><ul>
<li class="chapter" data-level="2.3.1" data-path="2-3-linalg-innerprod.html"><a href="2-3-linalg-innerprod.html#normed"><i class="fa fa-check"></i><b>2.3.1</b> Distances, and angles</a></li>
<li class="chapter" data-level="2.3.2" data-path="2-3-linalg-innerprod.html"><a href="2-3-linalg-innerprod.html#orthogonal-matrices"><i class="fa fa-check"></i><b>2.3.2</b> Orthogonal matrices</a></li>
<li class="chapter" data-level="2.3.3" data-path="2-3-linalg-innerprod.html"><a href="2-3-linalg-innerprod.html#projection-matrix"><i class="fa fa-check"></i><b>2.3.3</b> Projections</a></li>
</ul></li>
<li class="chapter" data-level="2.4" data-path="2-4-centering-matrix.html"><a href="2-4-centering-matrix.html"><i class="fa fa-check"></i><b>2.4</b> The Centering Matrix</a></li>
<li class="chapter" data-level="2.5" data-path="2-5-tasks-ch2.html"><a href="2-5-tasks-ch2.html"><i class="fa fa-check"></i><b>2.5</b> Computer tasks</a></li>
<li class="chapter" data-level="2.6" data-path="2-6-exercises-ch2.html"><a href="2-6-exercises-ch2.html"><i class="fa fa-check"></i><b>2.6</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="3-linalg-decomp.html"><a href="3-linalg-decomp.html"><i class="fa fa-check"></i><b>3</b> Matrix decompositions</a><ul>
<li class="chapter" data-level="3.1" data-path="3-1-matrix-matrix.html"><a href="3-1-matrix-matrix.html"><i class="fa fa-check"></i><b>3.1</b> Matrix-matrix products</a></li>
<li class="chapter" data-level="3.2" data-path="3-2-spectraleigen-decomposition.html"><a href="3-2-spectraleigen-decomposition.html"><i class="fa fa-check"></i><b>3.2</b> Spectral/eigen decomposition</a><ul>
<li class="chapter" data-level="3.2.1" data-path="3-2-spectraleigen-decomposition.html"><a href="3-2-spectraleigen-decomposition.html#eigenvalues-and-eigenvectors"><i class="fa fa-check"></i><b>3.2.1</b> Eigenvalues and eigenvectors</a></li>
<li class="chapter" data-level="3.2.2" data-path="3-2-spectraleigen-decomposition.html"><a href="3-2-spectraleigen-decomposition.html#spectral-decomposition"><i class="fa fa-check"></i><b>3.2.2</b> Spectral decomposition</a></li>
<li class="chapter" data-level="3.2.3" data-path="3-2-spectraleigen-decomposition.html"><a href="3-2-spectraleigen-decomposition.html#matrixroots"><i class="fa fa-check"></i><b>3.2.3</b> Matrix square roots</a></li>
</ul></li>
<li class="chapter" data-level="3.3" data-path="3-3-linalg-SVD.html"><a href="3-3-linalg-SVD.html"><i class="fa fa-check"></i><b>3.3</b> Singular Value Decomposition (SVD)</a><ul>
<li class="chapter" data-level="3.3.1" data-path="3-3-linalg-SVD.html"><a href="3-3-linalg-SVD.html#examples"><i class="fa fa-check"></i><b>3.3.1</b> Examples</a></li>
</ul></li>
<li class="chapter" data-level="3.4" data-path="3-4-svdopt.html"><a href="3-4-svdopt.html"><i class="fa fa-check"></i><b>3.4</b> SVD optimization results</a></li>
<li class="chapter" data-level="3.5" data-path="3-5-low-rank-approximation.html"><a href="3-5-low-rank-approximation.html"><i class="fa fa-check"></i><b>3.5</b> Low-rank approximation</a><ul>
<li class="chapter" data-level="3.5.1" data-path="3-5-low-rank-approximation.html"><a href="3-5-low-rank-approximation.html#matrix-norms"><i class="fa fa-check"></i><b>3.5.1</b> Matrix norms</a></li>
<li class="chapter" data-level="3.5.2" data-path="3-5-low-rank-approximation.html"><a href="3-5-low-rank-approximation.html#eckart-young-mirsky-theorem"><i class="fa fa-check"></i><b>3.5.2</b> Eckart-Young-Mirsky Theorem</a></li>
<li class="chapter" data-level="3.5.3" data-path="3-5-low-rank-approximation.html"><a href="3-5-low-rank-approximation.html#example-image-compression"><i class="fa fa-check"></i><b>3.5.3</b> Example: image compression</a></li>
</ul></li>
<li class="chapter" data-level="3.6" data-path="3-6-tasks-ch3.html"><a href="3-6-tasks-ch3.html"><i class="fa fa-check"></i><b>3.6</b> Computer tasks</a></li>
<li class="chapter" data-level="3.7" data-path="3-7-exercises-ch3.html"><a href="3-7-exercises-ch3.html"><i class="fa fa-check"></i><b>3.7</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="part-ii-dimension-reduction-methods.html"><a href="part-ii-dimension-reduction-methods.html"><i class="fa fa-check"></i>PART II: Dimension reduction methods</a><ul>
<li class="chapter" data-level="" data-path="part-ii-dimension-reduction-methods.html"><a href="part-ii-dimension-reduction-methods.html#a-warning"><i class="fa fa-check"></i>A warning</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="4-pca.html"><a href="4-pca.html"><i class="fa fa-check"></i><b>4</b> Principal Component Analysis (PCA)</a><ul>
<li class="chapter" data-level="4.1" data-path="4-1-pca-an-informal-introduction.html"><a href="4-1-pca-an-informal-introduction.html"><i class="fa fa-check"></i><b>4.1</b> PCA: an informal introduction</a><ul>
<li class="chapter" data-level="4.1.1" data-path="4-1-pca-an-informal-introduction.html"><a href="4-1-pca-an-informal-introduction.html#notation-recap"><i class="fa fa-check"></i><b>4.1.1</b> Notation recap</a></li>
<li class="chapter" data-level="4.1.2" data-path="4-1-pca-an-informal-introduction.html"><a href="4-1-pca-an-informal-introduction.html#first-principal-component"><i class="fa fa-check"></i><b>4.1.2</b> First principal component</a></li>
<li class="chapter" data-level="4.1.3" data-path="4-1-pca-an-informal-introduction.html"><a href="4-1-pca-an-informal-introduction.html#second-principal-component"><i class="fa fa-check"></i><b>4.1.3</b> Second principal component</a></li>
<li class="chapter" data-level="4.1.4" data-path="4-1-pca-an-informal-introduction.html"><a href="4-1-pca-an-informal-introduction.html#geometric-interpretation-1"><i class="fa fa-check"></i><b>4.1.4</b> Geometric interpretation</a></li>
<li class="chapter" data-level="4.1.5" data-path="4-1-pca-an-informal-introduction.html"><a href="4-1-pca-an-informal-introduction.html#example"><i class="fa fa-check"></i><b>4.1.5</b> Example</a></li>
<li class="chapter" data-level="4.1.6" data-path="4-1-pca-an-informal-introduction.html"><a href="4-1-pca-an-informal-introduction.html#example-iris"><i class="fa fa-check"></i><b>4.1.6</b> Example: Iris</a></li>
</ul></li>
<li class="chapter" data-level="4.2" data-path="4-2-pca-a-formal-description-with-proofs.html"><a href="4-2-pca-a-formal-description-with-proofs.html"><i class="fa fa-check"></i><b>4.2</b> PCA: a formal description with proofs</a><ul>
<li class="chapter" data-level="4.2.1" data-path="4-2-pca-a-formal-description-with-proofs.html"><a href="4-2-pca-a-formal-description-with-proofs.html#properties-of-principal-components"><i class="fa fa-check"></i><b>4.2.1</b> Properties of principal components</a></li>
<li class="chapter" data-level="4.2.2" data-path="4-2-pca-a-formal-description-with-proofs.html"><a href="4-2-pca-a-formal-description-with-proofs.html#example-football"><i class="fa fa-check"></i><b>4.2.2</b> Example: Football</a></li>
<li class="chapter" data-level="4.2.3" data-path="4-2-pca-a-formal-description-with-proofs.html"><a href="4-2-pca-a-formal-description-with-proofs.html#pcawithR"><i class="fa fa-check"></i><b>4.2.3</b> PCA based on <span class="math inline">\(\boldsymbol R\)</span> versus PCA based on <span class="math inline">\(\boldsymbol S\)</span></a></li>
<li class="chapter" data-level="4.2.4" data-path="4-2-pca-a-formal-description-with-proofs.html"><a href="4-2-pca-a-formal-description-with-proofs.html#population-pca"><i class="fa fa-check"></i><b>4.2.4</b> Population PCA</a></li>
<li class="chapter" data-level="4.2.5" data-path="4-2-pca-a-formal-description-with-proofs.html"><a href="4-2-pca-a-formal-description-with-proofs.html#pca-under-transformations-of-variables"><i class="fa fa-check"></i><b>4.2.5</b> PCA under transformations of variables</a></li>
</ul></li>
<li class="chapter" data-level="4.3" data-path="4-3-an-alternative-view-of-pca.html"><a href="4-3-an-alternative-view-of-pca.html"><i class="fa fa-check"></i><b>4.3</b> An alternative view of PCA</a><ul>
<li class="chapter" data-level="4.3.1" data-path="4-3-an-alternative-view-of-pca.html"><a href="4-3-an-alternative-view-of-pca.html#example-mnist-handwritten-digits"><i class="fa fa-check"></i><b>4.3.1</b> Example: MNIST handwritten digits</a></li>
</ul></li>
<li class="chapter" data-level="4.4" data-path="4-4-pca-comptask.html"><a href="4-4-pca-comptask.html"><i class="fa fa-check"></i><b>4.4</b> Computer tasks</a></li>
<li class="chapter" data-level="4.5" data-path="4-5-exercises-1.html"><a href="4-5-exercises-1.html"><i class="fa fa-check"></i><b>4.5</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="5-cca.html"><a href="5-cca.html"><i class="fa fa-check"></i><b>5</b> Canonical Correlation Analysis (CCA)</a><ul>
<li class="chapter" data-level="5.1" data-path="5-1-cca1.html"><a href="5-1-cca1.html"><i class="fa fa-check"></i><b>5.1</b> The first pair of canonical variables</a><ul>
<li class="chapter" data-level="5.1.1" data-path="5-1-cca1.html"><a href="5-1-cca1.html#premcca"><i class="fa fa-check"></i><b>5.1.1</b> Example: Premier league football</a></li>
</ul></li>
<li class="chapter" data-level="5.2" data-path="5-2-the-full-set-of-canonical-correlations.html"><a href="5-2-the-full-set-of-canonical-correlations.html"><i class="fa fa-check"></i><b>5.2</b> The full set of canonical correlations</a><ul>
<li class="chapter" data-level="5.2.1" data-path="5-2-the-full-set-of-canonical-correlations.html"><a href="5-2-the-full-set-of-canonical-correlations.html#example-continued"><i class="fa fa-check"></i><b>5.2.1</b> Example continued</a></li>
</ul></li>
<li class="chapter" data-level="5.3" data-path="5-3-properties.html"><a href="5-3-properties.html"><i class="fa fa-check"></i><b>5.3</b> Properties</a><ul>
<li class="chapter" data-level="5.3.1" data-path="5-3-properties.html"><a href="5-3-properties.html#connection-with-linear-regression-when-q1"><i class="fa fa-check"></i><b>5.3.1</b> Connection with linear regression when <span class="math inline">\(q=1\)</span></a></li>
<li class="chapter" data-level="5.3.2" data-path="5-3-properties.html"><a href="5-3-properties.html#invarianceequivariance-properties-of-cca"><i class="fa fa-check"></i><b>5.3.2</b> Invariance/equivariance properties of CCA</a></li>
</ul></li>
<li class="chapter" data-level="5.4" data-path="5-4-exercises-2.html"><a href="5-4-exercises-2.html"><i class="fa fa-check"></i><b>5.4</b> Exercises</a></li>
<li class="chapter" data-level="5.5" data-path="5-5-computer-tasks-1.html"><a href="5-5-computer-tasks-1.html"><i class="fa fa-check"></i><b>5.5</b> Computer tasks</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="6-mds.html"><a href="6-mds.html"><i class="fa fa-check"></i><b>6</b> Multidimensional Scaling (MDS)</a><ul>
<li class="chapter" data-level="6.1" data-path="6-1-classical-mds.html"><a href="6-1-classical-mds.html"><i class="fa fa-check"></i><b>6.1</b> Classical MDS</a><ul>
<li class="chapter" data-level="6.1.1" data-path="6-1-classical-mds.html"><a href="6-1-classical-mds.html#example-1"><i class="fa fa-check"></i><b>6.1.1</b> Example 1</a></li>
<li class="chapter" data-level="6.1.2" data-path="6-1-classical-mds.html"><a href="6-1-classical-mds.html#example-2"><i class="fa fa-check"></i><b>6.1.2</b> Example 2</a></li>
<li class="chapter" data-level="6.1.3" data-path="6-1-classical-mds.html"><a href="6-1-classical-mds.html#example-3"><i class="fa fa-check"></i><b>6.1.3</b> Example 3</a></li>
</ul></li>
<li class="chapter" data-level="6.2" data-path="6-2-principal-coordinates.html"><a href="6-2-principal-coordinates.html"><i class="fa fa-check"></i><b>6.2</b> Principal Coordinates</a></li>
<li class="chapter" data-level="6.3" data-path="6-3-properties-1.html"><a href="6-3-properties-1.html"><i class="fa fa-check"></i><b>6.3</b> Properties</a></li>
<li class="chapter" data-level="6.4" data-path="6-4-similarity-measures.html"><a href="6-4-similarity-measures.html"><i class="fa fa-check"></i><b>6.4</b> Similarity measures</a></li>
<li class="chapter" data-level="6.5" data-path="6-5-exercises-3.html"><a href="6-5-exercises-3.html"><i class="fa fa-check"></i><b>6.5</b> Exercises</a></li>
<li class="chapter" data-level="6.6" data-path="6-6-computer-tasks-2.html"><a href="6-6-computer-tasks-2.html"><i class="fa fa-check"></i><b>6.6</b> Computer Tasks</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="part-iii-inference-using-the-multivariate-normal-distribution-mvn.html"><a href="part-iii-inference-using-the-multivariate-normal-distribution-mvn.html"><i class="fa fa-check"></i>Part III: Inference using the Multivariate Normal Distribution (MVN)</a></li>
<li class="chapter" data-level="7" data-path="7-multinormal.html"><a href="7-multinormal.html"><i class="fa fa-check"></i><b>7</b> The Multivariate Normal Distribution</a><ul>
<li class="chapter" data-level="7.1" data-path="7-1-definition-and-properties-of-the-mvn.html"><a href="7-1-definition-and-properties-of-the-mvn.html"><i class="fa fa-check"></i><b>7.1</b> Definition and Properties of the MVN</a><ul>
<li class="chapter" data-level="7.1.1" data-path="7-1-definition-and-properties-of-the-mvn.html"><a href="7-1-definition-and-properties-of-the-mvn.html#transformations"><i class="fa fa-check"></i><b>7.1.1</b> Transformations</a></li>
<li class="chapter" data-level="7.1.2" data-path="7-1-definition-and-properties-of-the-mvn.html"><a href="7-1-definition-and-properties-of-the-mvn.html#moment-generating-functions"><i class="fa fa-check"></i><b>7.1.2</b> Moment Generating Functions</a></li>
<li class="chapter" data-level="7.1.3" data-path="7-1-definition-and-properties-of-the-mvn.html"><a href="7-1-definition-and-properties-of-the-mvn.html#sampling-results-for-the-mvn"><i class="fa fa-check"></i><b>7.1.3</b> Sampling results for the MVN</a></li>
</ul></li>
<li class="chapter" data-level="7.2" data-path="7-2-the-wishart-distribution.html"><a href="7-2-the-wishart-distribution.html"><i class="fa fa-check"></i><b>7.2</b> The Wishart distribution</a><ul>
<li class="chapter" data-level="7.2.1" data-path="7-2-the-wishart-distribution.html"><a href="7-2-the-wishart-distribution.html#properties-2"><i class="fa fa-check"></i><b>7.2.1</b> Properties</a></li>
<li class="chapter" data-level="7.2.2" data-path="7-2-the-wishart-distribution.html"><a href="7-2-the-wishart-distribution.html#cochrans-theorem"><i class="fa fa-check"></i><b>7.2.2</b> Cochran’s theorem</a></li>
</ul></li>
<li class="chapter" data-level="7.3" data-path="7-3-hotellings-t2-distribution.html"><a href="7-3-hotellings-t2-distribution.html"><i class="fa fa-check"></i><b>7.3</b> Hotelling’s <span class="math inline">\(T^2\)</span> distribution</a></li>
<li class="chapter" data-level="7.4" data-path="7-4-inference-based-on-the-mvn.html"><a href="7-4-inference-based-on-the-mvn.html"><i class="fa fa-check"></i><b>7.4</b> Inference based on the MVN</a><ul>
<li class="chapter" data-level="7.4.1" data-path="7-4-inference-based-on-the-mvn.html"><a href="7-4-inference-based-on-the-mvn.html#onesampleSigma"><i class="fa fa-check"></i><b>7.4.1</b> <span class="math inline">\(\boldsymbol \Sigma\)</span> known</a></li>
<li class="chapter" data-level="7.4.2" data-path="7-4-inference-based-on-the-mvn.html"><a href="7-4-inference-based-on-the-mvn.html#onesample"><i class="fa fa-check"></i><b>7.4.2</b> <span class="math inline">\(\boldsymbol \Sigma\)</span> unknown: 1 sample</a></li>
<li class="chapter" data-level="7.4.3" data-path="7-4-inference-based-on-the-mvn.html"><a href="7-4-inference-based-on-the-mvn.html#boldsymbol-sigma-unknown-2-samples"><i class="fa fa-check"></i><b>7.4.3</b> <span class="math inline">\(\boldsymbol \Sigma\)</span> unknown: 2 samples</a></li>
</ul></li>
<li class="chapter" data-level="7.5" data-path="7-5-exercises-4.html"><a href="7-5-exercises-4.html"><i class="fa fa-check"></i><b>7.5</b> Exercises</a></li>
<li class="chapter" data-level="7.6" data-path="7-6-computer-tasks-3.html"><a href="7-6-computer-tasks-3.html"><i class="fa fa-check"></i><b>7.6</b> Computer tasks</a></li>
</ul></li>
<li class="divider"></li>
<li> <a href="https://rich-d-wilkinson.github.io/teaching.html" target="blank">University of Nottingham</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Multivariate Statistics</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="properties" class="section level2">
<h2><span class="header-section-number">5.3</span> Properties</h2>
<p>The CC variables have a mean of zero. Their correlation structure is given in the following proposition:</p>

<div class="proposition">
<span id="prp:ccavar" class="proposition"><strong>Proposition 5.5  </strong></span>Assume that <span class="math inline">\(\boldsymbol S_{\boldsymbol x\boldsymbol x}\)</span> and <span class="math inline">\(\boldsymbol S_{\boldsymbol y\boldsymbol y}\)</span> both have full rank. Then for <span class="math inline">\(1 \leq k,\ell \leq t\)</span>,
<span class="math display">\[
{\mathbb{C}\operatorname{or}}(\eta_k,  \psi_{\ell})=\begin{cases} \sigma_k &amp;\text{if} \quad k=\ell\\
0 &amp; \text{if} \quad k \neq \ell, \end{cases}
\]</span>
and
<span class="math display">\[{\mathbb{C}\operatorname{or}}(\eta_j, \eta_k)= {\mathbb{C}\operatorname{or}}(\psi_j, \psi_k)=\begin{cases}
1 &amp;\mbox{ if } j=k\\
0 &amp;\mbox{otherwise}
\end{cases}
\]</span>
</div>


<div class="proof">
 <span class="proof"><em>Proof. </em></span> You will prove this in the exercises.
</div>

<div id="connection-with-linear-regression-when-q1" class="section level3">
<h3><span class="header-section-number">5.3.1</span> Connection with linear regression when <span class="math inline">\(q=1\)</span></h3>
<p>Although CCA is clearly a different technique to linear regression, it turns out that when either <span class="math inline">\(\dim \boldsymbol x=p=1\)</span> or <span class="math inline">\(\dim \boldsymbol y=q=1\)</span>, there is a close connection between the two approaches.</p>
<p>Consider the case <span class="math inline">\(q=1\)</span> and <span class="math inline">\(p&gt;1\)</span>. Hence there is only a single <span class="math inline">\(y\)</span>-variable but we still have <span class="math inline">\(p&gt;1\)</span> <span class="math inline">\(x\)</span>-variables.</p>
<p>Let’s make the following assumptions:</p>
<ol style="list-style-type: decimal">
<li>The <span class="math inline">\(\boldsymbol x_i\)</span> have been centred so that <span class="math inline">\(\bar{\boldsymbol x}={\mathbf 0}_p\)</span>, the zero vector.</li>
<li>The covariance matrix for the <span class="math inline">\(x\)</span>-variables, <span class="math inline">\(\boldsymbol S_{\boldsymbol x\boldsymbol x}\)</span>, has full rank <span class="math inline">\(p\)</span>.</li>
</ol>
<p>The first assumption means that
<span class="math display">\[\boldsymbol S_{xx}=\frac{1}{n}\boldsymbol X^\top \boldsymbol X\quad \mbox{and}\quad \boldsymbol S_{xy}=\frac{1}{n}\boldsymbol X^\top \boldsymbol y,\]</span> and the second means that <span class="math inline">\((\boldsymbol X^\top \boldsymbol X)^{-1}\)</span> exists.</p>
<p>Since <span class="math inline">\(q=1\)</span>, the matrix we decompose in CCA
<span class="math display">\[
\boldsymbol Q=\boldsymbol S_{\boldsymbol x\boldsymbol x}^{-1/2} \boldsymbol S_{\boldsymbol xy}\boldsymbol S_{yy}^{-1/2}
\]</span>
is a <span class="math inline">\(p \times 1\)</span> vector. Consequently, its
SVD is just
<span class="math display">\[
\boldsymbol Q=\sigma_1 \boldsymbol u_1,
\]</span>
where
<span class="math display">\[
\sigma_1=\vert \vert \boldsymbol Q\vert \vert_F = (\boldsymbol Q^\top \boldsymbol Q)^{\frac{1}{2}} \qquad \text{and} \qquad \boldsymbol u_1=\boldsymbol Q/\vert \vert \boldsymbol Q\vert \vert_F.
\]</span></p>
<p>Consequently, the first canoncial correlation vector for <span class="math inline">\(\boldsymbol x\)</span> is
<span class="math display">\[\begin{align*}
\boldsymbol a&amp;=\boldsymbol S_{\boldsymbol x\boldsymbol x}^{-1/2}\boldsymbol u_1 =\boldsymbol S_{\boldsymbol x\boldsymbol x}^{-1/2} \frac{\boldsymbol Q}{||\boldsymbol Q||_F}\\
&amp;=\boldsymbol S_{\boldsymbol x\boldsymbol x}^{-1/2} \frac{1}{\vert \vert \boldsymbol S_{\boldsymbol x\boldsymbol x}^{-1/2}\boldsymbol S_{\boldsymbol xy}S_{yy}^{-1/2}\vert \vert_F}\boldsymbol S_{\boldsymbol x\boldsymbol x}^{-1/2}\boldsymbol S_{\boldsymbol x\boldsymbol y}S_{yy}^{-1/2}\\
&amp;=\frac{1}{\vert \vert \boldsymbol S_{\boldsymbol x\boldsymbol x}^{-1/2}\boldsymbol S_{\boldsymbol xy}\vert \vert_F}\boldsymbol S_{\boldsymbol x\boldsymbol x}^{-1}\boldsymbol S_{\boldsymbol xy}\\
&amp;= c (\boldsymbol X^\top \boldsymbol X)^{-1}\boldsymbol X^\top \boldsymbol y
\end{align*}\]</span>
where $c $ is a scalar constant.</p>
<p>Thus, we can see that the first canonical correlation vector <span class="math inline">\(\boldsymbol a\)</span> is a scalar multiple of
<span class="math display">\[
\hat{\pmb \beta}=\left ( \boldsymbol X^\top \boldsymbol X\right )^{-1} \boldsymbol X^\top \boldsymbol y,
\]</span>
the classical least squares estimator. Therefore the least squares estimator <span class="math inline">\(\hat{\pmb \beta}\)</span> solves <a href="5-1-cca1.html#eq:opt26">(5.2)</a>. However, it does not usually solve the constrained optimisation problem <a href="5-1-cca1.html#eq:opt27a">(5.4)</a> because typically <span class="math inline">\(\hat{\pmb \beta}^\top \boldsymbol S_{\boldsymbol x\boldsymbol x}\hat{\pmb \beta} \not= 1\)</span>, so that the constraint in Equation <a href="5-1-cca1.html#eq:opt27a">(5.4)</a> will not be satisfied.</p>
</div>
<div id="invarianceequivariance-properties-of-cca" class="section level3">
<h3><span class="header-section-number">5.3.2</span> Invariance/equivariance properties of CCA</h3>
<p>Suppose we apply orthogonal transformations and translations to the <span class="math inline">\(\boldsymbol x_i\)</span> and the <span class="math inline">\(\boldsymbol y_i\)</span> of the form
<span class="math display" id="eq:transformations">\[\begin{equation}
{\mathbf h}_i={\mathbf T}\boldsymbol x_i + {\pmb \mu} \qquad \text{and} \qquad {\mathbf k}_i={\mathbf R}\boldsymbol y_i +{\pmb \eta},
\qquad i=1,\ldots , n,
\tag{5.12}
\end{equation}\]</span>
where <span class="math inline">\(\mathbf T\)</span> (<span class="math inline">\(p \times p\)</span>) and <span class="math inline">\(\mathbf R\)</span> (<span class="math inline">\(q \times q\)</span>) are orthogonal matrices, and <span class="math inline">\(\pmb \mu\)</span> (<span class="math inline">\(p \times 1\)</span>) and
<span class="math inline">\(\pmb \eta\)</span> (<span class="math inline">\(q \times 1\)</span>) are fixed vectors.</p>
<p>How do these transformations affect the CC analysis?</p>
<p>Firstly, since CCA depends only on sample covariance matrices, it follows that the translation vectors <span class="math inline">\(\pmb \mu\)</span> and <span class="math inline">\(\pmb \eta\)</span> have no effect on the analysis.</p>
<p>Secondly, let’s consider the effect of the rotations by an orthogonal matrix. We’ve seen that CCA in the original <span class="math inline">\(\boldsymbol x\)</span> and <span class="math inline">\(\boldsymbol y\)</span> coordinates depends on
<span class="math display" id="eq:Axy">\[\begin{equation}
\boldsymbol Q\equiv \boldsymbol Q_{\boldsymbol x\boldsymbol y}=\boldsymbol S_{\boldsymbol x\boldsymbol x}^{-1/2}\boldsymbol S_{\boldsymbol x\boldsymbol y}\boldsymbol S_{\boldsymbol y\boldsymbol y}^{-1/2}.
\tag{5.13}
\end{equation}\]</span>
In the new coordinates we have
<span class="math display">\[
\tilde{\boldsymbol S}_{\boldsymbol h\boldsymbol h}={\mathbf T} \boldsymbol S_{\boldsymbol x\boldsymbol x}{\mathbf T}^\top, \qquad \tilde{\boldsymbol S}_{\boldsymbol k\boldsymbol k}={\mathbf R}\boldsymbol S_{\boldsymbol y\boldsymbol y}{\mathbf R}^\top,
\]</span>
<span class="math display">\[
\tilde{\boldsymbol S}_{\boldsymbol h\boldsymbol k}={\mathbf T}\boldsymbol S_{\boldsymbol x\boldsymbol y}{\mathbf R}^\top =
\tilde{\boldsymbol S}_{\boldsymbol k\boldsymbol h}^\top,
\]</span>
where here and below, a tilde above a symbol is used to indicate that the corresponding term is defined in terms of the new <span class="math inline">\(\boldsymbol h\)</span>, <span class="math inline">\(\boldsymbol k\)</span> coordinates, rather
than the old <span class="math inline">\(\boldsymbol x\)</span>, <span class="math inline">\(\boldsymbol y\)</span> coordinates.
Due to the fact that <span class="math inline">\(\mathbf T\)</span> and <span class="math inline">\(\mathbf R\)</span> are orthogonal,
<span class="math display">\[
\tilde{\boldsymbol S}_{\boldsymbol b\boldsymbol h}^{ 1/2}={\mathbf T}\boldsymbol S_{\boldsymbol x\boldsymbol x}^{ 1/2}{\mathbf T}^\top, \qquad
\tilde{\boldsymbol S}_{\boldsymbol h\boldsymbol h}^{ -1/2}={\mathbf T}\boldsymbol S_{\boldsymbol x\boldsymbol x}^{ -1/2}{\mathbf T}^\top
\]</span>
<span class="math display">\[
\tilde{\boldsymbol S}_{\boldsymbol k\boldsymbol k}^{ 1/2}={\mathbf R}\boldsymbol S_{\boldsymbol y\boldsymbol y}^{ 1/2}{\mathbf R}^\top \qquad \text{and} \qquad
\tilde{\boldsymbol S}_{\boldsymbol k\boldsymbol k}^{ -1/2}={\mathbf R}\boldsymbol S_{\boldsymbol y\boldsymbol y}^{- 1/2}{\mathbf R}^\top.
\]</span>
The analogue of <a href="5-3-properties.html#eq:Axy">(5.13)</a> in the new coordinates is given by
<span class="math display">\[\begin{align*}
\tilde{\boldsymbol Q}_{\mathbf h k}&amp;=\tilde{\boldsymbol S}_{\boldsymbol h\boldsymbol h}^{-1/2}\tilde{\boldsymbol S}_{\mathbf h k}\tilde{\boldsymbol S}_{\boldsymbol k\boldsymbol k}^{-1/2}\\
&amp;={\mathbf T} \boldsymbol S_{\boldsymbol x\boldsymbol x}^{-1/2}{\mathbf T}^\top {\mathbf T}\boldsymbol S_{\boldsymbol x\boldsymbol y}{\mathbf R}^\top {\mathbf R}\boldsymbol S_{\boldsymbol y\boldsymbol y}^{-1/2}{\mathbf R}^\top\\
&amp;={\mathbf T}\boldsymbol S_{\boldsymbol x\boldsymbol x}^{-1/2}\boldsymbol S_{\boldsymbol x\boldsymbol y}\boldsymbol S_{\boldsymbol y\boldsymbol y}^{-1/2}{\mathbf R}^\top\\
&amp;={\mathbf T} \boldsymbol Q_{\boldsymbol x\boldsymbol y}{\mathbf R}^\top.
\end{align*}\]</span>
So, again using the fact that <span class="math inline">\(\mathbf T\)</span> and <span class="math inline">\(\mathbf R\)</span> are orthogonal matrices, if <span class="math inline">\(\boldsymbol Q_{\boldsymbol x\boldsymbol y}\)</span> has SVD <span class="math inline">\(\sum_{j=1}^t \sigma_j {\mathbf u}_j {\mathbf v}_j^\top\)</span>, then <span class="math inline">\(\tilde{\boldsymbol Q}_{\boldsymbol h\boldsymbol k}\)</span> has SVD
<span class="math display">\[\begin{align*}
\tilde{\boldsymbol Q}_{\boldsymbol h\boldsymbol k}&amp;={\mathbf T }\boldsymbol Q_{\boldsymbol x\boldsymbol y}{\mathbf R}^\top
={\mathbf T} \left ( \sum_{j=1}^t \sigma_j {\mathbf u}_j {\mathbf v}_j^\top \right){\mathbf R}^\top\\
&amp;=\sum_{j=1}^t \sigma_j {\mathbf T}{\mathbf u}_j {\mathbf v}_j^\top {\mathbf R}^\top
=\sum_{j=1}^t \sigma_j \left ( {\mathbf T} {\mathbf u}_j \right )\left ({\mathbf R}{\mathbf v}_j  \right )^\top
=\sum_{j=1}^t \sigma_j \tilde{\boldsymbol u}_j \tilde{\mathbf v}_j^\top,
\end{align*}\]</span>
where, for <span class="math inline">\(j=1, \ldots,t\)</span>, the <span class="math inline">\(\tilde{\boldsymbol u}_j={\mathbf T}\boldsymbol u_j\)</span> are mutually orthogonal unit vectors,
and the <span class="math inline">\(\tilde{\mathbf v}_j={\mathbf R}{\mathbf v}_j\)</span> are also mutually orthogonal unit vectors.</p>
<p>Consequently, <span class="math inline">\(\tilde{\boldsymbol Q}_{\mathbf h k}\)</span> has the same singular values as <span class="math inline">\(\boldsymbol Q_{\boldsymbol x\boldsymbol y}\)</span>, namely <span class="math inline">\(\sigma_1, \ldots , \sigma_t\)</span> in both cases, and so the canonical correlation coefficients are invariant with respect to the transformations <a href="5-3-properties.html#eq:transformations">(5.12)</a>. Moreover, since the optimal linear combinations for the <span class="math inline">\(j\)</span>th CC in the original coordinates are given by <span class="math inline">\(\boldsymbol a_j =\boldsymbol S_{\boldsymbol x\boldsymbol x}^{-1/2}{\mathbf u}_j\)</span> and <span class="math inline">\(\boldsymbol b_j=\boldsymbol S_{\boldsymbol y\boldsymbol y}^{-1/2}{\mathbf v}_j\)</span>, the optimal linear combinations in the new coordinates are given by
<span class="math display">\[\begin{align*}
\tilde{\boldsymbol a}_{j}&amp;=\boldsymbol S_{\boldsymbol h\boldsymbol h}^{-1/2}{\mathbf T}{\mathbf u}_j\\
&amp;={\mathbf T}\boldsymbol S_{\boldsymbol x\boldsymbol x}^{-1/2}{\mathbf T}^\top {\mathbf T}{\mathbf u}_j\\
&amp;={\mathbf T}\boldsymbol S_{\boldsymbol x\boldsymbol x}^{-1/2}{\mathbf u}_j \\
&amp;={\mathbf T}\boldsymbol a_{j},
\end{align*}\]</span>
and a similar argument shows that <span class="math inline">\(\tilde{\boldsymbol b}_{j}={\mathbf R}\boldsymbol b_{j}\)</span>. So under transformations <a href="5-3-properties.html#eq:transformations">(5.12)</a>,
the optimal vectors <span class="math inline">\(\boldsymbol a_{j}\)</span> and <span class="math inline">\(\boldsymbol b_{j}\)</span> transform in an equivariant manner to <span class="math inline">\(\tilde{\boldsymbol a}_{j}\)</span> and <span class="math inline">\(\tilde{\boldsymbol b}_{j}\)</span>, respectively, <span class="math inline">\(j=1, \ldots , t\)</span>.</p>
<p>If either of <span class="math inline">\(\mathbf T\)</span> or <span class="math inline">\(\mathbf R\)</span> in <a href="5-3-properties.html#eq:transformations">(5.12)</a> is not an orthogonal matrix then the singular values are not invariant and the CC vectors do not transform in an equivariant manner.</p>
<!--## Testing for zero canonical correlation coefficients

So far in Part II of this module we have not considered formal statistical inference (e.g. hypothesis testing, construction of confidence regions).  Inference in various multivariate settings is considered in Part III.  However, before moving on, we briefly explain how to perform tests for zero correlations in the CCA setting, under the assumption that the $\bz_i∂ = (\bx_i^\top , \by_i^\top)^\top$ are IID multivariate normal.

As previously, suppose that the $\bx_i$ are $p \times 1$ vectors and the $\by_i$ are $q \times 1$ vectors and the sample size, i.e.  the number of $\bz_i$ vectors, is $n$.  Let $\bSigma_{\bx \by} =\text{Cov}(\bx,\by)$ denote the population cross-covariance matrix as before and consider the null hypothesis
$$
H_0: \, \bSigma_{\bx \by}={\mathbf 0}_{p,q},
$$
i.e. $\bSigma_{\bx \by}$ is the $p \times q$ matrix of zeros.  Let $H_A$ denote the general alternative
$$
H_A:\, \bSigma_{\bx \by} \quad *unrestricted*.
$$

Then the large-sample log-likelihood ratio test statistic for testing $H_0$ versus $H_A$ is as follows:
$$
W_0=-\left \{n-\frac{1}{2}(p+q+3)  \right \}\sum_{j=1}^{\min(p,q)} \log (1-\sigma_j^2),
$$
where $\sigma_1\geq \sigma_2 \cdots \geq \sigma_{\min(p,q)} \geq 0$ are the sample canonical correlations.
Moreover, when $n$ is large, $W_0$ is approximately $\chi_{pq}^2$ under $H_0$,  and $H_0$ should be rejected
when $W_0$ is sufficiently large.

We now consider a test concerning the rank of $\bSigma_{\bx \by}$.  For $0 \leq t <\min(p,q)$, consider  the hypothesis:
$$
H_t: \,   \text{at most $t$ of the CC coefficients are non-zero}.
$$
It turns out there is a similar statistic to $W_0$ above, for testing $H_t$ against $H_A$, defined by
$$
W_t=-\left \{n-\frac{1}{2}(p+q+3)  \right \}\sum_{j=t+1}^{\min(p,q)} \log (1-\sigma_j^2),
$$
where, under $H_t$ with $n$ large, $W_t$ is approximately $\chi_{(p-t)(q-t)}^2$.  Also, we reject
$H_t$ when $W_t$ is sufficiently large.


**Example \@ref(exm:prem) (continued)**.
  Here $p=q=2$, $n=20$ and $\sigma_1=0.974$ and $\sigma_2=0.508$.
So we should refer $W_0$ to $\chi_4^2$ and refer $W_1$ to $\chi_1^2$.  Here, $W_0=53.92$ and $W_1=4.92$.  So hypothesis $H_0$ is strongly rejected, with $p$-value $<\,<0.001$.  In contrast, $H_1$ is rejected at the $0.05$ level but is not rejected at the $0.01$ level.  So there is only moderate evidence that the 2nd CC coefficient is non-zero.

-->
<!--https://stats.idre.ucla.edu/r/dae/canonical-correlation-analysis/#:~:text=Canonical%20correlation%20analysis%20is%20used,among%20two%20sets%20of%20variables.&text=Canonical%20correlation%20analysis%20determines%20a,both%20within%20and%20between%20sets.


```r
mm <- read.csv("https://stats.idre.ucla.edu/stat/data/mmreg.csv")
colnames(mm) <- c("Control", "Concept", "Motivation", "Read", "Write", "Math",
"Science", "Sex")
psych <- mm[,1:3]
acad <- mm[,4:8]
library(CCA)
matcor(psych, acad)
```

```
## $Xcor
##              Control   Concept Motivation
## Control    1.0000000 0.1711878  0.2451323
## Concept    0.1711878 1.0000000  0.2885707
## Motivation 0.2451323 0.2885707  1.0000000
## 
## $Ycor
##                Read     Write       Math    Science         Sex
## Read     1.00000000 0.6285909  0.6792757  0.6906929 -0.04174278
## Write    0.62859089 1.0000000  0.6326664  0.5691498  0.24433183
## Math     0.67927568 0.6326664  1.0000000  0.6495261 -0.04821830
## Science  0.69069291 0.5691498  0.6495261  1.0000000 -0.13818587
## Sex     -0.04174278 0.2443318 -0.0482183 -0.1381859  1.00000000
## 
## $XYcor
##              Control     Concept Motivation        Read      Write       Math
## Control    1.0000000  0.17118778 0.24513227  0.37356505 0.35887684  0.3372690
## Concept    0.1711878  1.00000000 0.28857075  0.06065584 0.01944856  0.0535977
## Motivation 0.2451323  0.28857075 1.00000000  0.21060992 0.25424818  0.1950135
## Read       0.3735650  0.06065584 0.21060992  1.00000000 0.62859089  0.6792757
## Write      0.3588768  0.01944856 0.25424818  0.62859089 1.00000000  0.6326664
## Math       0.3372690  0.05359770 0.19501347  0.67927568 0.63266640  1.0000000
## Science    0.3246269  0.06982633 0.11566948  0.69069291 0.56914983  0.6495261
## Sex        0.1134108 -0.12595132 0.09810277 -0.04174278 0.24433183 -0.0482183
##                Science         Sex
## Control     0.32462694  0.11341075
## Concept     0.06982633 -0.12595132
## Motivation  0.11566948  0.09810277
## Read        0.69069291 -0.04174278
## Write       0.56914983  0.24433183
## Math        0.64952612 -0.04821830
## Science     1.00000000 -0.13818587
## Sex        -0.13818587  1.00000000
```

```r
cc1 <- cc(psych, acad)
```
-->
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="5-2-the-full-set-of-canonical-correlations.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="5-4-exercises-2.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["MultivariateStatistics.pdf", "MultivariateStatistics.epub"],
"toc": {
"collapse": "section"
},
"pandoc_args": "--top-level-division=[chapter|part]"
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
