<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>4.1 PCA: an informal introduction | Multivariate Statistics</title>
  <meta name="description" content="The lecture notes for MATH3030/4068: Multivariate Analysis / Applied Multivariate Statistics" />
  <meta name="generator" content="bookdown 0.21 and GitBook 2.6.7" />

  <meta property="og:title" content="4.1 PCA: an informal introduction | Multivariate Statistics" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="The lecture notes for MATH3030/4068: Multivariate Analysis / Applied Multivariate Statistics" />
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="4.1 PCA: an informal introduction | Multivariate Statistics" />
  
  <meta name="twitter:description" content="The lecture notes for MATH3030/4068: Multivariate Analysis / Applied Multivariate Statistics" />
  

<meta name="author" content="Prof. Richard Wilkinson" />



  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="4-pca.html"/>
<link rel="next" href="4-2-pca-a-formal-description-with-proofs.html"/>
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />









<script src="libs/kePrint-0.0.1/kePrint.js"></script>
<link href="libs/lightable-0.0.1/lightable.css" rel="stylesheet" />


<style type="text/css">
a.sourceLine { display: inline-block; line-height: 1.25; }
a.sourceLine { pointer-events: none; color: inherit; text-decoration: inherit; }
a.sourceLine:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode { white-space: pre; position: relative; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
code.sourceCode { white-space: pre-wrap; }
a.sourceLine { text-indent: -1em; padding-left: 1em; }
}
pre.numberSource a.sourceLine
  { position: relative; left: -4em; }
pre.numberSource a.sourceLine::before
  { content: attr(data-line-number);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; pointer-events: all; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {  }
@media screen {
a.sourceLine::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Applied multivariate statistics</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Introduction</a></li>
<li class="chapter" data-level="" data-path="part-i-prerequisites.html"><a href="part-i-prerequisites.html"><i class="fa fa-check"></i>PART I: Prerequisites</a></li>
<li class="chapter" data-level="1" data-path="1-stat-prelim.html"><a href="1-stat-prelim.html"><i class="fa fa-check"></i><b>1</b> Statistical Preliminaries</a><ul>
<li class="chapter" data-level="1.1" data-path="1-1-notation.html"><a href="1-1-notation.html"><i class="fa fa-check"></i><b>1.1</b> Notation</a><ul>
<li class="chapter" data-level="1.1.1" data-path="1-1-notation.html"><a href="1-1-notation.html#example-datasets"><i class="fa fa-check"></i><b>1.1.1</b> Example datasets</a></li>
<li class="chapter" data-level="1.1.2" data-path="1-1-notation.html"><a href="1-1-notation.html#aims-of-multivariate-data-analysis"><i class="fa fa-check"></i><b>1.1.2</b> Aims of multivariate data analysis</a></li>
</ul></li>
<li class="chapter" data-level="1.2" data-path="1-2-exploratory-data-analysis-eda.html"><a href="1-2-exploratory-data-analysis-eda.html"><i class="fa fa-check"></i><b>1.2</b> Exploratory data analysis (EDA)</a><ul>
<li class="chapter" data-level="1.2.1" data-path="1-2-exploratory-data-analysis-eda.html"><a href="1-2-exploratory-data-analysis-eda.html#data-visualization"><i class="fa fa-check"></i><b>1.2.1</b> Data visualization</a></li>
<li class="chapter" data-level="1.2.2" data-path="1-2-exploratory-data-analysis-eda.html"><a href="1-2-exploratory-data-analysis-eda.html#summary-statistics"><i class="fa fa-check"></i><b>1.2.2</b> Summary statistics</a></li>
</ul></li>
<li class="chapter" data-level="1.3" data-path="1-3-randvec.html"><a href="1-3-randvec.html"><i class="fa fa-check"></i><b>1.3</b> Random vectors and matrices</a><ul>
<li class="chapter" data-level="1.3.1" data-path="1-3-randvec.html"><a href="1-3-randvec.html#estimators"><i class="fa fa-check"></i><b>1.3.1</b> Estimators</a></li>
</ul></li>
<li class="chapter" data-level="1.4" data-path="1-4-computer-tasks.html"><a href="1-4-computer-tasks.html"><i class="fa fa-check"></i><b>1.4</b> Computer tasks</a></li>
<li class="chapter" data-level="1.5" data-path="1-5-exercises.html"><a href="1-5-exercises.html"><i class="fa fa-check"></i><b>1.5</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="2-linalg-prelim.html"><a href="2-linalg-prelim.html"><i class="fa fa-check"></i><b>2</b> Review of linear algebra</a><ul>
<li class="chapter" data-level="2.1" data-path="2-1-linalg-basics.html"><a href="2-1-linalg-basics.html"><i class="fa fa-check"></i><b>2.1</b> Basics</a><ul>
<li class="chapter" data-level="2.1.1" data-path="2-1-linalg-basics.html"><a href="2-1-linalg-basics.html#notation-1"><i class="fa fa-check"></i><b>2.1.1</b> Notation</a></li>
<li class="chapter" data-level="2.1.2" data-path="2-1-linalg-basics.html"><a href="2-1-linalg-basics.html#elementary-matrix-operations"><i class="fa fa-check"></i><b>2.1.2</b> Elementary matrix operations</a></li>
<li class="chapter" data-level="2.1.3" data-path="2-1-linalg-basics.html"><a href="2-1-linalg-basics.html#special-matrices"><i class="fa fa-check"></i><b>2.1.3</b> Special matrices</a></li>
<li class="chapter" data-level="2.1.4" data-path="2-1-linalg-basics.html"><a href="2-1-linalg-basics.html#vectordiff"><i class="fa fa-check"></i><b>2.1.4</b> Vector Differentiation</a></li>
</ul></li>
<li class="chapter" data-level="2.2" data-path="2-2-linalg-vecspaces.html"><a href="2-2-linalg-vecspaces.html"><i class="fa fa-check"></i><b>2.2</b> Vector spaces</a><ul>
<li class="chapter" data-level="2.2.1" data-path="2-2-linalg-vecspaces.html"><a href="2-2-linalg-vecspaces.html#linear-independence"><i class="fa fa-check"></i><b>2.2.1</b> Linear independence</a></li>
<li class="chapter" data-level="2.2.2" data-path="2-2-linalg-vecspaces.html"><a href="2-2-linalg-vecspaces.html#colsspace"><i class="fa fa-check"></i><b>2.2.2</b> Row and column spaces</a></li>
<li class="chapter" data-level="2.2.3" data-path="2-2-linalg-vecspaces.html"><a href="2-2-linalg-vecspaces.html#linear-transformations"><i class="fa fa-check"></i><b>2.2.3</b> Linear transformations</a></li>
</ul></li>
<li class="chapter" data-level="2.3" data-path="2-3-linalg-innerprod.html"><a href="2-3-linalg-innerprod.html"><i class="fa fa-check"></i><b>2.3</b> Inner product spaces</a><ul>
<li class="chapter" data-level="2.3.1" data-path="2-3-linalg-innerprod.html"><a href="2-3-linalg-innerprod.html#normed"><i class="fa fa-check"></i><b>2.3.1</b> Distances, and angles</a></li>
<li class="chapter" data-level="2.3.2" data-path="2-3-linalg-innerprod.html"><a href="2-3-linalg-innerprod.html#orthogonal-matrices"><i class="fa fa-check"></i><b>2.3.2</b> Orthogonal matrices</a></li>
<li class="chapter" data-level="2.3.3" data-path="2-3-linalg-innerprod.html"><a href="2-3-linalg-innerprod.html#projection-matrix"><i class="fa fa-check"></i><b>2.3.3</b> Projections</a></li>
</ul></li>
<li class="chapter" data-level="2.4" data-path="2-4-centering-matrix.html"><a href="2-4-centering-matrix.html"><i class="fa fa-check"></i><b>2.4</b> The Centering Matrix</a></li>
<li class="chapter" data-level="2.5" data-path="2-5-tasks-ch2.html"><a href="2-5-tasks-ch2.html"><i class="fa fa-check"></i><b>2.5</b> Computer tasks</a></li>
<li class="chapter" data-level="2.6" data-path="2-6-exercises-ch2.html"><a href="2-6-exercises-ch2.html"><i class="fa fa-check"></i><b>2.6</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="3-linalg-decomp.html"><a href="3-linalg-decomp.html"><i class="fa fa-check"></i><b>3</b> Matrix decompositions</a><ul>
<li class="chapter" data-level="3.1" data-path="3-1-matrix-matrix.html"><a href="3-1-matrix-matrix.html"><i class="fa fa-check"></i><b>3.1</b> Matrix-matrix products</a></li>
<li class="chapter" data-level="3.2" data-path="3-2-spectraleigen-decomposition.html"><a href="3-2-spectraleigen-decomposition.html"><i class="fa fa-check"></i><b>3.2</b> Spectral/eigen decomposition</a><ul>
<li class="chapter" data-level="3.2.1" data-path="3-2-spectraleigen-decomposition.html"><a href="3-2-spectraleigen-decomposition.html#eigenvalues-and-eigenvectors"><i class="fa fa-check"></i><b>3.2.1</b> Eigenvalues and eigenvectors</a></li>
<li class="chapter" data-level="3.2.2" data-path="3-2-spectraleigen-decomposition.html"><a href="3-2-spectraleigen-decomposition.html#spectral-decomposition"><i class="fa fa-check"></i><b>3.2.2</b> Spectral decomposition</a></li>
<li class="chapter" data-level="3.2.3" data-path="3-2-spectraleigen-decomposition.html"><a href="3-2-spectraleigen-decomposition.html#matrixroots"><i class="fa fa-check"></i><b>3.2.3</b> Matrix square roots</a></li>
</ul></li>
<li class="chapter" data-level="3.3" data-path="3-3-linalg-SVD.html"><a href="3-3-linalg-SVD.html"><i class="fa fa-check"></i><b>3.3</b> Singular Value Decomposition (SVD)</a><ul>
<li class="chapter" data-level="3.3.1" data-path="3-3-linalg-SVD.html"><a href="3-3-linalg-SVD.html#examples"><i class="fa fa-check"></i><b>3.3.1</b> Examples</a></li>
</ul></li>
<li class="chapter" data-level="3.4" data-path="3-4-svdopt.html"><a href="3-4-svdopt.html"><i class="fa fa-check"></i><b>3.4</b> SVD optimization results</a></li>
<li class="chapter" data-level="3.5" data-path="3-5-low-rank-approximation.html"><a href="3-5-low-rank-approximation.html"><i class="fa fa-check"></i><b>3.5</b> Low-rank approximation</a><ul>
<li class="chapter" data-level="3.5.1" data-path="3-5-low-rank-approximation.html"><a href="3-5-low-rank-approximation.html#matrix-norms"><i class="fa fa-check"></i><b>3.5.1</b> Matrix norms</a></li>
<li class="chapter" data-level="3.5.2" data-path="3-5-low-rank-approximation.html"><a href="3-5-low-rank-approximation.html#eckart-young-mirsky-theorem"><i class="fa fa-check"></i><b>3.5.2</b> Eckart-Young-Mirsky Theorem</a></li>
<li class="chapter" data-level="3.5.3" data-path="3-5-low-rank-approximation.html"><a href="3-5-low-rank-approximation.html#example-image-compression"><i class="fa fa-check"></i><b>3.5.3</b> Example: image compression</a></li>
</ul></li>
<li class="chapter" data-level="3.6" data-path="3-6-tasks-ch3.html"><a href="3-6-tasks-ch3.html"><i class="fa fa-check"></i><b>3.6</b> Computer tasks</a></li>
<li class="chapter" data-level="3.7" data-path="3-7-exercises-ch3.html"><a href="3-7-exercises-ch3.html"><i class="fa fa-check"></i><b>3.7</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="part-ii-dimension-reduction-methods.html"><a href="part-ii-dimension-reduction-methods.html"><i class="fa fa-check"></i>PART II: Dimension reduction methods</a><ul>
<li class="chapter" data-level="" data-path="part-ii-dimension-reduction-methods.html"><a href="part-ii-dimension-reduction-methods.html#a-warning"><i class="fa fa-check"></i>A warning</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="4-pca.html"><a href="4-pca.html"><i class="fa fa-check"></i><b>4</b> Principal Component Analysis (PCA)</a><ul>
<li class="chapter" data-level="4.1" data-path="4-1-pca-an-informal-introduction.html"><a href="4-1-pca-an-informal-introduction.html"><i class="fa fa-check"></i><b>4.1</b> PCA: an informal introduction</a><ul>
<li class="chapter" data-level="4.1.1" data-path="4-1-pca-an-informal-introduction.html"><a href="4-1-pca-an-informal-introduction.html#notation-recap"><i class="fa fa-check"></i><b>4.1.1</b> Notation recap</a></li>
<li class="chapter" data-level="4.1.2" data-path="4-1-pca-an-informal-introduction.html"><a href="4-1-pca-an-informal-introduction.html#first-principal-component"><i class="fa fa-check"></i><b>4.1.2</b> First principal component</a></li>
<li class="chapter" data-level="4.1.3" data-path="4-1-pca-an-informal-introduction.html"><a href="4-1-pca-an-informal-introduction.html#second-principal-component"><i class="fa fa-check"></i><b>4.1.3</b> Second principal component</a></li>
<li class="chapter" data-level="4.1.4" data-path="4-1-pca-an-informal-introduction.html"><a href="4-1-pca-an-informal-introduction.html#geometric-interpretation-1"><i class="fa fa-check"></i><b>4.1.4</b> Geometric interpretation</a></li>
<li class="chapter" data-level="4.1.5" data-path="4-1-pca-an-informal-introduction.html"><a href="4-1-pca-an-informal-introduction.html#example"><i class="fa fa-check"></i><b>4.1.5</b> Example</a></li>
<li class="chapter" data-level="4.1.6" data-path="4-1-pca-an-informal-introduction.html"><a href="4-1-pca-an-informal-introduction.html#example-iris"><i class="fa fa-check"></i><b>4.1.6</b> Example: Iris</a></li>
</ul></li>
<li class="chapter" data-level="4.2" data-path="4-2-pca-a-formal-description-with-proofs.html"><a href="4-2-pca-a-formal-description-with-proofs.html"><i class="fa fa-check"></i><b>4.2</b> PCA: a formal description with proofs</a><ul>
<li class="chapter" data-level="4.2.1" data-path="4-2-pca-a-formal-description-with-proofs.html"><a href="4-2-pca-a-formal-description-with-proofs.html#properties-of-principal-components"><i class="fa fa-check"></i><b>4.2.1</b> Properties of principal components</a></li>
<li class="chapter" data-level="4.2.2" data-path="4-2-pca-a-formal-description-with-proofs.html"><a href="4-2-pca-a-formal-description-with-proofs.html#pca:football"><i class="fa fa-check"></i><b>4.2.2</b> Example: Football</a></li>
<li class="chapter" data-level="4.2.3" data-path="4-2-pca-a-formal-description-with-proofs.html"><a href="4-2-pca-a-formal-description-with-proofs.html#pcawithR"><i class="fa fa-check"></i><b>4.2.3</b> PCA based on <span class="math inline">\(\mathbf R\)</span> versus PCA based on <span class="math inline">\(\mathbf S\)</span></a></li>
<li class="chapter" data-level="4.2.4" data-path="4-2-pca-a-formal-description-with-proofs.html"><a href="4-2-pca-a-formal-description-with-proofs.html#population-pca"><i class="fa fa-check"></i><b>4.2.4</b> Population PCA</a></li>
<li class="chapter" data-level="4.2.5" data-path="4-2-pca-a-formal-description-with-proofs.html"><a href="4-2-pca-a-formal-description-with-proofs.html#pca-under-transformations-of-variables"><i class="fa fa-check"></i><b>4.2.5</b> PCA under transformations of variables</a></li>
</ul></li>
<li class="chapter" data-level="4.3" data-path="4-3-an-alternative-view-of-pca.html"><a href="4-3-an-alternative-view-of-pca.html"><i class="fa fa-check"></i><b>4.3</b> An alternative view of PCA</a><ul>
<li class="chapter" data-level="4.3.1" data-path="4-3-an-alternative-view-of-pca.html"><a href="4-3-an-alternative-view-of-pca.html#pca-mnist"><i class="fa fa-check"></i><b>4.3.1</b> Example: MNIST handwritten digits</a></li>
</ul></li>
<li class="chapter" data-level="4.4" data-path="4-4-pca-comptask.html"><a href="4-4-pca-comptask.html"><i class="fa fa-check"></i><b>4.4</b> Computer tasks</a></li>
<li class="chapter" data-level="4.5" data-path="4-5-exercises-1.html"><a href="4-5-exercises-1.html"><i class="fa fa-check"></i><b>4.5</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="5-cca.html"><a href="5-cca.html"><i class="fa fa-check"></i><b>5</b> Canonical Correlation Analysis (CCA)</a><ul>
<li class="chapter" data-level="5.1" data-path="5-1-cca1.html"><a href="5-1-cca1.html"><i class="fa fa-check"></i><b>5.1</b> The first pair of canonical variables</a><ul>
<li class="chapter" data-level="5.1.1" data-path="5-1-cca1.html"><a href="5-1-cca1.html#the-first-canonical-components"><i class="fa fa-check"></i><b>5.1.1</b> The first canonical components</a></li>
<li class="chapter" data-level="5.1.2" data-path="5-1-cca1.html"><a href="5-1-cca1.html#premcca"><i class="fa fa-check"></i><b>5.1.2</b> Example: Premier league football</a></li>
</ul></li>
<li class="chapter" data-level="5.2" data-path="5-2-the-full-set-of-canonical-correlations.html"><a href="5-2-the-full-set-of-canonical-correlations.html"><i class="fa fa-check"></i><b>5.2</b> The full set of canonical correlations</a><ul>
<li class="chapter" data-level="5.2.1" data-path="5-2-the-full-set-of-canonical-correlations.html"><a href="5-2-the-full-set-of-canonical-correlations.html#example-continued"><i class="fa fa-check"></i><b>5.2.1</b> Example continued</a></li>
</ul></li>
<li class="chapter" data-level="5.3" data-path="5-3-properties.html"><a href="5-3-properties.html"><i class="fa fa-check"></i><b>5.3</b> Properties</a><ul>
<li class="chapter" data-level="5.3.1" data-path="5-3-properties.html"><a href="5-3-properties.html#connection-with-linear-regression-when-q1"><i class="fa fa-check"></i><b>5.3.1</b> Connection with linear regression when <span class="math inline">\(q=1\)</span></a></li>
<li class="chapter" data-level="5.3.2" data-path="5-3-properties.html"><a href="5-3-properties.html#invarianceequivariance-properties-of-cca"><i class="fa fa-check"></i><b>5.3.2</b> Invariance/equivariance properties of CCA</a></li>
</ul></li>
<li class="chapter" data-level="5.4" data-path="5-4-computer-tasks-1.html"><a href="5-4-computer-tasks-1.html"><i class="fa fa-check"></i><b>5.4</b> Computer tasks</a></li>
<li class="chapter" data-level="5.5" data-path="5-5-exercises-2.html"><a href="5-5-exercises-2.html"><i class="fa fa-check"></i><b>5.5</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="6-mds.html"><a href="6-mds.html"><i class="fa fa-check"></i><b>6</b> Multidimensional Scaling (MDS)</a><ul>
<li class="chapter" data-level="6.1" data-path="6-1-classical-mds.html"><a href="6-1-classical-mds.html"><i class="fa fa-check"></i><b>6.1</b> Classical MDS</a><ul>
<li class="chapter" data-level="6.1.1" data-path="6-1-classical-mds.html"><a href="6-1-classical-mds.html#non-euclidean-distance-matrices"><i class="fa fa-check"></i><b>6.1.1</b> Non-Euclidean distance matrices</a></li>
<li class="chapter" data-level="6.1.2" data-path="6-1-classical-mds.html"><a href="6-1-classical-mds.html#principal-coordinate-analysis"><i class="fa fa-check"></i><b>6.1.2</b> Principal Coordinate Analysis</a></li>
</ul></li>
<li class="chapter" data-level="6.2" data-path="6-2-similarity.html"><a href="6-2-similarity.html"><i class="fa fa-check"></i><b>6.2</b> Similarity measures</a><ul>
<li class="chapter" data-level="6.2.1" data-path="6-2-similarity.html"><a href="6-2-similarity.html#binary-attributes"><i class="fa fa-check"></i><b>6.2.1</b> Binary attributes</a></li>
<li class="chapter" data-level="6.2.2" data-path="6-2-similarity.html"><a href="6-2-similarity.html#example-classical-mds-with-the-mnist-data"><i class="fa fa-check"></i><b>6.2.2</b> Example: Classical MDS with the MNIST data</a></li>
</ul></li>
<li class="chapter" data-level="6.3" data-path="6-3-non-metric-mds.html"><a href="6-3-non-metric-mds.html"><i class="fa fa-check"></i><b>6.3</b> Non-metric MDS</a></li>
<li class="chapter" data-level="6.4" data-path="6-4-exercises-3.html"><a href="6-4-exercises-3.html"><i class="fa fa-check"></i><b>6.4</b> Exercises</a></li>
<li class="chapter" data-level="6.5" data-path="6-5-computer-tasks-2.html"><a href="6-5-computer-tasks-2.html"><i class="fa fa-check"></i><b>6.5</b> Computer Tasks</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="part-iii-inference-using-the-multivariate-normal-distribution-mvn.html"><a href="part-iii-inference-using-the-multivariate-normal-distribution-mvn.html"><i class="fa fa-check"></i>Part III: Inference using the Multivariate Normal Distribution (MVN)</a></li>
<li class="chapter" data-level="7" data-path="7-multinormal.html"><a href="7-multinormal.html"><i class="fa fa-check"></i><b>7</b> The Multivariate Normal Distribution</a><ul>
<li class="chapter" data-level="7.1" data-path="7-1-definition-and-properties-of-the-mvn.html"><a href="7-1-definition-and-properties-of-the-mvn.html"><i class="fa fa-check"></i><b>7.1</b> Definition and Properties of the MVN</a><ul>
<li class="chapter" data-level="7.1.1" data-path="7-1-definition-and-properties-of-the-mvn.html"><a href="7-1-definition-and-properties-of-the-mvn.html#basics"><i class="fa fa-check"></i><b>7.1.1</b> Basics</a></li>
<li class="chapter" data-level="7.1.2" data-path="7-1-definition-and-properties-of-the-mvn.html"><a href="7-1-definition-and-properties-of-the-mvn.html#transformations"><i class="fa fa-check"></i><b>7.1.2</b> Transformations</a></li>
<li class="chapter" data-level="7.1.3" data-path="7-1-definition-and-properties-of-the-mvn.html"><a href="7-1-definition-and-properties-of-the-mvn.html#independence"><i class="fa fa-check"></i><b>7.1.3</b> Independence</a></li>
<li class="chapter" data-level="7.1.4" data-path="7-1-definition-and-properties-of-the-mvn.html"><a href="7-1-definition-and-properties-of-the-mvn.html#confidence-ellipses"><i class="fa fa-check"></i><b>7.1.4</b> Confidence ellipses</a></li>
<li class="chapter" data-level="7.1.5" data-path="7-1-definition-and-properties-of-the-mvn.html"><a href="7-1-definition-and-properties-of-the-mvn.html#sampling-results-for-the-mvn"><i class="fa fa-check"></i><b>7.1.5</b> Sampling results for the MVN</a></li>
</ul></li>
<li class="chapter" data-level="7.2" data-path="7-2-the-wishart-distribution.html"><a href="7-2-the-wishart-distribution.html"><i class="fa fa-check"></i><b>7.2</b> The Wishart distribution</a><ul>
<li class="chapter" data-level="7.2.1" data-path="7-2-the-wishart-distribution.html"><a href="7-2-the-wishart-distribution.html#properties-1"><i class="fa fa-check"></i><b>7.2.1</b> Properties</a></li>
<li class="chapter" data-level="7.2.2" data-path="7-2-the-wishart-distribution.html"><a href="7-2-the-wishart-distribution.html#cochrans-theorem"><i class="fa fa-check"></i><b>7.2.2</b> Cochran’s theorem</a></li>
</ul></li>
<li class="chapter" data-level="7.3" data-path="7-3-hotellings-t2-distribution.html"><a href="7-3-hotellings-t2-distribution.html"><i class="fa fa-check"></i><b>7.3</b> Hotelling’s <span class="math inline">\(T^2\)</span> distribution</a></li>
<li class="chapter" data-level="7.4" data-path="7-4-inference-based-on-the-mvn.html"><a href="7-4-inference-based-on-the-mvn.html"><i class="fa fa-check"></i><b>7.4</b> Inference based on the MVN</a><ul>
<li class="chapter" data-level="7.4.1" data-path="7-4-inference-based-on-the-mvn.html"><a href="7-4-inference-based-on-the-mvn.html#onesampleSigma"><i class="fa fa-check"></i><b>7.4.1</b> <span class="math inline">\(\boldsymbol{\Sigma}\)</span> known</a></li>
<li class="chapter" data-level="7.4.2" data-path="7-4-inference-based-on-the-mvn.html"><a href="7-4-inference-based-on-the-mvn.html#onesample"><i class="fa fa-check"></i><b>7.4.2</b> <span class="math inline">\(\boldsymbol{\Sigma}\)</span> unknown: 1 sample</a></li>
<li class="chapter" data-level="7.4.3" data-path="7-4-inference-based-on-the-mvn.html"><a href="7-4-inference-based-on-the-mvn.html#boldsymbolsigma-unknown-2-samples"><i class="fa fa-check"></i><b>7.4.3</b> <span class="math inline">\(\boldsymbol{\Sigma}\)</span> unknown: 2 samples</a></li>
</ul></li>
<li class="chapter" data-level="7.5" data-path="7-5-exercises-4.html"><a href="7-5-exercises-4.html"><i class="fa fa-check"></i><b>7.5</b> Exercises</a></li>
<li class="chapter" data-level="7.6" data-path="7-6-computer-tasks-3.html"><a href="7-6-computer-tasks-3.html"><i class="fa fa-check"></i><b>7.6</b> Computer tasks</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="part-iv-classification-and-clustering.html"><a href="part-iv-classification-and-clustering.html"><i class="fa fa-check"></i>Part IV: Classification and Clustering</a></li>
<li class="chapter" data-level="8" data-path="8-lda.html"><a href="8-lda.html"><i class="fa fa-check"></i><b>8</b> Discriminant analysis</a><ul>
<li class="chapter" data-level="8.0.1" data-path="8-lda.html"><a href="8-lda.html#linear-discriminant-analysis"><i class="fa fa-check"></i><b>8.0.1</b> Linear discriminant analysis</a></li>
<li class="chapter" data-level="8.1" data-path="8-1-lda-ML.html"><a href="8-1-lda-ML.html"><i class="fa fa-check"></i><b>8.1</b> Maximum likelihood (ML) discriminant rule</a><ul>
<li class="chapter" data-level="8.1.1" data-path="8-1-lda-ML.html"><a href="8-1-lda-ML.html#multivariate-normal-populations"><i class="fa fa-check"></i><b>8.1.1</b> Multivariate normal populations</a></li>
<li class="chapter" data-level="8.1.2" data-path="8-1-lda-ML.html"><a href="8-1-lda-ML.html#sample-lda"><i class="fa fa-check"></i><b>8.1.2</b> The sample ML discriminant rule</a></li>
<li class="chapter" data-level="8.1.3" data-path="8-1-lda-ML.html"><a href="8-1-lda-ML.html#two-populations"><i class="fa fa-check"></i><b>8.1.3</b> Two populations</a></li>
<li class="chapter" data-level="8.1.4" data-path="8-1-lda-ML.html"><a href="8-1-lda-ML.html#more-than-two-populations"><i class="fa fa-check"></i><b>8.1.4</b> More than two populations</a></li>
</ul></li>
<li class="chapter" data-level="8.2" data-path="8-2-lda-Bayes.html"><a href="8-2-lda-Bayes.html"><i class="fa fa-check"></i><b>8.2</b> Bayes discriminant rule</a><ul>
<li class="chapter" data-level="8.2.1" data-path="8-2-lda-Bayes.html"><a href="8-2-lda-Bayes.html#example-lda-using-the-iris-data"><i class="fa fa-check"></i><b>8.2.1</b> Example: LDA using the Iris data</a></li>
<li class="chapter" data-level="8.2.2" data-path="8-2-lda-Bayes.html"><a href="8-2-lda-Bayes.html#quadratic-discriminant-analysis-qda"><i class="fa fa-check"></i><b>8.2.2</b> Quadratic Discriminant Analysis (QDA)</a></li>
<li class="chapter" data-level="8.2.3" data-path="8-2-lda-Bayes.html"><a href="8-2-lda-Bayes.html#prediction-accuracy"><i class="fa fa-check"></i><b>8.2.3</b> Prediction accuracy</a></li>
</ul></li>
<li class="chapter" data-level="8.3" data-path="8-3-FLDA.html"><a href="8-3-FLDA.html"><i class="fa fa-check"></i><b>8.3</b> Fisher’s linear discriminant rule</a><ul>
<li class="chapter" data-level="8.3.1" data-path="8-3-FLDA.html"><a href="8-3-FLDA.html#iris-example-continued-1"><i class="fa fa-check"></i><b>8.3.1</b> Iris example continued</a></li>
<li class="chapter" data-level="8.3.2" data-path="8-3-FLDA.html"><a href="8-3-FLDA.html#links-between-methods"><i class="fa fa-check"></i><b>8.3.2</b> Links between methods</a></li>
</ul></li>
<li class="chapter" data-level="8.4" data-path="8-4-computer-tasks-4.html"><a href="8-4-computer-tasks-4.html"><i class="fa fa-check"></i><b>8.4</b> Computer tasks</a></li>
<li class="chapter" data-level="8.5" data-path="8-5-exercises-5.html"><a href="8-5-exercises-5.html"><i class="fa fa-check"></i><b>8.5</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="9" data-path="9-cluster.html"><a href="9-cluster.html"><i class="fa fa-check"></i><b>9</b> Cluster Analysis</a><ul>
<li class="chapter" data-level="9.1" data-path="9-1-k-means-clustering.html"><a href="9-1-k-means-clustering.html"><i class="fa fa-check"></i><b>9.1</b> K-means clustering</a><ul>
<li class="chapter" data-level="9.1.1" data-path="9-1-k-means-clustering.html"><a href="9-1-k-means-clustering.html#estimating-boldsymbol-delta"><i class="fa fa-check"></i><b>9.1.1</b> Estimating <span class="math inline">\(\boldsymbol \delta\)</span></a></li>
<li class="chapter" data-level="9.1.2" data-path="9-1-k-means-clustering.html"><a href="9-1-k-means-clustering.html#k-means"><i class="fa fa-check"></i><b>9.1.2</b> K-means</a></li>
<li class="chapter" data-level="9.1.3" data-path="9-1-k-means-clustering.html"><a href="9-1-k-means-clustering.html#example-iris-data"><i class="fa fa-check"></i><b>9.1.3</b> Example: Iris data</a></li>
<li class="chapter" data-level="9.1.4" data-path="9-1-k-means-clustering.html"><a href="9-1-k-means-clustering.html#choosing-k"><i class="fa fa-check"></i><b>9.1.4</b> Choosing <span class="math inline">\(K\)</span></a></li>
</ul></li>
<li class="chapter" data-level="9.2" data-path="9-2-model-based-clustering.html"><a href="9-2-model-based-clustering.html"><i class="fa fa-check"></i><b>9.2</b> Model-based clustering</a><ul>
<li class="chapter" data-level="9.2.1" data-path="9-2-model-based-clustering.html"><a href="9-2-model-based-clustering.html#maximum-likelihood-estimation"><i class="fa fa-check"></i><b>9.2.1</b> Maximum-likelihood estimation</a></li>
<li class="chapter" data-level="9.2.2" data-path="9-2-model-based-clustering.html"><a href="9-2-model-based-clustering.html#multivariate-gaussian-clusters"><i class="fa fa-check"></i><b>9.2.2</b> Multivariate Gaussian clusters</a></li>
<li class="chapter" data-level="9.2.3" data-path="9-2-model-based-clustering.html"><a href="9-2-model-based-clustering.html#example-iris-1"><i class="fa fa-check"></i><b>9.2.3</b> Example: Iris</a></li>
</ul></li>
<li class="chapter" data-level="9.3" data-path="9-3-hierarchical-clustering-methods.html"><a href="9-3-hierarchical-clustering-methods.html"><i class="fa fa-check"></i><b>9.3</b> Hierarchical clustering methods</a><ul>
<li class="chapter" data-level="9.3.1" data-path="9-3-hierarchical-clustering-methods.html"><a href="9-3-hierarchical-clustering-methods.html#distance-measures"><i class="fa fa-check"></i><b>9.3.1</b> Distance measures</a></li>
<li class="chapter" data-level="9.3.2" data-path="9-3-hierarchical-clustering-methods.html"><a href="9-3-hierarchical-clustering-methods.html#toy-example"><i class="fa fa-check"></i><b>9.3.2</b> Toy Example</a></li>
<li class="chapter" data-level="9.3.3" data-path="9-3-hierarchical-clustering-methods.html"><a href="9-3-hierarchical-clustering-methods.html#comparison-of-methods"><i class="fa fa-check"></i><b>9.3.3</b> Comparison of methods</a></li>
</ul></li>
<li class="chapter" data-level="9.4" data-path="9-4-summary.html"><a href="9-4-summary.html"><i class="fa fa-check"></i><b>9.4</b> Summary</a></li>
<li class="chapter" data-level="9.5" data-path="9-5-computer-tasks-5.html"><a href="9-5-computer-tasks-5.html"><i class="fa fa-check"></i><b>9.5</b> Computer tasks</a></li>
<li class="chapter" data-level="9.6" data-path="9-6-exercises-6.html"><a href="9-6-exercises-6.html"><i class="fa fa-check"></i><b>9.6</b> Exercises</a></li>
</ul></li>
<li class="divider"></li>
<li> <a href="https://rich-d-wilkinson.github.io/teaching.html" target="blank">University of Nottingham</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Multivariate Statistics</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="pca-an-informal-introduction" class="section level2">
<h2><span class="header-section-number">4.1</span> PCA: an informal introduction</h2>
<p>There are two different ways of motivating
principal component analysis (PCA), which may in part explain why PCA is so widely used.</p>
<p>The first motivation, and the topic of this section, is to introduce PCA as method for maximizing the variance of the transformed variables <span class="math inline">\(\mathbf y\)</span>. We start by choosing <span class="math inline">\(\mathbf u_1\)</span> so that <span class="math inline">\(y_1=\mathbf u_1^\top \mathbf x\)</span> has maximum variance. We then choose <span class="math inline">\(\mathbf u_2\)</span> so that <span class="math inline">\(y_2=\mathbf u_2^\top \mathbf x\)</span> has maximum variance subject to being uncorrelated with <span class="math inline">\(y_1\)</span>, and so on.</p>
<p>The idea is to produce a set of variables <span class="math inline">\(y_1, y_2, \ldots, y_r\)</span> that are uncorrelated, but which are most informative about the data. The thinking is that if a variable has large variance it must be informative/important.</p>
<p>The name <strong>principal component analysis</strong> comes from thinking of this as splitting the data <span class="math inline">\(\mathbf X\)</span> into its most important parts. It therefore won’t surprise you to find that this involves the matrix decompositions we studied in Chapter <a href="3-linalg-decomp.html#linalg-decomp">3</a>.</p>
<p><a href="https://twitter.com/allison_horst/status/1288904459490213888?lang=en">Allison Horst (@allison_horst)</a> gave a great illustration of how to think about PCA on Twitter. Imagine you are a whale shark with a wide mouth</p>
<p><img src="figs/WideMouthShark1.png" /></p>
<p>and that you’re swimming towards a delicious swarm of krill.</p>
<p><img src="figs/WideMouthShark2.png" /></p>
<p>What way should you tilt your shark head in order to eat as many krill as possible? The answer is given by the first principal component of the data!</p>
<div id="notation-recap" class="section level3">
<h3><span class="header-section-number">4.1.1</span> Notation recap</h3>
<p>As before, let <span class="math inline">\(\mathbf x_1,\ldots,\mathbf x_n\)</span> be <span class="math inline">\(p \times 1\)</span> vectors of measurements on <span class="math inline">\(n\)</span> experimental units and write
<span class="math display">\[\mathbf X=\left( \begin{array}{ccc}
- &amp;\mathbf x_1^\top&amp;-\\
- &amp;\mathbf x_2^\top&amp;-\\
- &amp;..&amp;-\\
- &amp;\mathbf x_n^\top&amp;-
\end{array}\right)
\]</span></p>
<p><strong>IMPORTANT NOTE:</strong>
In this section we will assume that <span class="math inline">\(\mathbf X\)</span> has been column centered so that the mean of each column is <span class="math inline">\(0\)</span> (i.e., the sample mean of <span class="math inline">\(\mathbf x_1,\ldots,\mathbf x_n\)</span> is the zero vector <span class="math inline">\(\boldsymbol 0\in \mathbb{R}^p\)</span>). If <span class="math inline">\(\mathbf X\)</span> has not been column centered, replace <span class="math inline">\(\mathbf X\)</span> by
<span class="math display">\[\mathbf H\mathbf X\]</span> where <span class="math inline">\(\mathbf H\)</span> is the centering matrix (see <a href="2-4-centering-matrix.html#centering-matrix">2.4</a>), or equivalently, replace <span class="math inline">\(\mathbf x_i\)</span> by <span class="math inline">\(\mathbf x_i - \bar{\mathbf x}\)</span>. It is possible to write out the details of PCA replacing <span class="math inline">\(\mathbf X\)</span> by <span class="math inline">\(\mathbf H\mathbf X\)</span> throughout, but this gets messy and obscures the important detail. Most software implementations (and in particular <code>prcomp</code> in R), automatically centre your data for you, and so in practice you don’t need to worry about doing this when using a software package.</p>
<p>The sample covariance matrix for <span class="math inline">\(\mathbf X\)</span> (assuming it has been column centered) is
<span class="math display">\[\mathbf S= \frac{1}{n}\mathbf X^\top \mathbf X= \frac{1}{n}\sum \mathbf x_i\mathbf x_i^\top\]</span></p>
<p>Given some vector <span class="math inline">\(\mathbf u\)</span>, the transformed variables
<span class="math display">\[y_i = \mathbf u^\top \mathbf x_i\]</span>
have</p>
<ul>
<li><p><strong>mean <span class="math inline">\(0\)</span></strong>:
<span class="math display">\[\bar{y}= \frac{1}{n}\sum_{i=1}^n y_i = \frac{1}{n}\sum_{i=1}^n \mathbf u^\top \mathbf x_i =\frac{1}{n} \mathbf u^\top \sum_{i=1}^n  \mathbf x_i = 0\]</span>
as the mean of the <span class="math inline">\(\mathbf x_i\)</span> is <span class="math inline">\(\boldsymbol 0\)</span>.</p></li>
<li><p><strong>sample covariance matrix</strong> <span class="math display">\[\mathbf u^\top \mathbf S\mathbf u\]</span>
as
<span class="math display">\[\frac{1}{n} \sum_{i=1}^n y_i^2 = \frac{1}{n} \sum_{i=1}^n \mathbf u^\top \mathbf x_i \mathbf x_i^\top\mathbf u= \frac{1}{n}\mathbf u^\top \sum_{i=1}^n  \mathbf x_i \mathbf x_i^\top \mathbf u= \mathbf u^\top \mathbf S\mathbf u
\]</span></p></li>
</ul>
</div>
<div id="first-principal-component" class="section level3">
<h3><span class="header-section-number">4.1.2</span> First principal component</h3>
<p>We would like to find the <span class="math inline">\(\mathbf u\)</span> which maximises the sample variance, <span class="math inline">\(\mathbf u^\top \mathbf S\mathbf u\)</span> over unit vectors <span class="math inline">\(\mathbf u\)</span>, i.e., vectors with <span class="math inline">\(||\mathbf u||=1\)</span>. Why do we focus on unit vectors? If we don’t, we could make the variance as large as we like, e.g., if we replace <span class="math inline">\(\mathbf u\)</span> by <span class="math inline">\(10\mathbf u\)</span> it would increase the variance by a factor of 100. Thus, we constrain the problem and only consider unit vectors for <span class="math inline">\(\mathbf u\)</span>.</p>
<p>We know from Proposition <a href="3-4-svdopt.html#prp:two8">3.7</a> in Section <a href="3-4-svdopt.html#svdopt">3.4</a> that <span class="math inline">\(\mathbf v_1\)</span>, the first eigenvector of <span class="math inline">\(\mathbf S\)</span> (also the first right singular vector of <span class="math inline">\(\mathbf X\)</span>), maximizes <span class="math inline">\(\mathbf u^\top \mathbf S\mathbf u\)</span> with
<span class="math display">\[  \max_{\mathbf u: ||\mathbf u||=1} \mathbf u^\top \mathbf S\mathbf u= \mathbf v_1 \mathbf S\mathbf v_1 =\lambda_1\]</span>
where <span class="math inline">\(\lambda_1\)</span> is the largest eigenvalue of <span class="math inline">\(\mathbf S\)</span>.</p>
<p>So the first principal component of <span class="math inline">\(\mathbf X\)</span> is <span class="math inline">\(\mathbf v_1\)</span>, and the first transformed variable (sometimes called a principal component score) is <span class="math inline">\(y_1 = \mathbf v_1 ^\top \mathbf x\)</span>.
Applying this to each data point we get <span class="math inline">\(n\)</span> instances of this new variable
<span class="math display">\[y_{i1} = \mathbf v_1 ^\top \mathbf x_i.\]</span></p>
<p><strong>A note on singular values</strong>: We know <span class="math inline">\(\mathbf S= \frac{1}{n}\mathbf X^\top\mathbf X\)</span> and so the eigenvalues of <span class="math inline">\(\mathbf S\)</span> are the same as the squared singular values of <span class="math inline">\(\frac{1}{\sqrt{n}} \mathbf X\)</span>:</p>
<p><span class="math display">\[\sqrt{\lambda_1} = \sigma_1\left(\frac{1}{\sqrt{n}} \mathbf X\right)\]</span></p>
<p>If we scale <span class="math inline">\(\mathbf X\)</span> by a factor <span class="math inline">\(c\)</span>, then the singular values are scaled by the same amount, i.e.,
<span class="math display">\[\sigma_i(c\mathbf X)=c\sigma_i(\mathbf X)\]</span>
and in particular
<span class="math display">\[ \sigma_i\left(\frac{1}{\sqrt{n}} \mathbf X\right) = \frac{1}{\sqrt{n}} \sigma_i(\mathbf X)\]</span>
We will need to remember this scaling if we use the SVD of <span class="math inline">\(\mathbf X\)</span> to do PCA. Note that scaling <span class="math inline">\(\mathbf X\)</span> does not change the singular vectors/principal components.</p>
</div>
<div id="second-principal-component" class="section level3">
<h3><span class="header-section-number">4.1.3</span> Second principal component</h3>
<p><span class="math inline">\(y_1\)</span> is the transformed variable that has maximum variance. What should we choose to be our next transformed variable, i.e., what <span class="math inline">\(\mathbf u_2\)</span> should we choose for <span class="math inline">\(y_2 = \mathbf u_2^\top \mathbf x\)</span>? It makes sense to choose <span class="math inline">\(y_2\)</span> to be uncorrelated with <span class="math inline">\(y_1\)</span>, as otherwise it contains some of the same information given by <span class="math inline">\(y_1\)</span>. The sample covariance between <span class="math inline">\(y_1\)</span> and <span class="math inline">\(\mathbf u_2^\top \mathbf x\)</span> is
<span class="math display">\[\begin{align*}
s_{y_2y_1} &amp;=\frac{1}{n}\sum_{i=1}^n \mathbf u_2^\top \mathbf x_i \mathbf x_i^\top \mathbf v_1\\ 
&amp;= \mathbf u_2^\top \mathbf S\mathbf v_1\\
&amp; = \lambda_1 \mathbf u_2^\top \mathbf v_1 \mbox{ as } \mathbf v_1 \mbox{ is an eigenvector of } S
\end{align*}\]</span>
So to make <span class="math inline">\(y_2\)</span> uncorrelated with <span class="math inline">\(y_1\)</span> we have to choose <span class="math inline">\(\mathbf u_2\)</span> to be orthogonal to <span class="math inline">\(\mathbf v_1\)</span>, i.e., <span class="math inline">\(\mathbf u_2^\top \mathbf v_1=0\)</span>. So we choose <span class="math inline">\(\mathbf u_2\)</span> to be the solution to the optimization problem</p>
<p><span class="math display">\[\max_{\mathbf u} \mathbf u^\top \mathbf S\mathbf u\mbox{ subject to } \mathbf u^\top \mathbf v_1=0.\]</span>
The solution to this problem is to take <span class="math inline">\(\mathbf u_2 = \mathbf v_2\)</span>, i.e., the second eigenvector of <span class="math inline">\(\mathbf S\)</span> (or second right singular vector of <span class="math inline">\(\mathbf X\)</span>), and then <span class="math display">\[\mathbf v_2^\top \mathbf S\mathbf v_2=\lambda_2.\]</span>
We’ll prove this result in the next section.</p>
<div id="later-principal-components" class="section level4 unnumbered">
<h4>Later principal components</h4>
<p>Our first transformed variable is
<span class="math display">\[y_{i1}= \mathbf v_1^\top \mathbf x_i\]</span>
and our second transformed variable is
<span class="math display">\[y_{i2}= \mathbf v_2^\top \mathbf x_i.\]</span>
At this point, you can probably guess that the <span class="math inline">\(j^{th}\)</span> transformed variable is going to be
<span class="math display">\[y_{ij}= \mathbf v_j^\top \mathbf x_i.\]</span>
where <span class="math inline">\(\mathbf v_j\)</span> is the <span class="math inline">\(j^{th}\)</span> eigenvector of <span class="math inline">\(\mathbf S\)</span>.</p>
<ul>
<li>The transformed variables <span class="math inline">\(y_{i}\)</span> are the <strong>principal component scores</strong>. <span class="math inline">\(y_1\)</span> is the first score etc.</li>
<li>The eigenvectors/right singular vectors are sometimes refered to as the <strong>loadings</strong> or simply as the <strong>principal components</strong>.</li>
</ul>
</div>
</div>
<div id="geometric-interpretation-1" class="section level3">
<h3><span class="header-section-number">4.1.4</span> Geometric interpretation</h3>
<p>We think of PCA as projecting the data points <span class="math inline">\(\mathbf x\)</span> onto a subspace <span class="math inline">\(V\)</span>. The basis vectors for this subspace are the eigenvectors of <span class="math inline">\(\mathbf S\)</span>, which are the same as the right singular vectors of <span class="math inline">\(\mathbf X\)</span> (the loadings):
<span class="math display">\[V=\operatorname{span}\{\mathbf v_1, \ldots, \mathbf v_r\}.\]</span>
The orthogonal projection matrix (see Section <a href="2-3-linalg-innerprod.html#orthogproj">2.3.3.1</a>) for projecting onto <span class="math inline">\(V\)</span> is
<span class="math display">\[\mathbf P_V = \mathbf V\mathbf V^\top\]</span>
as <span class="math inline">\(\mathbf V^\top \mathbf V=\mathbf I\)</span>.<br />
The coordinates of the data points projected onto <span class="math inline">\(V\)</span> (with respect to the basis for <span class="math inline">\(V\)</span>) are the <strong>principal component scores</strong>:</p>
<p><span class="math display">\[\mathbf y_i= \left(\begin{array}{c}y_{i1}\\\vdots\\y_{ir}\end{array}\right)= \mathbf V^\top \mathbf x_i\]</span>
where <span class="math display">\[\mathbf V= \left(\begin{array}{ccc} | &amp;&amp;|\\\mathbf v_1&amp;\ldots&amp; \mathbf v_r\\  | &amp;&amp;|\end{array}\right)\]</span>
is the matrix of right singular vectors from the SVD of <span class="math inline">\(\mathbf X\)</span>.
The transformed variables are</p>
<p><span class="math display">\[\mathbf Y= \left( \begin{array}{ccc}
- &amp;\mathbf y_1^\top&amp;-\\
- &amp;..&amp;-\\
- &amp;\mathbf y_n^\top&amp;-
\end{array}\right ) = \mathbf X\mathbf V.
\]</span>
Substituting the SVD for <span class="math inline">\(\mathbf X= \mathbf U\boldsymbol{\Sigma}\mathbf V^\top\)</span> we can see the transformed variable matrix/principal component scores are
<span class="math display">\[\mathbf Y= \mathbf U\boldsymbol{\Sigma}.\]</span></p>
<p><span class="math inline">\(\mathbf Y\)</span> is a <span class="math inline">\(n \times r\)</span> matrix, and so if <span class="math inline">\(r&lt;p\)</span> we have reduced the dimension of <span class="math inline">\(\mathbf X\)</span>, keeping the most important parts of the data</p>
</div>
<div id="example" class="section level3">
<h3><span class="header-section-number">4.1.5</span> Example</h3>
<p>We consider the marks of <span class="math inline">\(n=10\)</span> students who studied G11PRB and G11STA.</p>
<table class="table" style="width: auto !important; margin-left: auto; margin-right: auto;">
<thead>
<tr>
<th style="text-align:right;">
student
</th>
<th style="text-align:right;">
PRB
</th>
<th style="text-align:right;">
STA
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:right;">
1
</td>
<td style="text-align:right;">
81
</td>
<td style="text-align:right;">
75
</td>
</tr>
<tr>
<td style="text-align:right;">
2
</td>
<td style="text-align:right;">
79
</td>
<td style="text-align:right;">
73
</td>
</tr>
<tr>
<td style="text-align:right;">
3
</td>
<td style="text-align:right;">
66
</td>
<td style="text-align:right;">
79
</td>
</tr>
<tr>
<td style="text-align:right;">
4
</td>
<td style="text-align:right;">
53
</td>
<td style="text-align:right;">
55
</td>
</tr>
<tr>
<td style="text-align:right;">
5
</td>
<td style="text-align:right;">
43
</td>
<td style="text-align:right;">
53
</td>
</tr>
<tr>
<td style="text-align:right;">
6
</td>
<td style="text-align:right;">
59
</td>
<td style="text-align:right;">
49
</td>
</tr>
<tr>
<td style="text-align:right;">
7
</td>
<td style="text-align:right;">
62
</td>
<td style="text-align:right;">
72
</td>
</tr>
<tr>
<td style="text-align:right;">
8
</td>
<td style="text-align:right;">
79
</td>
<td style="text-align:right;">
92
</td>
</tr>
<tr>
<td style="text-align:right;">
9
</td>
<td style="text-align:right;">
49
</td>
<td style="text-align:right;">
58
</td>
</tr>
<tr>
<td style="text-align:right;">
10
</td>
<td style="text-align:right;">
55
</td>
<td style="text-align:right;">
56
</td>
</tr>
</tbody>
</table>
<p>These data haven’t been column centered, so let’s do that in R. You can do it using the centering matrix as previously, but here is a different approach:</p>
<div class="sourceCode" id="cb74"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb74-1" data-line-number="1">secondyr &lt;-<span class="st"> </span><span class="kw">data.frame</span>(</a>
<a class="sourceLine" id="cb74-2" data-line-number="2">  <span class="dt">student =</span> <span class="dv">1</span><span class="op">:</span><span class="dv">10</span>,</a>
<a class="sourceLine" id="cb74-3" data-line-number="3"><span class="dt">PRB=</span><span class="kw">c</span>(<span class="dv">81</span> , <span class="dv">79</span> , <span class="dv">66</span> , <span class="dv">53</span> , <span class="dv">43</span> , <span class="dv">59</span> , <span class="dv">62</span> , <span class="dv">79</span> , <span class="dv">49</span> , <span class="dv">55</span>),</a>
<a class="sourceLine" id="cb74-4" data-line-number="4"><span class="dt">STA =</span><span class="kw">c</span>(<span class="dv">75</span> , <span class="dv">73</span> , <span class="dv">79</span> , <span class="dv">55</span> , <span class="dv">53</span> , <span class="dv">49</span> , <span class="dv">72</span> , <span class="dv">92</span> , <span class="dv">58</span> , <span class="dv">56</span>)</a>
<a class="sourceLine" id="cb74-5" data-line-number="5">        )</a>
<a class="sourceLine" id="cb74-6" data-line-number="6">xbar &lt;-<span class="st"> </span><span class="kw">colMeans</span>(secondyr[,<span class="dv">2</span><span class="op">:</span><span class="dv">3</span>]) <span class="co">#only columns 2 and 3 are data</span></a>
<a class="sourceLine" id="cb74-7" data-line-number="7">X &lt;-<span class="st"> </span><span class="kw">as.matrix</span>(<span class="kw">sweep</span>(secondyr[,<span class="dv">2</span><span class="op">:</span><span class="dv">3</span>], <span class="dv">2</span>, xbar) ) </a></code></pre></div>
<table class="table" style="width: auto !important; margin-left: auto; margin-right: auto;">
<thead>
<tr>
<th style="text-align:right;">
PRB
</th>
<th style="text-align:right;">
STA
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:right;">
18.4
</td>
<td style="text-align:right;">
8.8
</td>
</tr>
<tr>
<td style="text-align:right;">
16.4
</td>
<td style="text-align:right;">
6.8
</td>
</tr>
<tr>
<td style="text-align:right;">
3.4
</td>
<td style="text-align:right;">
12.8
</td>
</tr>
<tr>
<td style="text-align:right;">
-9.6
</td>
<td style="text-align:right;">
-11.2
</td>
</tr>
<tr>
<td style="text-align:right;">
-19.6
</td>
<td style="text-align:right;">
-13.2
</td>
</tr>
<tr>
<td style="text-align:right;">
-3.6
</td>
<td style="text-align:right;">
-17.2
</td>
</tr>
<tr>
<td style="text-align:right;">
-0.6
</td>
<td style="text-align:right;">
5.8
</td>
</tr>
<tr>
<td style="text-align:right;">
16.4
</td>
<td style="text-align:right;">
25.8
</td>
</tr>
<tr>
<td style="text-align:right;">
-13.6
</td>
<td style="text-align:right;">
-8.2
</td>
</tr>
<tr>
<td style="text-align:right;">
-7.6
</td>
<td style="text-align:right;">
-10.2
</td>
</tr>
</tbody>
</table>
<p>The sample covariance matrix can be computed in two ways:</p>
<div class="sourceCode" id="cb75"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb75-1" data-line-number="1"><span class="dv">1</span><span class="op">/</span><span class="dv">10</span><span class="op">*</span><span class="st"> </span><span class="kw">t</span>(X)<span class="op">%*%</span>X</a></code></pre></div>
<pre><code>##        PRB    STA
## PRB 162.04 135.38
## STA 135.38 175.36</code></pre>
<div class="sourceCode" id="cb77"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb77-1" data-line-number="1"><span class="kw">cov</span>(X)<span class="op">*</span><span class="dv">9</span><span class="op">/</span><span class="dv">10</span> </a></code></pre></div>
<pre><code>##        PRB    STA
## PRB 162.04 135.38
## STA 135.38 175.36</code></pre>
<div class="sourceCode" id="cb79"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb79-1" data-line-number="1"><span class="co"># Remember R uses the unbiased factor 1/(n-1), </span></a>
<a class="sourceLine" id="cb79-2" data-line-number="2"><span class="co"># so the 9/10=(n-1)/n changes this to 1/n </span></a>
<a class="sourceLine" id="cb79-3" data-line-number="3"><span class="co"># to match the notes</span></a></code></pre></div>
<p>We can find the singular value decomposition of <span class="math inline">\(\mathbf X\)</span> using R</p>
<div class="sourceCode" id="cb80"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb80-1" data-line-number="1">(<span class="dt">X_svd =</span> <span class="kw">svd</span>(X))</a></code></pre></div>
<pre><code>## $d
## [1] 55.15829 18.20887
## 
## $u
##              [,1]        [,2]
##  [1,] -0.34556317 -0.39864295
##  [2,] -0.29430029 -0.39482564
##  [3,] -0.21057607  0.34946080
##  [4,]  0.26707104 -0.04226416
##  [5,]  0.41833934  0.27975879
##  [6,]  0.27085156 -0.50812066
##  [7,] -0.06865802  0.24349429
##  [8,] -0.54378479  0.32464825
##  [9,]  0.27768146  0.23043980
## [10,]  0.22893893 -0.08394852
## 
## $v
##            [,1]       [,2]
## [1,] -0.6895160 -0.7242705
## [2,] -0.7242705  0.6895160</code></pre>
<p>So we can see that the eigenvectors/right singular vectors/loadings are</p>
<p><span class="math display">\[\mathbf v_1=\begin{pmatrix} -0.69 \\ -0.724 \end{pmatrix},\qquad \mathbf v_2=\begin{pmatrix} -0.724 \\ 0.69 \end{pmatrix}\]</span></p>
<p>Sometimes the new variables have an obvious interpretation. In this case the first PC gives approximately equal weight to PRB and STA and thus represents some form of negative ‘’average’’ mark. Note that the singular vectors are only determined upto multiplication by <span class="math inline">\(\pm 1\)</span>. In this case, R has chosen <span class="math inline">\(\mathbf v_1\)</span> to have negative entries, but we could multiply <span class="math inline">\(\mathbf v_1\)</span> by <span class="math inline">\(-1\)</span> so that the first PC was more like the avearge.
As it is, a student that has a high mark on PRB and STA will have a low negative value for <span class="math inline">\(y_1\)</span>. The second PC, meanwhile, represents a contrast between PRB and STA. For example, a large positive value for <span class="math inline">\(y_2\)</span> implies the student did much better on STA than PRB, and a large negative value implies the opposite.</p>
<p>If we plot the data along with the principal components. The two lines, centred on <span class="math inline">\(\bar{\mathbf x}\)</span>, are in the direction of the principal components/eigenvectors, and their lengths are <span class="math inline">\(2 \sqrt{\lambda_j}\)</span>, <span class="math inline">\(j=1,2\)</span>.
We can see that the first PC is in the direction of greatest variation (shown in red), and that the second PC (shown in green) is orthogonal to the first PC.</p>
<p><img src="04-pca_files/figure-html/unnamed-chunk-8-1.png" width="576" /></p>
<p>We can find the transformed variables by computing either <span class="math inline">\(\mathbf X\mathbf V\)</span> or <span class="math inline">\(\mathbf U\boldsymbol{\Sigma}\)</span></p>
<div class="sourceCode" id="cb82"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb82-1" data-line-number="1">X <span class="op">%*%</span><span class="st"> </span>X_svd<span class="op">$</span>v</a></code></pre></div>
<pre><code>##             [,1]       [,2]
##  [1,] -19.060674 -7.2588361
##  [2,] -16.233101 -7.1893271
##  [3,] -11.615016  6.3632849
##  [4,]  14.731183 -0.7695824
##  [5,]  23.074883  5.0940904
##  [6,]  14.939710 -9.2523011
##  [7,]  -3.787059  4.4337549
##  [8,] -29.994240  5.9114764
##  [9,]  15.316435  4.1960474
## [10,]  12.627880 -1.5286074</code></pre>
<div class="sourceCode" id="cb84"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb84-1" data-line-number="1">X_svd<span class="op">$</span>u <span class="op">%*%</span><span class="st"> </span><span class="kw">diag</span>(X_svd<span class="op">$</span>d)</a></code></pre></div>
<pre><code>##             [,1]       [,2]
##  [1,] -19.060674 -7.2588361
##  [2,] -16.233101 -7.1893271
##  [3,] -11.615016  6.3632849
##  [4,]  14.731183 -0.7695824
##  [5,]  23.074883  5.0940904
##  [6,]  14.939710 -9.2523011
##  [7,]  -3.787059  4.4337549
##  [8,] -29.994240  5.9114764
##  [9,]  15.316435  4.1960474
## [10,]  12.627880 -1.5286074</code></pre>
<p>If we plot the PC scores we can see that the variation is now in line with the new coordinate axes:</p>
<p><img src="04-pca_files/figure-html/unnamed-chunk-10-1.png" width="672" /></p>
<p>R also has a built-in function for doing PCA.</p>
<div class="sourceCode" id="cb86"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb86-1" data-line-number="1">pca &lt;-<span class="st"> </span><span class="kw">prcomp</span>(secondyr[,<span class="dv">2</span><span class="op">:</span><span class="dv">3</span>]) <span class="co"># prcomp will automatocally remove the column mean</span></a>
<a class="sourceLine" id="cb86-2" data-line-number="2">pca<span class="op">$</span>rotation <span class="co"># the loadings</span></a></code></pre></div>
<pre><code>##            PC1        PC2
## PRB -0.6895160 -0.7242705
## STA -0.7242705  0.6895160</code></pre>
<div class="sourceCode" id="cb88"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb88-1" data-line-number="1">pca<span class="op">$</span>x <span class="co"># the scores</span></a></code></pre></div>
<pre><code>##              PC1        PC2
##  [1,] -19.060674 -7.2588361
##  [2,] -16.233101 -7.1893271
##  [3,] -11.615016  6.3632849
##  [4,]  14.731183 -0.7695824
##  [5,]  23.074883  5.0940904
##  [6,]  14.939710 -9.2523011
##  [7,]  -3.787059  4.4337549
##  [8,] -29.994240  5.9114764
##  [9,]  15.316435  4.1960474
## [10,]  12.627880 -1.5286074</code></pre>
<p>Note that the new variables have sample mean <span class="math inline">\(\bar{\mathbf y}=\boldsymbol 0\)</span>. The sample covariance matrix is a diagonal with entries given by the eigenvalues (see part 4. of Proposition <a href="4-2-pca-a-formal-description-with-proofs.html#prp:pca2">4.2</a>). Note that there is always some numerical error (so quantities are never 0, and instead are just very small numnbers).</p>
<p><span class="math display">\[
\boldsymbol \Lambda= \text{diag}(\lambda_1,\lambda_2) =  \begin{pmatrix} \lambda_1 &amp; 0 \\ 0 &amp; \lambda_2 \end{pmatrix}.
\]</span></p>
<div class="sourceCode" id="cb90"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb90-1" data-line-number="1"><span class="kw">colMeans</span>(pca<span class="op">$</span>x)</a></code></pre></div>
<pre><code>##           PC1           PC2 
##  2.842171e-15 -9.769963e-16</code></pre>
<div class="sourceCode" id="cb92"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb92-1" data-line-number="1"><span class="kw">cov</span>(pca<span class="op">$</span>x)<span class="op">*</span><span class="dv">9</span><span class="op">/</span><span class="dv">10</span> <span class="co"># to convert to using 1/n as the denominator </span></a></code></pre></div>
<pre><code>##              PC1          PC2
## PC1 3.042437e+02 1.974167e-14
## PC2 1.974167e-14 3.315628e+01</code></pre>
<p>Finally, note that we did the singular value decomposition for <span class="math inline">\(\mathbf X\)</span> above not <span class="math inline">\(\frac{1}{\sqrt{10}}\mathbf X\)</span>, and so we’d need to square and scale the singular values to find the eigenvalues. Let’s check:</p>
<div class="sourceCode" id="cb94"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb94-1" data-line-number="1">X_svd<span class="op">$</span>d<span class="op">^</span><span class="dv">2</span><span class="op">/</span><span class="dv">10</span> <span class="co"># square and scale the singular values</span></a></code></pre></div>
<pre><code>## [1] 304.24372  33.15628</code></pre>
<div class="sourceCode" id="cb96"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb96-1" data-line-number="1"><span class="kw">eigen</span>(<span class="kw">t</span>(X) <span class="op">%*%</span><span class="st"> </span>X<span class="op">/</span><span class="dv">10</span>)<span class="op">$</span>values  <span class="co"># compute the eigenvalues of the covariance matrix</span></a></code></pre></div>
<pre><code>## [1] 304.24372  33.15628</code></pre>
<div class="sourceCode" id="cb98"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb98-1" data-line-number="1"><span class="kw">svd</span>(X<span class="op">/</span><span class="kw">sqrt</span>(<span class="dv">10</span>))<span class="op">$</span>d<span class="op">^</span><span class="dv">2</span> <span class="co"># compute the singular values of X/sqrt(10) and square</span></a></code></pre></div>
<pre><code>## [1] 304.24372  33.15628</code></pre>
</div>
<div id="example-iris" class="section level3">
<h3><span class="header-section-number">4.1.6</span> Example: Iris</h3>
<p>In general when using R to do PCA, we don’t need to compute the SVD and then do the projections, as there is an R command <code>prcomp</code> that will do it all for us. The <code>princomp</code> will also do PCA, but is less stable than <code>prcomp</code>, and it is recommended that you use <code>prcomp</code> in preference.</p>
<p>Let’s do PCA on the iris dataset discussed in Chapter <a href="1-stat-prelim.html#stat-prelim">1</a>. The <code>prcomp</code> returns the square root of the eigenvalues (the standard devaiation of the PC scores), and the PC scores.</p>
<div class="sourceCode" id="cb100"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb100-1" data-line-number="1">iris.pca =<span class="st"> </span><span class="kw">prcomp</span>(iris[,<span class="dv">1</span><span class="op">:</span><span class="dv">4</span>])</a>
<a class="sourceLine" id="cb100-2" data-line-number="2">iris.pca<span class="op">$</span>sdev <span class="co"># the square root of the eigenvalues</span></a></code></pre></div>
<pre><code>## [1] 2.0562689 0.4926162 0.2796596 0.1543862</code></pre>
<div class="sourceCode" id="cb102"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb102-1" data-line-number="1"><span class="kw">head</span>(iris.pca<span class="op">$</span>x)  <span class="co">#the PC scores</span></a></code></pre></div>
<pre><code>##            PC1        PC2         PC3          PC4
## [1,] -2.684126 -0.3193972  0.02791483  0.002262437
## [2,] -2.714142  0.1770012  0.21046427  0.099026550
## [3,] -2.888991  0.1449494 -0.01790026  0.019968390
## [4,] -2.745343  0.3182990 -0.03155937 -0.075575817
## [5,] -2.728717 -0.3267545 -0.09007924 -0.061258593
## [6,] -2.280860 -0.7413304 -0.16867766 -0.024200858</code></pre>
<p>The PC loadings/eigenvectors can also be accessed, as can the sample mean</p>
<div class="sourceCode" id="cb104"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb104-1" data-line-number="1">iris.pca<span class="op">$</span>rotation <span class="co">#the eigenvecstors</span></a></code></pre></div>
<pre><code>##                      PC1         PC2         PC3        PC4
## Sepal.Length  0.36138659 -0.65658877  0.58202985  0.3154872
## Sepal.Width  -0.08452251 -0.73016143 -0.59791083 -0.3197231
## Petal.Length  0.85667061  0.17337266 -0.07623608 -0.4798390
## Petal.Width   0.35828920  0.07548102 -0.54583143  0.7536574</code></pre>
<div class="sourceCode" id="cb106"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb106-1" data-line-number="1">iris.pca<span class="op">$</span>center <span class="co"># the sample mean of the data</span></a></code></pre></div>
<pre><code>## Sepal.Length  Sepal.Width Petal.Length  Petal.Width 
##     5.843333     3.057333     3.758000     1.199333</code></pre>
<p>A scree plot can be obtained simply by using the <code>plot</code> command. The summary command also gives useful information about the importance of each PC.</p>
<div class="sourceCode" id="cb108"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb108-1" data-line-number="1"><span class="kw">plot</span>(iris.pca)</a></code></pre></div>
<p><img src="04-pca_files/figure-html/unnamed-chunk-16-1.png" width="672" /></p>
<div class="sourceCode" id="cb109"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb109-1" data-line-number="1"><span class="kw">summary</span>(iris.pca)</a></code></pre></div>
<pre><code>## Importance of components:
##                           PC1     PC2    PC3     PC4
## Standard deviation     2.0563 0.49262 0.2797 0.15439
## Proportion of Variance 0.9246 0.05307 0.0171 0.00521
## Cumulative Proportion  0.9246 0.97769 0.9948 1.00000</code></pre>
<p>To plot the PC scores, you can either manually create a plot or use the <code>ggfortify</code> package. For example, here is a plot of the first two PC scores coloured according to the species of iris.</p>
<div class="sourceCode" id="cb111"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb111-1" data-line-number="1">iris<span class="op">$</span>PC1=iris.pca<span class="op">$</span>x[,<span class="dv">1</span>]</a>
<a class="sourceLine" id="cb111-2" data-line-number="2">iris<span class="op">$</span>PC2=iris.pca<span class="op">$</span>x[,<span class="dv">2</span>]</a>
<a class="sourceLine" id="cb111-3" data-line-number="3"><span class="kw">qplot</span>(PC1, PC2, <span class="dt">colour=</span>Species, <span class="dt">data=</span>iris)</a></code></pre></div>
<p><img src="04-pca_files/figure-html/unnamed-chunk-17-1.png" width="672" /></p>
<p>The <code>ggfortify</code> package provides a nice wrapper for some of this functionality.</p>
<div class="sourceCode" id="cb112"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb112-1" data-line-number="1"><span class="kw">library</span>(ggfortify)</a>
<a class="sourceLine" id="cb112-2" data-line-number="2"><span class="kw">autoplot</span>(iris.pca, <span class="dt">data =</span> iris, <span class="dt">colour =</span> <span class="st">&#39;Species&#39;</span>, <span class="dt">scale=</span><span class="ot">FALSE</span>)</a></code></pre></div>
<p><img src="04-pca_files/figure-html/unnamed-chunk-18-1.png" width="672" /></p>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="4-pca.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="4-2-pca-a-formal-description-with-proofs.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["MultivariateStatistics.pdf"],
"toc": {
"collapse": "section"
},
"pandoc_args": "--top-level-division=[chapter|part]"
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
