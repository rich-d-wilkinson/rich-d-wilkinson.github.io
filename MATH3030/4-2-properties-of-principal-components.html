<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>4.2 Properties of principal components | Multivariate Statistics</title>
  <meta name="description" content="The lecture notes for MATH3030/4068: Multivariate Analysis / Applied Multivariate Statistics" />
  <meta name="generator" content="bookdown 0.20.6 and GitBook 2.6.7" />

  <meta property="og:title" content="4.2 Properties of principal components | Multivariate Statistics" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="The lecture notes for MATH3030/4068: Multivariate Analysis / Applied Multivariate Statistics" />
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="4.2 Properties of principal components | Multivariate Statistics" />
  
  <meta name="twitter:description" content="The lecture notes for MATH3030/4068: Multivariate Analysis / Applied Multivariate Statistics" />
  

<meta name="author" content="Prof.Â Richard Wilkinson" />



  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="4-1-principal-component-vectors-and-scores.html"/>
<link rel="next" href="4-3-population-pca.html"/>
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />









<script src="libs/kePrint-0.0.1/kePrint.js"></script>
<link href="libs/lightable-0.0.1/lightable.css" rel="stylesheet" />


<style type="text/css">
a.sourceLine { display: inline-block; line-height: 1.25; }
a.sourceLine { pointer-events: none; color: inherit; text-decoration: inherit; }
a.sourceLine:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode { white-space: pre; position: relative; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
code.sourceCode { white-space: pre-wrap; }
a.sourceLine { text-indent: -1em; padding-left: 1em; }
}
pre.numberSource a.sourceLine
  { position: relative; left: -4em; }
pre.numberSource a.sourceLine::before
  { content: attr(data-line-number);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; pointer-events: all; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {  }
@media screen {
a.sourceLine::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Applied multivartiate statistics</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Introduction</a></li>
<li class="chapter" data-level="" data-path="part-i-prerequisites.html"><a href="part-i-prerequisites.html"><i class="fa fa-check"></i>PART I: Prerequisites</a></li>
<li class="chapter" data-level="1" data-path="1-stat-prelim.html"><a href="1-stat-prelim.html"><i class="fa fa-check"></i><b>1</b> Statistical Preliminaries</a><ul>
<li class="chapter" data-level="1.1" data-path="1-1-multivariate-data.html"><a href="1-1-multivariate-data.html"><i class="fa fa-check"></i><b>1.1</b> Multivariate data</a></li>
<li class="chapter" data-level="1.2" data-path="1-2-summary-statistics.html"><a href="1-2-summary-statistics.html"><i class="fa fa-check"></i><b>1.2</b> Summary statistics</a></li>
<li class="chapter" data-level="1.3" data-path="1-3-graphical-techniques.html"><a href="1-3-graphical-techniques.html"><i class="fa fa-check"></i><b>1.3</b> Graphical techniques</a></li>
<li class="chapter" data-level="1.4" data-path="1-4-random-vectors-and-matrices.html"><a href="1-4-random-vectors-and-matrices.html"><i class="fa fa-check"></i><b>1.4</b> Random Vectors and Matrices</a></li>
<li class="chapter" data-level="1.5" data-path="1-5-unbiased-estimators.html"><a href="1-5-unbiased-estimators.html"><i class="fa fa-check"></i><b>1.5</b> Unbiased Estimators</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="2-linalg-prelim.html"><a href="2-linalg-prelim.html"><i class="fa fa-check"></i><b>2</b> Review of linear algebra</a><ul>
<li class="chapter" data-level="2.1" data-path="2-1-linalg-basics.html"><a href="2-1-linalg-basics.html"><i class="fa fa-check"></i><b>2.1</b> Basics</a><ul>
<li class="chapter" data-level="2.1.1" data-path="2-1-linalg-basics.html"><a href="2-1-linalg-basics.html#notation"><i class="fa fa-check"></i><b>2.1.1</b> Notation</a></li>
<li class="chapter" data-level="2.1.2" data-path="2-1-linalg-basics.html"><a href="2-1-linalg-basics.html#elementary-matrix-operations"><i class="fa fa-check"></i><b>2.1.2</b> Elementary matrix operations</a></li>
<li class="chapter" data-level="2.1.3" data-path="2-1-linalg-basics.html"><a href="2-1-linalg-basics.html#special-matrices"><i class="fa fa-check"></i><b>2.1.3</b> Special matrices</a></li>
<li class="chapter" data-level="2.1.4" data-path="2-1-linalg-basics.html"><a href="2-1-linalg-basics.html#vector-differentiation"><i class="fa fa-check"></i><b>2.1.4</b> Vector Differentiation</a></li>
</ul></li>
<li class="chapter" data-level="2.2" data-path="2-2-linalg-vecspaces.html"><a href="2-2-linalg-vecspaces.html"><i class="fa fa-check"></i><b>2.2</b> Vector spaces</a><ul>
<li class="chapter" data-level="2.2.1" data-path="2-2-linalg-vecspaces.html"><a href="2-2-linalg-vecspaces.html#linear-independence"><i class="fa fa-check"></i><b>2.2.1</b> Linear independence</a></li>
<li class="chapter" data-level="2.2.2" data-path="2-2-linalg-vecspaces.html"><a href="2-2-linalg-vecspaces.html#colsspace"><i class="fa fa-check"></i><b>2.2.2</b> Row and column spaces</a></li>
<li class="chapter" data-level="2.2.3" data-path="2-2-linalg-vecspaces.html"><a href="2-2-linalg-vecspaces.html#linear-transformations"><i class="fa fa-check"></i><b>2.2.3</b> Linear transformations</a></li>
</ul></li>
<li class="chapter" data-level="2.3" data-path="2-3-linalg-innerprod.html"><a href="2-3-linalg-innerprod.html"><i class="fa fa-check"></i><b>2.3</b> Inner product spaces</a><ul>
<li class="chapter" data-level="2.3.1" data-path="2-3-linalg-innerprod.html"><a href="2-3-linalg-innerprod.html#normed"><i class="fa fa-check"></i><b>2.3.1</b> Distances, and angles</a></li>
<li class="chapter" data-level="2.3.2" data-path="2-3-linalg-innerprod.html"><a href="2-3-linalg-innerprod.html#orthogonal-matrices"><i class="fa fa-check"></i><b>2.3.2</b> Orthogonal matrices</a></li>
<li class="chapter" data-level="2.3.3" data-path="2-3-linalg-innerprod.html"><a href="2-3-linalg-innerprod.html#projections"><i class="fa fa-check"></i><b>2.3.3</b> Projections</a></li>
</ul></li>
<li class="chapter" data-level="2.4" data-path="2-4-linalg-misc.html"><a href="2-4-linalg-misc.html"><i class="fa fa-check"></i><b>2.4</b> Miscellaneous topics</a><ul>
<li class="chapter" data-level="2.4.1" data-path="2-4-linalg-misc.html"><a href="2-4-linalg-misc.html#the-centering-matrix"><i class="fa fa-check"></i><b>2.4.1</b> The Centering Matrix</a></li>
<li class="chapter" data-level="2.4.2" data-path="2-4-linalg-misc.html"><a href="2-4-linalg-misc.html#quadratic-forms-and-ellipses"><i class="fa fa-check"></i><b>2.4.2</b> Quadratic forms and ellipses</a></li>
<li class="chapter" data-level="2.4.3" data-path="2-4-linalg-misc.html"><a href="2-4-linalg-misc.html#lines-and-hyperplanes-in-mathbbrp"><i class="fa fa-check"></i><b>2.4.3</b> Lines and Hyperplanes in <span class="math inline">\(\mathbb{R}^p\)</span></a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="3" data-path="3-linalg-decomp.html"><a href="3-linalg-decomp.html"><i class="fa fa-check"></i><b>3</b> Matrix decompositions</a><ul>
<li class="chapter" data-level="3.1" data-path="3-1-matrix-matrix.html"><a href="3-1-matrix-matrix.html"><i class="fa fa-check"></i><b>3.1</b> Matrix-matrix products</a></li>
<li class="chapter" data-level="3.2" data-path="3-2-eigenvalues-and-eigenvectors.html"><a href="3-2-eigenvalues-and-eigenvectors.html"><i class="fa fa-check"></i><b>3.2</b> Eigenvalues and eigenvectors</a></li>
<li class="chapter" data-level="3.3" data-path="3-3-spectraleigen-decomposition.html"><a href="3-3-spectraleigen-decomposition.html"><i class="fa fa-check"></i><b>3.3</b> Spectral/eigen decomposition</a><ul>
<li class="chapter" data-level="3.3.1" data-path="3-3-spectraleigen-decomposition.html"><a href="3-3-spectraleigen-decomposition.html#matrix-square-roots"><i class="fa fa-check"></i><b>3.3.1</b> Matrix square roots</a></li>
</ul></li>
<li class="chapter" data-level="3.4" data-path="3-4-linalg-SVD.html"><a href="3-4-linalg-SVD.html"><i class="fa fa-check"></i><b>3.4</b> Singular Value Decomposition (SVD)</a><ul>
<li class="chapter" data-level="3.4.1" data-path="3-4-linalg-SVD.html"><a href="3-4-linalg-SVD.html#examples"><i class="fa fa-check"></i><b>3.4.1</b> Examples</a></li>
</ul></li>
<li class="chapter" data-level="3.5" data-path="3-5-optimization-results.html"><a href="3-5-optimization-results.html"><i class="fa fa-check"></i><b>3.5</b> Optimization results</a></li>
<li class="chapter" data-level="3.6" data-path="3-6-best-approximating-matrices.html"><a href="3-6-best-approximating-matrices.html"><i class="fa fa-check"></i><b>3.6</b> Best approximating matrices</a><ul>
<li class="chapter" data-level="3.6.1" data-path="3-6-best-approximating-matrices.html"><a href="3-6-best-approximating-matrices.html#matrix-norms"><i class="fa fa-check"></i><b>3.6.1</b> Matrix norms</a></li>
<li class="chapter" data-level="3.6.2" data-path="3-6-best-approximating-matrices.html"><a href="3-6-best-approximating-matrices.html#eckart-young-mirsky-theorem"><i class="fa fa-check"></i><b>3.6.2</b> Eckart-Young-Mirsky Theorem</a></li>
<li class="chapter" data-level="3.6.3" data-path="3-6-best-approximating-matrices.html"><a href="3-6-best-approximating-matrices.html#example-image-compression"><i class="fa fa-check"></i><b>3.6.3</b> Example: image compression</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="" data-path="part-ii-dimension-reduction-methods.html"><a href="part-ii-dimension-reduction-methods.html"><i class="fa fa-check"></i>PART II: Dimension reduction methods</a><ul>
<li class="chapter" data-level="" data-path="part-ii-dimension-reduction-methods.html"><a href="part-ii-dimension-reduction-methods.html#a-warning"><i class="fa fa-check"></i>A warning</a></li>
<li class="chapter" data-level="3.6.4" data-path="part-ii-dimension-reduction-methods.html"><a href="part-ii-dimension-reduction-methods.html#why-reduce-dimension"><i class="fa fa-check"></i><b>3.6.4</b> Why reduce dimension?</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="4-pca.html"><a href="4-pca.html"><i class="fa fa-check"></i><b>4</b> Principal component analysis</a><ul>
<li class="chapter" data-level="4.1" data-path="4-1-principal-component-vectors-and-scores.html"><a href="4-1-principal-component-vectors-and-scores.html"><i class="fa fa-check"></i><b>4.1</b> Principal component vectors and scores</a></li>
<li class="chapter" data-level="4.2" data-path="4-2-properties-of-principal-components.html"><a href="4-2-properties-of-principal-components.html"><i class="fa fa-check"></i><b>4.2</b> Properties of principal components</a></li>
<li class="chapter" data-level="4.3" data-path="4-3-population-pca.html"><a href="4-3-population-pca.html"><i class="fa fa-check"></i><b>4.3</b> Population PCA</a></li>
<li class="chapter" data-level="4.4" data-path="4-4-an-alternative-derivation-of-pca.html"><a href="4-4-an-alternative-derivation-of-pca.html"><i class="fa fa-check"></i><b>4.4</b> An Alternative Derivation of PCA</a></li>
<li class="chapter" data-level="4.5" data-path="4-5-pca-under-transformations-of-variables.html"><a href="4-5-pca-under-transformations-of-variables.html"><i class="fa fa-check"></i><b>4.5</b> PCA under transformations of variables</a></li>
<li class="chapter" data-level="4.6" data-path="4-6-pca-based-on-boldsymbol-s-versus-pca-based-on-boldsymbol-r.html"><a href="4-6-pca-based-on-boldsymbol-s-versus-pca-based-on-boldsymbol-r.html"><i class="fa fa-check"></i><b>4.6</b> PCA based on <span class="math inline">\(\boldsymbol S\)</span> versus PCA based on <span class="math inline">\(\boldsymbol R\)</span></a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="5-cca.html"><a href="5-cca.html"><i class="fa fa-check"></i><b>5</b> Canonical Correlation Analysis</a><ul>
<li class="chapter" data-level="5.1" data-path="5-1-canonical-correlation-analysis.html"><a href="5-1-canonical-correlation-analysis.html"><i class="fa fa-check"></i><b>5.1</b> Canonical Correlation Analysis</a></li>
<li class="chapter" data-level="5.2" data-path="5-2-the-full-set-of-canonical-correlations.html"><a href="5-2-the-full-set-of-canonical-correlations.html"><i class="fa fa-check"></i><b>5.2</b> The full set of canonical correlations</a></li>
<li class="chapter" data-level="5.3" data-path="5-3-connection-with-linear-regression-when-q1.html"><a href="5-3-connection-with-linear-regression-when-q1.html"><i class="fa fa-check"></i><b>5.3</b> Connection with linear regression when <span class="math inline">\(q=1\)</span></a></li>
<li class="chapter" data-level="5.4" data-path="5-4-population-cca.html"><a href="5-4-population-cca.html"><i class="fa fa-check"></i><b>5.4</b> Population CCA</a></li>
<li class="chapter" data-level="5.5" data-path="5-5-invarianceequivariance-properties-of-cca.html"><a href="5-5-invarianceequivariance-properties-of-cca.html"><i class="fa fa-check"></i><b>5.5</b> Invariance/equivariance properties of CCA</a></li>
<li class="chapter" data-level="5.6" data-path="5-6-testing-for-zero-canonical-correlation-coefficients.html"><a href="5-6-testing-for-zero-canonical-correlation-coefficients.html"><i class="fa fa-check"></i><b>5.6</b> Testing for zero canonical correlation coefficients</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="6-mds.html"><a href="6-mds.html"><i class="fa fa-check"></i><b>6</b> Multidimensional Scaling</a><ul>
<li class="chapter" data-level="6.1" data-path="6-1-multidimensional-scaling.html"><a href="6-1-multidimensional-scaling.html"><i class="fa fa-check"></i><b>6.1</b> Multidimensional Scaling</a></li>
<li class="chapter" data-level="6.2" data-path="6-2-principal-coordinates.html"><a href="6-2-principal-coordinates.html"><i class="fa fa-check"></i><b>6.2</b> Principal Coordinates</a></li>
<li class="chapter" data-level="6.3" data-path="6-3-similarity-measures.html"><a href="6-3-similarity-measures.html"><i class="fa fa-check"></i><b>6.3</b> Similarity measures</a></li>
</ul></li>
<li class="divider"></li>
<li> <a href="https://rich-d-wilkinson.github.io/teaching.html" target="blank">University of Nottingham</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Multivariate Statistics</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="properties-of-principal-components" class="section level2">
<h2><span class="header-section-number">4.2</span> Properties of principal components</h2>
<p>Let <span class="math inline">\(\boldsymbol x_1, \ldots, \boldsymbol x_n\)</span> have sample mean <span class="math inline">\(\bar{\boldsymbol x}\)</span> and sample covariance matrix <span class="math inline">\(\boldsymbol S\)</span>, with spectral decomposition <span class="math inline">\(\boldsymbol S=\boldsymbol Q\boldsymbol \Lambda\boldsymbol Q^\top\)</span> where
<span class="math inline">\(\boldsymbol Q=[\boldsymbol q_1, \ldots , \boldsymbol q_p]\)</span> is orthogonal and <span class="math inline">\(\boldsymbol \Lambda=\text{diag}\{\lambda_1, \ldots , \lambda_p\}\)</span>. The transformed variables have some important properties.
0.2truein</p>

<div class="proposition">
<p><span id="prp:pca2" class="proposition"><strong>Proposition 4.2  </strong></span>For <span class="math inline">\(j,k=1, \ldots , p\)</span>, the following results hold.</p>
<ol style="list-style-type: decimal">
<li><span class="math inline">\(\bar{y}_{+j} = n^{-1} \sum_{i=1}^n y_{ij}=n^{-1}\sum_{i=1}^n \boldsymbol q_j^\top (\boldsymbol x_i-\bar{\boldsymbol x})=0\)</span>;</li>
<li><span class="math inline">\(\boldsymbol q_j^\top \boldsymbol S\boldsymbol q_j = \lambda_j\)</span>;</li>
<li><span class="math inline">\(\boldsymbol q_j^\top \boldsymbol S\boldsymbol q_k = 0\)</span> for <span class="math inline">\(j \neq k\)</span>;</li>
<li><span class="math inline">\(\boldsymbol q_1^\top \boldsymbol S\boldsymbol q_1 \geq \boldsymbol q_2^\top \boldsymbol S\boldsymbol q_2 \geq \ldots \geq \boldsymbol q_p^\top \boldsymbol S\boldsymbol q_p\geq 0\)</span>;</li>
<li><span class="math inline">\(\sum_{j=1}^p \boldsymbol q_j^\top \boldsymbol S\boldsymbol q_j = \sum_{j=1}^p \lambda_j = \text{tr}(\boldsymbol S)\)</span>;</li>
<li><span class="math inline">\(\prod_{j=1}^p \boldsymbol q_j^\top \boldsymbol S\boldsymbol q_j = \prod_{j=1}^p \lambda_j = |\boldsymbol S|\)</span>.</li>
</ol>
</div>

<p>In words:</p>
<ul>
<li><p>part 1. tells us that the sample mean of <span class="math inline">\(y_{1j}, \ldots , y_{nj}\)</span> for each fixed <span class="math inline">\(j\)</span> is <span class="math inline">\(0\)</span>;</p></li>
<li><p>part 2. tells us that, for each fixed <span class="math inline">\(j\)</span>, the sample variance of
the <span class="math inline">\(y_{ij},\, i=1, \ldots , n\)</span> is <span class="math inline">\(\lambda_j\)</span>;</p></li>
<li><p>part 3. states that the sample covariance of the pairs <span class="math inline">\((y_{ij}, y_{ik})\)</span>, <span class="math inline">\(i=1, \ldots , n\)</span>, is <span class="math inline">\(0\)</span> if <span class="math inline">\(j \neq k\)</span>;</p></li>
<li><p>part 4. states that the sample variance of <span class="math inline">\(y_{ij}, \, i=1, \ldots , n\)</span>, is not less than the sample variance of <span class="math inline">\(y_{ik}, \, i=1, \ldots , n\)</span>, if <span class="math inline">\(j\leq k\)</span>;</p></li>
<li><p>part 5. states that the sum of the sample variances is equal to the trace of <span class="math inline">\(\boldsymbol S\)</span>;</p></li>
<li><p>and part 6. states that the product of the sample variances is equal to the determinant of <span class="math inline">\(\boldsymbol S\)</span>.</p></li>
</ul>
<p>From these properties we say that a proportion
<span class="math display">\[\frac{\lambda_j}{\lambda_1 + \ldots + \lambda_p}\]</span>
of the variability in the sample is `explainedâ by the <span class="math inline">\(j\)</span>th PC.</p>
<p>For the G11PRB and G11STA data above,
<span class="math display">\[\frac{\lambda_1}{\lambda_1 + \lambda_2} = \frac{304.24}{304.24+33.16} = 0.90,\]</span>
so 90% of the variability in the sample is explained by the 1st PC.</p>
<p>```{Example} We can apply PCA to a football league table where <span class="math inline">\(W\)</span>, <span class="math inline">\(D\)</span>, <span class="math inline">\(L\)</span> are the number of matches won, drawn and lost and <span class="math inline">\(F\)</span> and <span class="math inline">\(A\)</span> are the goals scored for and against. An extract of the table for a recent Premiership season is:
FIX FIX</p>
<table>
<thead>
<tr>
<th style="text-align:left;">
Team
</th>
<th style="text-align:right;">
W
</th>
<th style="text-align:right;">
D
</th>
<th style="text-align:right;">
L
</th>
<th style="text-align:right;">
F
</th>
<th style="text-align:right;">
A
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left;">
Chelsea
</td>
<td style="text-align:right;">
27
</td>
<td style="text-align:right;">
5
</td>
<td style="text-align:right;">
6
</td>
<td style="text-align:right;">
103
</td>
<td style="text-align:right;">
32
</td>
</tr>
<tr>
<td style="text-align:left;">
Manchester United
</td>
<td style="text-align:right;">
27
</td>
<td style="text-align:right;">
4
</td>
<td style="text-align:right;">
7
</td>
<td style="text-align:right;">
86
</td>
<td style="text-align:right;">
28
</td>
</tr>
<tr>
<td style="text-align:left;">
Arsenal
</td>
<td style="text-align:right;">
23
</td>
<td style="text-align:right;">
6
</td>
<td style="text-align:right;">
9
</td>
<td style="text-align:right;">
83
</td>
<td style="text-align:right;">
41
</td>
</tr>
<tr>
<td style="text-align:left;">
Tottenham Hotspur
</td>
<td style="text-align:right;">
21
</td>
<td style="text-align:right;">
7
</td>
<td style="text-align:right;">
10
</td>
<td style="text-align:right;">
67
</td>
<td style="text-align:right;">
41
</td>
</tr>
<tr>
<td style="text-align:left;">
Manchester City
</td>
<td style="text-align:right;">
18
</td>
<td style="text-align:right;">
13
</td>
<td style="text-align:right;">
7
</td>
<td style="text-align:right;">
73
</td>
<td style="text-align:right;">
45
</td>
</tr>
</tbody>
</table>
<p>The sample mean vector is</p>
<p><span class="math display">\[\bar{\boldsymbol x} =\begin{pmatrix}14.2 \\9.6 \\14.2 \\52.6 \\52.6 \\\end{pmatrix}\]</span></p>
<!--$$\bar{\bx} = \begin{pmatrix} 14.20 \\ 9.60 \\ 14.20 \\ 52.65 \\ 52.65 \end{pmatrix},$$-->
<p>and the sample covariance matrix is</p>
<p><span class="math display" id="eq:PLES">\[\begin{equation}
\boldsymbol S= \begin{pmatrix}39.4&amp;-8.27&amp;-31.1&amp;116&amp;-81.9 \\-8.27&amp;8.14&amp;0.13&amp;-29.4&amp;6.01 \\-31.1&amp;0.13&amp;31&amp;-86.3&amp;75.9 \\116&amp;-29.4&amp;-86.3&amp;392&amp;-209 \\-81.9&amp;6.01&amp;75.9&amp;-209&amp;231 \\\end{pmatrix}
\tag{4.3}
\end{equation}\]</span></p>
<!--\begin{equation}
\bS = \begin{pmatrix} 39.4 & -8.3 & -31.1 & 115.7 & -81.9 \\
-8.3 & 8.1 & 0.1 & -29.4 & 6.0 \\
-31.1 & 0.1 & 31.0 & -86.3 & 75.9 \\
115.7 & -29.4 & -86.3 & 392.2 & -208.7 \\
-81.9 & 6.0 & 75.9 & -208.7 & 230.9 \end{pmatrix}.
(\#eq:PLES)
\end{equation}-->
<p>The eigenvalues of <span class="math inline">\(\boldsymbol S\)</span> are
<span class="math display">\[\boldsymbol \Lambda= \text{diag}\begin{pmatrix}631&amp;96.7&amp;8.83&amp;2.44&amp;-4.97e-14 \\\end{pmatrix}\]</span></p>
<!--$$\bLambda = \tdiag(599.06, 91.85, 8.39, 2.32, 0.00).$$-->
<p>Note that we have a zero eigenvalue because one of our variables is a linear combination of the other variables, <span class="math inline">\(L = 38 - W - D\)</span>. The corresponding eigenvectors are
<span class="math display">\[\boldsymbol Q= [\boldsymbol q_1 \ldots \boldsymbol q_5] =\begin{pmatrix}0.251&amp;-0.0133&amp;-0.116&amp;0.768&amp;0.577 \\-0.0477&amp;-0.146&amp;0.74&amp;-0.309&amp;0.577 \\-0.204&amp;0.16&amp;-0.624&amp;-0.459&amp;0.577 \\0.776&amp;0.582&amp;0.0674&amp;-0.234&amp;-2e-15 \\-0.539&amp;0.784&amp;0.213&amp;0.222&amp;1.83e-15 \\\end{pmatrix}\]</span></p>
<!--$$\bQ = [\bq_1 \ldots \bq_5] =
\begin{pmatrix} 0.25 & 0.01 & -0.12 & 0.77 & 0.58 \\
-0.05 & 0.15 & 0.74 & -0.31 & 0.58 \\
-0.20 & -0.16 & -0.62 & -0.46 & 0.58 \\
0.78 & -0.58 & 0.07 & -0.23 & 0.00 \\
-0.54 & -0.78 & 0.21 & 0.22 & 0.00 \end{pmatrix}.$$-->
<p>The proportion of variability explained by each of the PCs is:
<span class="math display">\[
\begin{pmatrix}0.854&amp;0.131&amp;0.012&amp;0.0033&amp;-6.73e-17 \\\end{pmatrix}
\]</span></p>
<!--$$
%(0.854, 0.131, 0.012, 0.003, 0.000).
$$-->
<p>There is no point computing the scores for PC 5 because PC5 does not explain any of the variability in the data. Similarly, there is little value in computing the scores for PCs 3 &amp; 4 because they only account for 1.5% of the variability in the data.</p>
<p>We can, therefore, choose to compute only the first two PC scores. We are reducing the dimension of our data set from <span class="math inline">\(p=5\)</span> to <span class="math inline">\(p=2\)</span> while still retaining 98.5% of the variability. The first PC is given by:
<span class="math display">\[\begin{align*}
y_{i1} &amp;= 0.25(W_i-\bar{W}) +-0.05(D_i-\bar{D}) +-0.2(L_i-\bar{L})\\
&amp; \qquad +0.78(F_i-\bar{F}) +-0.54(A_i-\bar{A}),
\end{align*}\]</span>
and similarly for PC 2.</p>
<!--\begin{align*}
y_{i1} &= 0.25(W_i-\bar{W}) -0.05(D_i-\bar{D}) -0.20(L_i-\bar{L})\\
& \qquad +0.78(F_i-\bar{F}) -0.54(A_i-\bar{A}),
\end{align*}-->
The first five rows of our revised ``league tableââ are now
<table class="table" style="width: auto !important; margin-left: auto; margin-right: auto;">
<thead>
<tr>
<th style="text-align:left;">
Team
</th>
<th style="text-align:right;">
PC1
</th>
<th style="text-align:right;">
PC2
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left;">
Chelsea
</td>
<td style="text-align:right;">
55.3
</td>
<td style="text-align:right;">
12.3
</td>
</tr>
<tr>
<td style="text-align:left;">
Manchester United
</td>
<td style="text-align:right;">
44.1
</td>
<td style="text-align:right;">
-0.4
</td>
</tr>
<tr>
<td style="text-align:left;">
Arsenal
</td>
<td style="text-align:right;">
33.3
</td>
<td style="text-align:right;">
8.1
</td>
</tr>
<tr>
<td style="text-align:left;">
Tottenham Hotspur
</td>
<td style="text-align:right;">
20.1
</td>
<td style="text-align:right;">
-1.2
</td>
</tr>
<tr>
<td style="text-align:left;">
Manchester City
</td>
<td style="text-align:right;">
22.2
</td>
<td style="text-align:right;">
4.1
</td>
</tr>
</tbody>
</table>
<!--FIX FIX
\begin{tabular}{l|rr}
Team &      PC 1 & PC 2 \\ \hline
Chelsea &   55.3 & -12.3 \\
Man Utd &   44.1 & 0.4 \\
$\vdots$ && $\vdots$ \\
Portsmouth &    -25.4 & -1.7
\end{tabular}-->
<p>Now that we have reduced the dimension to <span class="math inline">\(p=2\)</span>, we can visualise the differences between the teams.</p>
<p><img src="04-pca_files/figure-html/unnamed-chunk-9-1.png" width="672" /></p>
<!--\begin{center}
\includegraphics[width=12cm,angle=0 ]{figs/premleag_pca1.pdf}
\end{center}-->
<p>We might interpret the PCs as follows. The first PC seems to measure overall performance. It rewards teams with 0.78 for every goal they score and 0.25 for every match they win, while penalising them by 0.54 for every goal they concede, 0.2 for every match they lose and 0.05 for every match they draw.</p>
<p>We could, therefore, rank teams by PC 1 and compare this with the rankings using 3 points for a win and 1 point for a draw. The rankings are the same for the top three teams but differ below that. Under our system Wigan would be relegated in place of Portsmouth.</p>
<p>The second PC has a strong negative loading for both goals for and against. A team with a large negative PC 2 score was, therefore, involved in matches with lots of goals. We could, therefore, interpret PC 2 as an ``entertainmentââ measure, ranking teams according to their involvement in high-scoring games.</p>
<p>The above example raises the question of how many PCs should we use in practice. If we reduce the dimension to <span class="math inline">\(p=1\)</span> then we can rank observations and analyse our new variable with univariate statistics. If we reduce the dimension to <span class="math inline">\(p=2\)</span> then it is still easy to visualise the data. However, reducing the dimension to <span class="math inline">\(p=1\)</span> or <span class="math inline">\(p=2\)</span> may involve losing lots of information and a sensible answer should depend on the objectives of the analysis and the data itself.</p>
<p>One tool for looking at the contributions of each PC is to look at the <strong>scree graph</strong> which plots the percentage of variance explained by PC <span class="math inline">\(j\)</span> against <span class="math inline">\(j\)</span>. The scree graph for the football example is:</p>
<p><img src="04-pca_files/figure-html/unnamed-chunk-10-1.png" width="672" /></p>
<!--\begin{center}
\includegraphics[width=12cm,angle=0]{figs/premleag_pca2.pdf}
\end{center}-->
<p>Possible methods for choosing the number of PCs include:</p>
<ul>
<li>retain enough PCs to explain, say, 90% of the total variation;</li>
<li>retain PCs where the eigenvalue is above the average.</li>
</ul>
<p>For the football example, the first method would retain 2 PCs whereas the second method would only retain 1 PC.</p>
<p>```</p>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="4-1-principal-component-vectors-and-scores.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="4-3-population-pca.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["MATH3030.pdf"],
"toc": {
"collapse": "section"
},
"pandoc_args": "--top-level-division=[chapter|part]"
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
