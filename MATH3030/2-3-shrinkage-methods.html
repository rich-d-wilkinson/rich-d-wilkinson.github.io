<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>2.3 Shrinkage methods | Multivariate Statistics</title>
  <meta name="description" content="The lecture notes for MATH3030/4068: Multivariate Analysis / Applied Multivariate Statistics" />
  <meta name="generator" content="bookdown 0.21 and GitBook 2.6.7" />

  <meta property="og:title" content="2.3 Shrinkage methods | Multivariate Statistics" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="The lecture notes for MATH3030/4068: Multivariate Analysis / Applied Multivariate Statistics" />
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="2.3 Shrinkage methods | Multivariate Statistics" />
  
  <meta name="twitter:description" content="The lecture notes for MATH3030/4068: Multivariate Analysis / Applied Multivariate Statistics" />
  

<meta name="author" content="Prof. Richard Wilkinson" />



  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="2-2-principal-component-regression-pcr.html"/>
<link rel="next" href="2-4-multi-output-linear-model.html"/>
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />











<style type="text/css">
a.sourceLine { display: inline-block; line-height: 1.25; }
a.sourceLine { pointer-events: none; color: inherit; text-decoration: inherit; }
a.sourceLine:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode { white-space: pre; position: relative; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
code.sourceCode { white-space: pre-wrap; }
a.sourceLine { text-indent: -1em; padding-left: 1em; }
}
pre.numberSource a.sourceLine
  { position: relative; left: -4em; }
pre.numberSource a.sourceLine::before
  { content: attr(data-line-number);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; pointer-events: all; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {  }
@media screen {
a.sourceLine::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Applied multivariate statistics</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Introduction</a></li>
<li class="chapter" data-level="1" data-path="1-cca.html"><a href="1-cca.html"><i class="fa fa-check"></i><b>1</b> Canonical Correlation Analysis (CCA)</a><ul>
<li class="chapter" data-level="1.1" data-path="1-1-cca1.html"><a href="1-1-cca1.html"><i class="fa fa-check"></i><b>1.1</b> The first pair of canonical variables</a><ul>
<li class="chapter" data-level="1.1.1" data-path="1-1-cca1.html"><a href="1-1-cca1.html#the-first-canonical-components"><i class="fa fa-check"></i><b>1.1.1</b> The first canonical components</a></li>
<li class="chapter" data-level="1.1.2" data-path="1-1-cca1.html"><a href="1-1-cca1.html#premcca"><i class="fa fa-check"></i><b>1.1.2</b> Example: Premier league football</a></li>
</ul></li>
<li class="chapter" data-level="1.2" data-path="1-2-the-full-set-of-canonical-correlations.html"><a href="1-2-the-full-set-of-canonical-correlations.html"><i class="fa fa-check"></i><b>1.2</b> The full set of canonical correlations</a><ul>
<li class="chapter" data-level="1.2.1" data-path="1-2-the-full-set-of-canonical-correlations.html"><a href="1-2-the-full-set-of-canonical-correlations.html#example-continued"><i class="fa fa-check"></i><b>1.2.1</b> Example continued</a></li>
</ul></li>
<li class="chapter" data-level="1.3" data-path="1-3-properties.html"><a href="1-3-properties.html"><i class="fa fa-check"></i><b>1.3</b> Properties</a><ul>
<li class="chapter" data-level="1.3.1" data-path="1-3-properties.html"><a href="1-3-properties.html#connection-with-linear-regression-when-q1"><i class="fa fa-check"></i><b>1.3.1</b> Connection with linear regression when <span class="math inline">\(q=1\)</span></a></li>
<li class="chapter" data-level="1.3.2" data-path="1-3-properties.html"><a href="1-3-properties.html#invarianceequivariance-properties-of-cca"><i class="fa fa-check"></i><b>1.3.2</b> Invariance/equivariance properties of CCA</a></li>
</ul></li>
<li class="chapter" data-level="1.4" data-path="1-4-computer-tasks.html"><a href="1-4-computer-tasks.html"><i class="fa fa-check"></i><b>1.4</b> Computer tasks</a></li>
<li class="chapter" data-level="1.5" data-path="1-5-exercises.html"><a href="1-5-exercises.html"><i class="fa fa-check"></i><b>1.5</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="2-lm.html"><a href="2-lm.html"><i class="fa fa-check"></i><b>2</b> Linear Models</a><ul>
<li class="chapter" data-level="" data-path="2-lm.html"><a href="2-lm.html#notation"><i class="fa fa-check"></i>Notation</a></li>
<li class="chapter" data-level="2.1" data-path="2-1-ordinary-least-squares-ols.html"><a href="2-1-ordinary-least-squares-ols.html"><i class="fa fa-check"></i><b>2.1</b> Ordinary least squares (OLS)</a><ul>
<li class="chapter" data-level="2.1.1" data-path="2-1-ordinary-least-squares-ols.html"><a href="2-1-ordinary-least-squares-ols.html#geometry"><i class="fa fa-check"></i><b>2.1.1</b> Geometry</a></li>
<li class="chapter" data-level="2.1.2" data-path="2-1-ordinary-least-squares-ols.html"><a href="2-1-ordinary-least-squares-ols.html#normal-linear-model"><i class="fa fa-check"></i><b>2.1.2</b> Normal linear model</a></li>
<li class="chapter" data-level="2.1.3" data-path="2-1-ordinary-least-squares-ols.html"><a href="2-1-ordinary-least-squares-ols.html#linear-models-in-r"><i class="fa fa-check"></i><b>2.1.3</b> Linear models in R</a></li>
<li class="chapter" data-level="2.1.4" data-path="2-1-ordinary-least-squares-ols.html"><a href="2-1-ordinary-least-squares-ols.html#problems-with-ols"><i class="fa fa-check"></i><b>2.1.4</b> Problems with OLS</a></li>
</ul></li>
<li class="chapter" data-level="2.2" data-path="2-2-principal-component-regression-pcr.html"><a href="2-2-principal-component-regression-pcr.html"><i class="fa fa-check"></i><b>2.2</b> Principal component regression (PCR)</a><ul>
<li class="chapter" data-level="2.2.1" data-path="2-2-principal-component-regression-pcr.html"><a href="2-2-principal-component-regression-pcr.html#pcr-in-r"><i class="fa fa-check"></i><b>2.2.1</b> PCR in R</a></li>
</ul></li>
<li class="chapter" data-level="2.3" data-path="2-3-shrinkage-methods.html"><a href="2-3-shrinkage-methods.html"><i class="fa fa-check"></i><b>2.3</b> Shrinkage methods</a><ul>
<li class="chapter" data-level="2.3.1" data-path="2-3-shrinkage-methods.html"><a href="2-3-shrinkage-methods.html#ridge-regression-in-r"><i class="fa fa-check"></i><b>2.3.1</b> Ridge regression in R</a></li>
</ul></li>
<li class="chapter" data-level="2.4" data-path="2-4-multi-output-linear-model.html"><a href="2-4-multi-output-linear-model.html"><i class="fa fa-check"></i><b>2.4</b> Multi-output Linear Model</a><ul>
<li class="chapter" data-level="2.4.1" data-path="2-4-multi-output-linear-model.html"><a href="2-4-multi-output-linear-model.html#normal-linear-model-1"><i class="fa fa-check"></i><b>2.4.1</b> Normal linear model</a></li>
<li class="chapter" data-level="2.4.2" data-path="2-4-multi-output-linear-model.html"><a href="2-4-multi-output-linear-model.html#reduced-rank-regression"><i class="fa fa-check"></i><b>2.4.2</b> Reduced rank regression</a></li>
</ul></li>
</ul></li>
<li class="divider"></li>
<li> <a href="https://rich-d-wilkinson.github.io/teaching.html" target="blank">University of Nottingham</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Multivariate Statistics</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="shrinkage-methods" class="section level2">
<h2><span class="header-section-number">2.3</span> Shrinkage methods</h2>
<p>Another approach to fitting linear models is to regularise the parameters. Instead of minimizing <span class="math inline">\(||\mathbf y- \mathbf X\boldsymbol \beta||^2_2\)</span>, we instead minimize</p>
<p><span class="math display">\[||\mathbf y- \mathbf X\boldsymbol \beta||^2_2+\lambda||\boldsymbol \beta||\]</span>
where we have added a penalty term <span class="math inline">\(\lambda||\boldsymbol \beta||\)</span> to constrain the size of the parameter (<span class="math inline">\(\lambda\geq 0\)</span>). This stops estimated values of <span class="math inline">\(\boldsymbol \beta\)</span> becoming too large.
Larger values of <span class="math inline">\(\lambda\)</span> penalise the size of <span class="math inline">\(\boldsymbol \beta\)</span> more strongly, and so have a bigger effect. As <span class="math inline">\(\lambda \rightarrow \infty\)</span> we find the optimal <span class="math inline">\(\boldsymbol \beta\)</span> tends to <span class="math inline">\(\boldsymbol 0\)</span>.</p>
<p><strong>Ridge regression</strong> is the name given to the estimate found when we use the <span class="math inline">\(||\cdot||_2\)</span> norm.
<span class="math display">\[\hat{\boldsymbol \beta}^{ridge} = \arg \min_{\boldsymbol \beta} \left\{||\mathbf y- \mathbf X\boldsymbol \beta||^2_2+\lambda||\boldsymbol \beta||_2^2 \right\}.\]</span>
You’ll show on the example sheets that</p>
<p><span class="math display">\[\hat{\boldsymbol \beta}^{ridge} = (\mathbf X^\top \mathbf X+ \lambda\mathbf I)^{-1}\mathbf X^\top \mathbf y\]</span></p>
<p>We can also show that the ridge regression estimator is small than the OLS estimator
<span class="math display">\[||\hat{\boldsymbol \beta}^{ols}||_2 \geq ||\hat{\boldsymbol \beta}^{ridge}||_2\]</span>
We say that ridge regression <em>shrinks</em> the parameter estimates towards zero. The motivation for this is that when there are many correlated covariates, we sometimes find that the coefficients are poorly determined, and so the OLS estimator can have high variance. As <a href="https://web.stanford.edu/~hastie/Papers/ESLII.pdf">Hastie et al.</a> put it</p>
<blockquote>
<p>“A wildly large positive coefficient on one variable can be cancelled by a large negative coefficient on its correlated cousin. By imposing a size constraint on the coefficients […] this phenomenon is prevented from occurring.”</p>
</blockquote>
<p>The fitted values are again <span class="math inline">\(\hat{\mathbf y}^{ridge} = \mathbf X\hat{\boldsymbol \beta}^{ridge}\)</span>.
If we substitute the SVD for <span class="math inline">\(\mathbf X\)</span> we find</p>
<p><span class="math display">\[\begin{align*}
\hat{\mathbf y}^{ridge}&amp;=\mathbf X\hat{\boldsymbol \beta}^{ridge}\\
&amp;=\mathbf X(\mathbf X^\top \mathbf X+ \lambda\mathbf I)^{-1}\mathbf X^\top \mathbf y\\
&amp;=\mathbf U\boldsymbol{\Sigma}\mathbf V^\top(\mathbf V\boldsymbol{\Sigma}^2 \mathbf V^\top +\lambda\mathbf I)^{-1}\mathbf V\boldsymbol{\Sigma}\mathbf U^\top \mathbf y\\
&amp;=\mathbf U\boldsymbol{\Sigma}\mathbf V^\top(\mathbf V(\boldsymbol{\Sigma}^2  +\lambda\mathbf I)\mathbf V^\top)^{-1}\mathbf V\boldsymbol{\Sigma}\mathbf U^\top \mathbf y\\
&amp;=\mathbf U\boldsymbol{\Sigma}(\boldsymbol{\Sigma}^2  +\lambda\mathbf I)^{-1}\boldsymbol{\Sigma}\mathbf U^\top \mathbf y\\
&amp;= \sum_{i=1}^p \frac{\sigma^2_i}{\sigma^2_i+\lambda} \mathbf u_i \mathbf u_i^\top \mathbf y
\end{align*}\]</span>
The penultimate line has assumed <span class="math inline">\(\mathbf V\)</span> is full rank here, but the argument can be made to work regardless. Since <span class="math inline">\(\lambda\geq 0\)</span>, we have that <span class="math inline">\(\frac{\sigma^2_i}{\sigma^2_i+\lambda}\leq 1\)</span>. So we can see that just like in OLS and PCR, we project the data onto the orthonormal basis <span class="math inline">\(\mathbf U\)</span>, but with ridge regression, we shrink the coefficients by the factors <span class="math inline">\(\frac{\sigma^2_i}{\sigma^2_i+\lambda}\leq 1\)</span>. A greater amount of shrinkage is applied to basis vectors with small singular values <span class="math inline">\(\sigma_i\)</span>.</p>
<p>If we compare the three different expressions for the fitted values
<span class="math display">\[\begin{align*}
\hat{\mathbf y}^{ols}&amp;=\sum_{i=1}^p \mathbf u_i \mathbf u_i^\top \mathbf y,\\
\hat{\mathbf y}^{ridge}&amp;=\sum_{i=1}^k \mathbf u_i \mathbf u_i^\top \mathbf y\\
\hat{\mathbf y}^{ridge}&amp;=\sum_{i=1}^p \frac{\sigma^2_i}{\sigma^2_i+\lambda} \mathbf u_i \mathbf u_i^\top \mathbf y
\end{align*}\]</span>
we can see that PCR just works in a lower dimensional space discarding dimensions with small singular values, whereas ridge regression retains all coordinates, but reduces the importance of the coordinates with small singular values.</p>
<p>In ridge regression we used the <span class="math inline">\(||\cdot||_2\)</span> norm to penalise the parameter size. Other methods use other norms, which can induce interesting effects. Of particular interest is the <strong>lasso</strong>, which uses the <span class="math inline">\(L_1\)</span> norm as a penalty <span class="math inline">\(||\boldsymbol \beta||_1\)</span>. This induces sparsity in the solution, but is beyond the scope of the module. If you are interested, take a look at the <a href="https://en.wikipedia.org/wiki/Lasso_(statistics)">wikipedia page</a></p>
<div id="ridge-regression-in-r" class="section level3">
<h3><span class="header-section-number">2.3.1</span> Ridge regression in R</h3>
<p>We’ll use the <code>glmnet</code> package to do ridge regression. The <code>glmnet</code> command solves the optimization problem
<span class="math display">\[\arg \min_{\boldsymbol \beta} \left\{||\mathbf y- \mathbf X\boldsymbol \beta||^2_2+\lambda \left((1-\alpha)||\boldsymbol \beta||_2^2+\alpha ||\boldsymbol \beta||_1^2\right)\right\}\]</span>
where we must specify the value of <span class="math inline">\(\alpha\)</span>.
Taking <span class="math inline">\(\alpha=0\)</span> gives us ridge regression, and <span class="math inline">\(\alpha=1\)</span> gives the lasso. Values of <span class="math inline">\(\alpha \in (0,1)\)</span> balance both the <span class="math inline">\(L_1\)</span> and <span class="math inline">\(L_2\)</span> penalties (a method known as the <strong>elastic-net</strong>).</p>
<p>We can choose to specify what values of <span class="math inline">\(\lambda\)</span> to use (if we don’t specify this, the <code>glmnet</code> package will pick some sensible values). Remember that larger values of <span class="math inline">\(\lambda\)</span> result in a bigger penalty, and greater shrinkage of the parameters.</p>
<div class="sourceCode" id="cb37"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb37-1" data-line-number="1"><span class="kw">library</span>(glmnet)</a>
<a class="sourceLine" id="cb37-2" data-line-number="2">X =<span class="st"> </span><span class="kw">as.matrix</span>(iris[,<span class="dv">2</span><span class="op">:</span><span class="dv">4</span>]) <span class="co"># glmnet doesn&#39;t work with data frames</span></a>
<a class="sourceLine" id="cb37-3" data-line-number="3">y=iris[,<span class="dv">1</span>]</a>
<a class="sourceLine" id="cb37-4" data-line-number="4">lambdas &lt;-<span class="st"> </span><span class="dv">10</span><span class="op">^</span><span class="kw">seq</span>(<span class="dv">3</span>, <span class="dv">-4</span>, <span class="dt">by =</span> <span class="fl">-.1</span>)</a>
<a class="sourceLine" id="cb37-5" data-line-number="5">iris.ridge &lt;-<span class="st"> </span><span class="kw">glmnet</span>(X,y, <span class="dt">alpha=</span><span class="dv">0</span>, <span class="dt">lambda =</span> lambdas)</a></code></pre></div>
<p>We can then look at how the parameter estimates change as the size of <span class="math inline">\(\lambda\)</span> changes.</p>
<div class="sourceCode" id="cb38"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb38-1" data-line-number="1"><span class="kw">plot</span>(iris.ridge, <span class="dt">xvar=</span><span class="st">&#39;lambda&#39;</span>) </a></code></pre></div>
<p><img src="08-multivarLM_files/figure-html/unnamed-chunk-7-1.png" width="672" /></p>
<div class="sourceCode" id="cb39"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb39-1" data-line-number="1"><span class="co">#matplot(log(lambdas), t(iris.ridge$beta), type=&#39;l&#39;) # does the same</span></a></code></pre></div>
<p>We can see that as <span class="math inline">\(\lambda\)</span> grows large, the parameters all shrink to <span class="math inline">\(0\)</span> as expected. For <span class="math inline">\(\lambda\approx 0\)</span>, we get the OLS estimates of the parameters</p>
<div class="sourceCode" id="cb40"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb40-1" data-line-number="1"><span class="kw">coef</span>(<span class="kw">lm</span>(y<span class="op">~</span>X))</a></code></pre></div>
<pre><code>##   (Intercept)  XSepal.Width XPetal.Length  XPetal.Width 
##     1.8559975     0.6508372     0.7091320    -0.5564827</code></pre>
<p>We usually choose <span class="math inline">\(\lambda\)</span> by finding which value gives the lower prediction error (using some form of predictive test, such as cross-validation).</p>
<div class="sourceCode" id="cb42"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb42-1" data-line-number="1">cv_fit &lt;-<span class="st"> </span><span class="kw">cv.glmnet</span>(X, y, <span class="dt">alpha =</span> <span class="dv">0</span>, <span class="dt">lambda =</span> lambdas)</a>
<a class="sourceLine" id="cb42-2" data-line-number="2"><span class="kw">plot</span>(cv_fit)</a></code></pre></div>
<p><img src="08-multivarLM_files/figure-html/unnamed-chunk-9-1.png" width="672" /></p>
<p>From this, we can see that very small values of <span class="math inline">\(\lambda\)</span> work best here, i.e., the OLS estimator is optimal. This is not a surprise as there are only three covariates in this problem. It is in larger problems that we expect shrinkage methods to be most useful.</p>
<p>The value of lambda that minimizes the prediction error can be found by</p>
<div class="sourceCode" id="cb43"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb43-1" data-line-number="1">cv_fit<span class="op">$</span>lambda.min </a></code></pre></div>
<pre><code>## [1] 0.001</code></pre>
<p>and the largest value of lambda that gives a prediction error within one standard deviation of the minimum can be found using:</p>
<div class="sourceCode" id="cb45"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb45-1" data-line-number="1">cv_fit<span class="op">$</span>lambda<span class="fl">.1</span>se</a></code></pre></div>
<pre><code>## [1] 0.01995262</code></pre>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="2-2-principal-component-regression-pcr.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="2-4-multi-output-linear-model.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["MultivariateStatistics.pdf"],
"toc": {
"collapse": "section"
},
"pandoc_args": "--top-level-division=[chapter|part]"
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
