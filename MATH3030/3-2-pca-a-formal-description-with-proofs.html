<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>3.2 PCA: a formal description with proofs | Multivariate Statistics</title>
  <meta name="description" content="The lecture notes for MATH3030/4068: Multivariate Analysis / Applied Multivariate Statistics" />
  <meta name="generator" content="bookdown 0.21 and GitBook 2.6.7" />

  <meta property="og:title" content="3.2 PCA: a formal description with proofs | Multivariate Statistics" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="The lecture notes for MATH3030/4068: Multivariate Analysis / Applied Multivariate Statistics" />
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="3.2 PCA: a formal description with proofs | Multivariate Statistics" />
  
  <meta name="twitter:description" content="The lecture notes for MATH3030/4068: Multivariate Analysis / Applied Multivariate Statistics" />
  

<meta name="author" content="Prof.Â Richard Wilkinson" />



  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="3-1-pca-an-informal-introduction.html"/>
<link rel="next" href="3-3-an-alternative-view-of-pca.html"/>
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />









<script src="libs/kePrint-0.0.1/kePrint.js"></script>
<link href="libs/lightable-0.0.1/lightable.css" rel="stylesheet" />


<style type="text/css">
a.sourceLine { display: inline-block; line-height: 1.25; }
a.sourceLine { pointer-events: none; color: inherit; text-decoration: inherit; }
a.sourceLine:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode { white-space: pre; position: relative; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
code.sourceCode { white-space: pre-wrap; }
a.sourceLine { text-indent: -1em; padding-left: 1em; }
}
pre.numberSource a.sourceLine
  { position: relative; left: -4em; }
pre.numberSource a.sourceLine::before
  { content: attr(data-line-number);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; pointer-events: all; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {  }
@media screen {
a.sourceLine::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Applied multivariate statistics</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Introduction</a></li>
<li class="chapter" data-level="1" data-path="1-linalg-prelim.html"><a href="1-linalg-prelim.html"><i class="fa fa-check"></i><b>1</b> Review of linear algebra</a><ul>
<li class="chapter" data-level="1.1" data-path="1-1-linalg-basics.html"><a href="1-1-linalg-basics.html"><i class="fa fa-check"></i><b>1.1</b> Basics</a><ul>
<li class="chapter" data-level="1.1.1" data-path="1-1-linalg-basics.html"><a href="1-1-linalg-basics.html#notation"><i class="fa fa-check"></i><b>1.1.1</b> Notation</a></li>
<li class="chapter" data-level="1.1.2" data-path="1-1-linalg-basics.html"><a href="1-1-linalg-basics.html#elementary-matrix-operations"><i class="fa fa-check"></i><b>1.1.2</b> Elementary matrix operations</a></li>
<li class="chapter" data-level="1.1.3" data-path="1-1-linalg-basics.html"><a href="1-1-linalg-basics.html#special-matrices"><i class="fa fa-check"></i><b>1.1.3</b> Special matrices</a></li>
<li class="chapter" data-level="1.1.4" data-path="1-1-linalg-basics.html"><a href="1-1-linalg-basics.html#vectordiff"><i class="fa fa-check"></i><b>1.1.4</b> Vector Differentiation</a></li>
</ul></li>
<li class="chapter" data-level="1.2" data-path="1-2-linalg-vecspaces.html"><a href="1-2-linalg-vecspaces.html"><i class="fa fa-check"></i><b>1.2</b> Vector spaces</a><ul>
<li class="chapter" data-level="1.2.1" data-path="1-2-linalg-vecspaces.html"><a href="1-2-linalg-vecspaces.html#linear-independence"><i class="fa fa-check"></i><b>1.2.1</b> Linear independence</a></li>
<li class="chapter" data-level="1.2.2" data-path="1-2-linalg-vecspaces.html"><a href="1-2-linalg-vecspaces.html#colsspace"><i class="fa fa-check"></i><b>1.2.2</b> Row and column spaces</a></li>
<li class="chapter" data-level="1.2.3" data-path="1-2-linalg-vecspaces.html"><a href="1-2-linalg-vecspaces.html#linear-transformations"><i class="fa fa-check"></i><b>1.2.3</b> Linear transformations</a></li>
</ul></li>
<li class="chapter" data-level="1.3" data-path="1-3-linalg-innerprod.html"><a href="1-3-linalg-innerprod.html"><i class="fa fa-check"></i><b>1.3</b> Inner product spaces</a><ul>
<li class="chapter" data-level="1.3.1" data-path="1-3-linalg-innerprod.html"><a href="1-3-linalg-innerprod.html#normed"><i class="fa fa-check"></i><b>1.3.1</b> Distances, and angles</a></li>
<li class="chapter" data-level="1.3.2" data-path="1-3-linalg-innerprod.html"><a href="1-3-linalg-innerprod.html#orthogonal-matrices"><i class="fa fa-check"></i><b>1.3.2</b> Orthogonal matrices</a></li>
<li class="chapter" data-level="1.3.3" data-path="1-3-linalg-innerprod.html"><a href="1-3-linalg-innerprod.html#projection-matrix"><i class="fa fa-check"></i><b>1.3.3</b> Projections</a></li>
</ul></li>
<li class="chapter" data-level="1.4" data-path="1-4-centering-matrix.html"><a href="1-4-centering-matrix.html"><i class="fa fa-check"></i><b>1.4</b> The Centering Matrix</a></li>
<li class="chapter" data-level="1.5" data-path="1-5-tasks-ch2.html"><a href="1-5-tasks-ch2.html"><i class="fa fa-check"></i><b>1.5</b> Computer tasks</a></li>
<li class="chapter" data-level="1.6" data-path="1-6-exercises-ch2.html"><a href="1-6-exercises-ch2.html"><i class="fa fa-check"></i><b>1.6</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="2-linalg-decomp.html"><a href="2-linalg-decomp.html"><i class="fa fa-check"></i><b>2</b> Matrix decompositions</a><ul>
<li class="chapter" data-level="2.1" data-path="2-1-matrix-matrix.html"><a href="2-1-matrix-matrix.html"><i class="fa fa-check"></i><b>2.1</b> Matrix-matrix products</a></li>
<li class="chapter" data-level="2.2" data-path="2-2-spectraleigen-decomposition.html"><a href="2-2-spectraleigen-decomposition.html"><i class="fa fa-check"></i><b>2.2</b> Spectral/eigen decomposition</a><ul>
<li class="chapter" data-level="2.2.1" data-path="2-2-spectraleigen-decomposition.html"><a href="2-2-spectraleigen-decomposition.html#eigenvalues-and-eigenvectors"><i class="fa fa-check"></i><b>2.2.1</b> Eigenvalues and eigenvectors</a></li>
<li class="chapter" data-level="2.2.2" data-path="2-2-spectraleigen-decomposition.html"><a href="2-2-spectraleigen-decomposition.html#spectral-decomposition"><i class="fa fa-check"></i><b>2.2.2</b> Spectral decomposition</a></li>
<li class="chapter" data-level="2.2.3" data-path="2-2-spectraleigen-decomposition.html"><a href="2-2-spectraleigen-decomposition.html#matrixroots"><i class="fa fa-check"></i><b>2.2.3</b> Matrix square roots</a></li>
</ul></li>
<li class="chapter" data-level="2.3" data-path="2-3-linalg-SVD.html"><a href="2-3-linalg-SVD.html"><i class="fa fa-check"></i><b>2.3</b> Singular Value Decomposition (SVD)</a><ul>
<li class="chapter" data-level="2.3.1" data-path="2-3-linalg-SVD.html"><a href="2-3-linalg-SVD.html#examples"><i class="fa fa-check"></i><b>2.3.1</b> Examples</a></li>
</ul></li>
<li class="chapter" data-level="2.4" data-path="2-4-svdopt.html"><a href="2-4-svdopt.html"><i class="fa fa-check"></i><b>2.4</b> SVD optimization results</a></li>
<li class="chapter" data-level="2.5" data-path="2-5-low-rank-approximation.html"><a href="2-5-low-rank-approximation.html"><i class="fa fa-check"></i><b>2.5</b> Low-rank approximation</a><ul>
<li class="chapter" data-level="2.5.1" data-path="2-5-low-rank-approximation.html"><a href="2-5-low-rank-approximation.html#matrix-norms"><i class="fa fa-check"></i><b>2.5.1</b> Matrix norms</a></li>
<li class="chapter" data-level="2.5.2" data-path="2-5-low-rank-approximation.html"><a href="2-5-low-rank-approximation.html#eckart-young-mirsky-theorem"><i class="fa fa-check"></i><b>2.5.2</b> Eckart-Young-Mirsky Theorem</a></li>
<li class="chapter" data-level="2.5.3" data-path="2-5-low-rank-approximation.html"><a href="2-5-low-rank-approximation.html#example-image-compression"><i class="fa fa-check"></i><b>2.5.3</b> Example: image compression</a></li>
</ul></li>
<li class="chapter" data-level="2.6" data-path="2-6-tasks-ch3.html"><a href="2-6-tasks-ch3.html"><i class="fa fa-check"></i><b>2.6</b> Computer tasks</a></li>
<li class="chapter" data-level="2.7" data-path="2-7-exercises-ch3.html"><a href="2-7-exercises-ch3.html"><i class="fa fa-check"></i><b>2.7</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="part-ii-dimension-reduction-methods.html"><a href="part-ii-dimension-reduction-methods.html"><i class="fa fa-check"></i>PART II: Dimension reduction methods</a><ul>
<li class="chapter" data-level="" data-path="part-ii-dimension-reduction-methods.html"><a href="part-ii-dimension-reduction-methods.html#a-warning"><i class="fa fa-check"></i>A warning</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="3-pca.html"><a href="3-pca.html"><i class="fa fa-check"></i><b>3</b> Principal Component Analysis (PCA)</a><ul>
<li class="chapter" data-level="3.1" data-path="3-1-pca-an-informal-introduction.html"><a href="3-1-pca-an-informal-introduction.html"><i class="fa fa-check"></i><b>3.1</b> PCA: an informal introduction</a><ul>
<li class="chapter" data-level="3.1.1" data-path="3-1-pca-an-informal-introduction.html"><a href="3-1-pca-an-informal-introduction.html#notation-recap"><i class="fa fa-check"></i><b>3.1.1</b> Notation recap</a></li>
<li class="chapter" data-level="3.1.2" data-path="3-1-pca-an-informal-introduction.html"><a href="3-1-pca-an-informal-introduction.html#first-principal-component"><i class="fa fa-check"></i><b>3.1.2</b> First principal component</a></li>
<li class="chapter" data-level="3.1.3" data-path="3-1-pca-an-informal-introduction.html"><a href="3-1-pca-an-informal-introduction.html#second-principal-component"><i class="fa fa-check"></i><b>3.1.3</b> Second principal component</a></li>
<li class="chapter" data-level="3.1.4" data-path="3-1-pca-an-informal-introduction.html"><a href="3-1-pca-an-informal-introduction.html#geometric-interpretation-1"><i class="fa fa-check"></i><b>3.1.4</b> Geometric interpretation</a></li>
<li class="chapter" data-level="3.1.5" data-path="3-1-pca-an-informal-introduction.html"><a href="3-1-pca-an-informal-introduction.html#example"><i class="fa fa-check"></i><b>3.1.5</b> Example</a></li>
<li class="chapter" data-level="3.1.6" data-path="3-1-pca-an-informal-introduction.html"><a href="3-1-pca-an-informal-introduction.html#example-iris"><i class="fa fa-check"></i><b>3.1.6</b> Example: Iris</a></li>
</ul></li>
<li class="chapter" data-level="3.2" data-path="3-2-pca-a-formal-description-with-proofs.html"><a href="3-2-pca-a-formal-description-with-proofs.html"><i class="fa fa-check"></i><b>3.2</b> PCA: a formal description with proofs</a><ul>
<li class="chapter" data-level="3.2.1" data-path="3-2-pca-a-formal-description-with-proofs.html"><a href="3-2-pca-a-formal-description-with-proofs.html#properties-of-principal-components"><i class="fa fa-check"></i><b>3.2.1</b> Properties of principal components</a></li>
<li class="chapter" data-level="3.2.2" data-path="3-2-pca-a-formal-description-with-proofs.html"><a href="3-2-pca-a-formal-description-with-proofs.html#pca:football"><i class="fa fa-check"></i><b>3.2.2</b> Example: Football</a></li>
<li class="chapter" data-level="3.2.3" data-path="3-2-pca-a-formal-description-with-proofs.html"><a href="3-2-pca-a-formal-description-with-proofs.html#pcawithR"><i class="fa fa-check"></i><b>3.2.3</b> PCA based on <span class="math inline">\(\mathbf R\)</span> versus PCA based on <span class="math inline">\(\mathbf S\)</span></a></li>
<li class="chapter" data-level="3.2.4" data-path="3-2-pca-a-formal-description-with-proofs.html"><a href="3-2-pca-a-formal-description-with-proofs.html#population-pca"><i class="fa fa-check"></i><b>3.2.4</b> Population PCA</a></li>
<li class="chapter" data-level="3.2.5" data-path="3-2-pca-a-formal-description-with-proofs.html"><a href="3-2-pca-a-formal-description-with-proofs.html#pca-under-transformations-of-variables"><i class="fa fa-check"></i><b>3.2.5</b> PCA under transformations of variables</a></li>
</ul></li>
<li class="chapter" data-level="3.3" data-path="3-3-an-alternative-view-of-pca.html"><a href="3-3-an-alternative-view-of-pca.html"><i class="fa fa-check"></i><b>3.3</b> An alternative view of PCA</a><ul>
<li class="chapter" data-level="3.3.1" data-path="3-3-an-alternative-view-of-pca.html"><a href="3-3-an-alternative-view-of-pca.html#example-mnist-handwritten-digits"><i class="fa fa-check"></i><b>3.3.1</b> Example: MNIST handwritten digits</a></li>
</ul></li>
<li class="chapter" data-level="3.4" data-path="3-4-pca-comptask.html"><a href="3-4-pca-comptask.html"><i class="fa fa-check"></i><b>3.4</b> Computer tasks</a></li>
<li class="chapter" data-level="3.5" data-path="3-5-exercises.html"><a href="3-5-exercises.html"><i class="fa fa-check"></i><b>3.5</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="4-mds.html"><a href="4-mds.html"><i class="fa fa-check"></i><b>4</b> Multidimensional Scaling (MDS)</a><ul>
<li class="chapter" data-level="4.1" data-path="4-1-classical-mds.html"><a href="4-1-classical-mds.html"><i class="fa fa-check"></i><b>4.1</b> Classical MDS</a><ul>
<li class="chapter" data-level="4.1.1" data-path="4-1-classical-mds.html"><a href="4-1-classical-mds.html#non-euclidean-distance-matrices"><i class="fa fa-check"></i><b>4.1.1</b> Non-Euclidean distance matrices</a></li>
<li class="chapter" data-level="4.1.2" data-path="4-1-classical-mds.html"><a href="4-1-classical-mds.html#example-3"><i class="fa fa-check"></i><b>4.1.2</b> Example 3</a></li>
</ul></li>
<li class="chapter" data-level="4.2" data-path="4-2-principal-coordinates.html"><a href="4-2-principal-coordinates.html"><i class="fa fa-check"></i><b>4.2</b> Principal Coordinates</a></li>
<li class="chapter" data-level="4.3" data-path="4-3-properties.html"><a href="4-3-properties.html"><i class="fa fa-check"></i><b>4.3</b> Properties</a></li>
<li class="chapter" data-level="4.4" data-path="4-4-similarity-measures.html"><a href="4-4-similarity-measures.html"><i class="fa fa-check"></i><b>4.4</b> Similarity measures</a><ul>
<li class="chapter" data-level="4.4.1" data-path="4-4-similarity-measures.html"><a href="4-4-similarity-measures.html#example-4"><i class="fa fa-check"></i><b>4.4.1</b> Example</a></li>
</ul></li>
<li class="chapter" data-level="4.5" data-path="4-5-non-metric-mds.html"><a href="4-5-non-metric-mds.html"><i class="fa fa-check"></i><b>4.5</b> Non-metric MDS</a></li>
<li class="chapter" data-level="4.6" data-path="4-6-exercises-1.html"><a href="4-6-exercises-1.html"><i class="fa fa-check"></i><b>4.6</b> Exercises</a></li>
<li class="chapter" data-level="4.7" data-path="4-7-computer-tasks.html"><a href="4-7-computer-tasks.html"><i class="fa fa-check"></i><b>4.7</b> Computer Tasks</a></li>
</ul></li>
<li class="divider"></li>
<li> <a href="https://rich-d-wilkinson.github.io/teaching.html" target="blank">University of Nottingham</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Multivariate Statistics</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="pca-a-formal-description-with-proofs" class="section level2">
<h2><span class="header-section-number">3.2</span> PCA: a formal description with proofs</h2>
<p>Letâs now summarize what weâve said so far and prove some results about principal component analysis.</p>
<p>Let <span class="math inline">\(\mathbf x_1, \ldots , \mathbf x_n\)</span> denote a sample of vectors in <span class="math inline">\(\mathbb{R}^p\)</span> with sample mean vector <span class="math inline">\(\bar{\mathbf x}\)</span> and sample covariance matrix <span class="math inline">\(\mathbf S\)</span>. Suppose <span class="math inline">\(\mathbf S=\mathbf X^\top \mathbf H\mathbf X\)</span> has spectral decomposition (see Proposition <a href="2-2-spectraleigen-decomposition.html#prp:spectraldecomp">2.3</a>)
<span class="math display" id="eq:pcaspect">\[\begin{equation}
\mathbf S=\mathbf V\boldsymbol \Lambda\mathbf V^\top = \sum_{j=1}^p  \lambda_j \mathbf v_j \mathbf v_j^\top,
\tag{3.2}
\end{equation}\]</span>
where the eigenvalues are <span class="math inline">\(\lambda_1 \geq \lambda_2 \geq \lambda_p \geq 0\)</span> with <span class="math inline">\(\boldsymbol \Lambda=\text{diag}\{\lambda_1, \ldots, \lambda_p\}\)</span>, and <span class="math inline">\(\mathbf V\)</span> contains the eigenvectors of <span class="math inline">\(\mathbf S\)</span>. If <span class="math inline">\(\mathbf S\)</span> is of full rank, then <span class="math inline">\(\lambda_p&gt;0\)</span>. If <span class="math inline">\(\mathbf S\)</span> is rank <span class="math inline">\(r\)</span>, with <span class="math inline">\(r&lt;p\)</span>, then <span class="math inline">\(\lambda_{r+1}=\ldots, \lambda_p=0\)</span> and we can truncate <span class="math inline">\(\mathbf V\)</span> to consider just the first <span class="math inline">\(r\)</span> columns.</p>
<p>The principal components of <span class="math inline">\(\mathbf X\)</span> are defined sequentially. If the <span class="math inline">\(\mathbf v_k\)</span> is value of <span class="math inline">\(\mathbf u\)</span> that maximizes the objective for the <span class="math inline">\(k^{th}\)</span> problem (for <span class="math inline">\(k&lt;j\)</span>), then the <span class="math inline">\(j^{th}\)</span> principal component is the solution to the following optimization problem:
<span class="math display" id="eq:pcmaxgen">\[\begin{equation}
\max_{\mathbf u: \, \vert \vert \mathbf u\vert \vert =1}\mathbf u^\top \mathbf S\mathbf u
\tag{3.3}
\end{equation}\]</span>
subject to
<span class="math display" id="eq:pccongen">\[\begin{equation}
\mathbf v_k^\top \mathbf u=0, \qquad k=1, \ldots , j-1.
\tag{3.4}
\end{equation}\]</span>
(for <span class="math inline">\(j=1\)</span> there is no orthogonality constraint).</p>

<div class="proposition">
<span id="prp:pca1" class="proposition"><strong>Proposition 3.1  </strong></span>The maximum of Equation <a href="3-2-pca-a-formal-description-with-proofs.html#eq:pcmaxgen">(3.3)</a>
subject to Equation <a href="3-2-pca-a-formal-description-with-proofs.html#eq:pccongen">(3.4)</a> is equal to <span class="math inline">\(\lambda_j\)</span> and is obtained when <span class="math inline">\(\mathbf u=\mathbf v_j\)</span>.
</div>


<div class="proof">
<p> <span class="proof"><em>Proof. </em></span> We can prove this using the method of Lagrange multipliers. For <span class="math inline">\(j=1\)</span> our objective is
<span class="math display">\[\mathcal{L} = \mathbf u^\top  \mathbf S\mathbf u+\lambda(1-\mathbf u^\top \mathbf u)\]</span>
Differentiating (see <a href="1-1-linalg-basics.html#vectordiff">1.1.4</a>) with respect to <span class="math inline">\(\mathbf u\)</span> and setting the derivative equal to zero gives
<span class="math display">\[2\mathbf S\mathbf u-2\lambda \mathbf u=0\]</span>
Rearranging we see that <span class="math inline">\(\mathbf u\)</span> must satify
<span class="math display">\[\mathbf S\mathbf u=\lambda \mathbf u\mbox{ with } \mathbf u^\top \mathbf u=1\]</span>
i.e., <span class="math inline">\(\mathbf u\)</span> is a unit eigenvector of <span class="math inline">\(\mathbf S\)</span>. Substituting this back in to the objective we see
<span class="math display">\[\mathbf u\mathbf S\mathbf u= \lambda\]</span>
and so we must choose <span class="math inline">\(\mathbf u=\mathbf v_1\)</span>, the eigenvector corresponding to the largest eigenvalue of <span class="math inline">\(\mathbf S\)</span>.</p>
We now proceed inductively and assume the result is true for <span class="math inline">\(k=1, \ldots, j-1\)</span>. The Lagrangian for the <span class="math inline">\(j^{th}\)</span> optimization problem is
<span class="math display">\[\mathcal{L} = \mathbf u^\top  \mathbf S\mathbf u+\lambda(1-\mathbf u^\top \mathbf u) +\sum_{k=1}^{j-1}\mu_k (0-\mathbf u^\top \mathbf v_k)\]</span>
where we now have <span class="math inline">\(j\)</span> Lagrange multipliers <span class="math inline">\(\lambda, \mu_1, \ldots, \mu_{j-1}\)</span> - one for each constraint.
Differentiating with respect to <span class="math inline">\(\mathbf u\)</span> and setting equal to zero gives
<span class="math display">\[0 = 2\mathbf S\mathbf u- 2\lambda \mathbf u- \sum_{k=1}^{j-1} \mu_k\mathbf v_k=0 \]</span>
If we left multiply by <span class="math inline">\(\mathbf v_l^\top\)</span> we get
<span class="math display">\[2\mathbf v_l^\top \mathbf S\mathbf u- 2\lambda \mathbf v_l \mathbf u- \sum \mu_k \mathbf v_l^\top \mathbf v_k =0\]</span>
We know <span class="math inline">\(\mathbf v_l\)</span> is an eigenvector of <span class="math inline">\(\mathbf S\)</span> and so <span class="math inline">\(\mathbf S\mathbf v_l=\lambda_l \mathbf v_l\)</span> and hence <span class="math inline">\(\mathbf v_k \mathbf S\mathbf u=0\)</span> as <span class="math inline">\(\mathbf v_l^\top \mathbf u=0\)</span>. Also <span class="math display">\[\mathbf v_l^\top\mathbf v_k=\begin{cases}1 &amp;\mbox{ if } k=l\\
0 &amp;\mbox{ otherwise, }\end{cases}\]</span> and thus weâve shown that <span class="math inline">\(\mu_l=0\)</span> for <span class="math inline">\(l=1, \ldots, j-1\)</span>. So again we have that <span class="math display">\[\mathbf S\mathbf u= \lambda \mathbf u\]</span>
i.e., <span class="math inline">\(\mathbf u\)</span> must be a unit eigenvector of <span class="math inline">\(\mathbf S\)</span>. It only remains to show <em>which</em> eigenvector it is. Because <span class="math inline">\(\mathbf u\)</span> must be orthogonal to <span class="math inline">\(\mathbf v_1, \ldots, \mathbf v_{j-1}\)</span>,
and as <span class="math inline">\(\mathbf v_l^\top \mathbf S\mathbf v_l = \lambda_l\)</span>, we must choose <span class="math inline">\(\mathbf u=\mathbf v_j\)</span>, the eigenvector corresponding to the <span class="math inline">\(j^{th}\)</span> largest eigenvalue.
</div>

<div id="properties-of-principal-components" class="section level3">
<h3><span class="header-section-number">3.2.1</span> Properties of principal components</h3>
<p>For <span class="math inline">\(j=1, \ldots , p\)</span>, the scores of the <span class="math inline">\(j^{th}\)</span> principal component (PC) are given by
<span class="math display">\[
y_{ij}=\mathbf v_j^\top(\mathbf x_i - \bar{\mathbf x}), \qquad i=1, \ldots , n.
\]</span>
The <span class="math inline">\(j^{th}\)</span> eigenvector <span class="math inline">\(\mathbf v_j\)</span> is sometimes referred to as the vector of <strong>loadings</strong> for the <span class="math inline">\(j^{th}\)</span> PC. Note that if <span class="math inline">\(\operatorname{rank}(S)=r&lt;p\)</span>, then the <span class="math inline">\(r+1^{th}, \ldots, p^{th}\)</span> scores are meaningless, as they will all be zero.</p>
<p>In vector notation
<span class="math display">\[
\mathbf y_i=( y_{i1}, y_{i2}, \ldots , y_{ip})^\top = \mathbf V^\top (\mathbf x_i -\bar{\mathbf x}), \qquad i=1, \ldots ,n.
\]</span>
In matrix form, the full set of PC scores is given by
<span class="math display">\[
\mathbf Y= [\mathbf y_1 , \ldots , \mathbf y_n]^\top =\mathbf H\mathbf X\mathbf V.
\]</span></p>
<p>If <span class="math inline">\(\tilde{\mathbf X}=\mathbf H\mathbf X\)</span> is the column centered data matrix, with singular value decomposition
<span class="math inline">\(\tilde{\mathbf X}=\mathbf U\boldsymbol{\Sigma}\mathbf V^\top\)</span> with <span class="math inline">\(\mathbf V\)</span> as in Equation <a href="3-2-pca-a-formal-description-with-proofs.html#eq:pcaspect">(3.2)</a>, then
<span class="math display">\[\mathbf Y= \tilde{\mathbf X}\mathbf V= \mathbf U\boldsymbol{\Sigma}.\]</span></p>
<p>The transformed variables <span class="math inline">\(\mathbf Y= \mathbf H\mathbf X\mathbf V\)</span> have some important properties which we collect together in the following proposition.</p>

<div class="proposition">
<p><span id="prp:pca2" class="proposition"><strong>Proposition 3.2  </strong></span>The following results hold:</p>
<ol style="list-style-type: decimal">
<li><p>The sample mean vector of <span class="math inline">\(\mathbf y_1, \ldots , \mathbf y_n\)</span> is the zero vector: <span class="math inline">\(\bar{\mathbf y}={\mathbf 0}_p\)</span></p></li>
<li><p>The sample covariance matrix of <span class="math inline">\(\mathbf y_1, \ldots, \mathbf y_n\)</span> is
<span class="math display">\[\Lambda = \operatorname{diag}(\lambda_1, \ldots, \lambda_p)\]</span>
i.e., for each fixed <span class="math inline">\(j\)</span>, the sample variance of <span class="math inline">\(y_{ij}\)</span> is <span class="math inline">\(\lambda_j\)</span>, and <span class="math inline">\(y_{ij}\)</span> is uncorrelated with with <span class="math inline">\(y_{ik}\)</span> for <span class="math inline">\(j\not = k\)</span>.</p></li>
<li><p>For <span class="math inline">\(j\leq k\)</span> the sample variance of <span class="math inline">\(\{y_{ij}\}_{i=1, \ldots , n}\)</span> is greater than or equal to the sample variance of <span class="math inline">\(\{y_{ik}\}_{i=1, \ldots , n}\)</span>.
<span class="math display">\[\mathbf v_1^\top \mathbf S\mathbf v_1 \geq \mathbf v_2^\top \mathbf S\mathbf v_2 \geq \ldots \geq \mathbf v_p^\top \mathbf S\mathbf v_p\geq 0\]</span>
Note that if <span class="math inline">\(\operatorname{rank}(S)=r&lt;p\)</span>, then <span class="math inline">\(\mathbf v_k^\top \mathbf S\mathbf v_k = 0\)</span> for <span class="math inline">\(k=r+1, \ldots, p\)</span>.</p></li>
<li><p>The sum of the sample variances is equal to the trace of <span class="math inline">\(\mathbf S\)</span>
<span class="math display">\[\sum_{j=1}^p \mathbf v_j^\top \mathbf S\mathbf v_j = \sum_{j=1}^p \lambda_j = \text{tr}(\mathbf S)\]</span></p></li>
<li>The product of the sample variances is equal to the determinant of <span class="math inline">\(\mathbf S\)</span>
<span class="math display">\[\prod_{j=1}^p \mathbf v_j^\top \mathbf S\mathbf v_j = \prod_{j=1}^p \lambda_j = |\mathbf S|.\]</span></li>
</ol>
</div>


<div class="proof">
<p> <span class="proof"><em>Proof. </em></span> For i.
<span class="math display">\[\bar{\mathbf y} = \frac{1}{n}\sum_{i=1}^n \mathbf V^\top(\mathbf x_i-\bar{\mathbf x}) = \frac{1}{n} \mathbf V^\top\sum_{i=1}^n(\mathbf x_i-\bar{\mathbf x}) =\boldsymbol 0.\]</span></p>
<p>For 2. the sample covariance matrix of <span class="math inline">\(\mathbf y_1, \ldots, \mathbf y_n\)</span> is
<span class="math display">\[\begin{align*}
\frac{1}{n}\sum_{i=1}^n \mathbf y_i \mathbf y_i^\top &amp;=\frac{1}{n} \sum \mathbf V^\top (\mathbf x_i-\bar{\mathbf x})(\mathbf x_i - \mathbf x)^\top \mathbf V\\
&amp;=\mathbf V^\top \mathbf S\mathbf V\\
&amp;=\mathbf V^\top \mathbf V\boldsymbol \Lambda\mathbf V^\top \mathbf V\mbox{ substiting the spectral decomposition for }\mathbf S\\
&amp;=\boldsymbol \Lambda
\end{align*}\]</span></p>
<ol start="3" style="list-style-type: decimal">
<li><p>is a consequence 2. and of ordering the eigenvalues in decreasing magnitude.</p></li>
<li><p>follows from lemma <a href="1-1-linalg-basics.html#lem:trace">1.1</a> and the spectral decomposition of <span class="math inline">\(\mathbf S\)</span>:
<span class="math display">\[\operatorname{tr}(\mathbf S) = \operatorname{tr}(\mathbf V\boldsymbol \Lambda\mathbf V^\top)  =\operatorname{tr}(\mathbf V^\top \mathbf V\boldsymbol \Lambda)=\operatorname{tr}(\boldsymbol \Lambda)=\sum\lambda_i\]</span></p></li>
<li>follows from <a href="2-2-spectraleigen-decomposition.html#prp:deteig">2.2</a>.</li>
</ol>
</div>

<p>From these properties we say that a proportion
<span class="math display">\[\frac{\lambda_j}{\lambda_1 + \ldots + \lambda_p}\]</span>
of the variability in the sample is âexplainedâ by the <span class="math inline">\(j^{th}\)</span> PC.</p>
<p>One tool for looking at the contributions of each PC is to look at the <strong>scree plot</strong> which plots the percentage of variance explained by PC <span class="math inline">\(j\)</span> against <span class="math inline">\(j\)</span>. Weâll see examples of scree plots below.</p>
</div>
<div id="pca:football" class="section level3">
<h3><span class="header-section-number">3.2.2</span> Example: Football</h3>
<p>We can apply PCA to a football league table where <span class="math inline">\(W\)</span>, <span class="math inline">\(D\)</span>, <span class="math inline">\(L\)</span> are the number of matches won, drawn and lost and <span class="math inline">\(G\)</span> and <span class="math inline">\(GA\)</span> are the goals scored for and against, and <span class="math inline">\(GD\)</span> is the goal difference (<span class="math inline">\(G-GA\)</span>). An extract of the table for the 2019-2020 Premier League season is:</p>
<table>
<thead>
<tr>
<th style="text-align:left;">
Team
</th>
<th style="text-align:right;">
W
</th>
<th style="text-align:right;">
D
</th>
<th style="text-align:right;">
L
</th>
<th style="text-align:right;">
G
</th>
<th style="text-align:right;">
GA
</th>
<th style="text-align:right;">
GD
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left;">
Liverpool
</td>
<td style="text-align:right;">
32
</td>
<td style="text-align:right;">
3
</td>
<td style="text-align:right;">
3
</td>
<td style="text-align:right;">
85
</td>
<td style="text-align:right;">
33
</td>
<td style="text-align:right;">
52
</td>
</tr>
<tr>
<td style="text-align:left;">
Manchester City
</td>
<td style="text-align:right;">
26
</td>
<td style="text-align:right;">
3
</td>
<td style="text-align:right;">
9
</td>
<td style="text-align:right;">
102
</td>
<td style="text-align:right;">
35
</td>
<td style="text-align:right;">
67
</td>
</tr>
<tr>
<td style="text-align:left;">
Manchester United
</td>
<td style="text-align:right;">
18
</td>
<td style="text-align:right;">
12
</td>
<td style="text-align:right;">
8
</td>
<td style="text-align:right;">
66
</td>
<td style="text-align:right;">
36
</td>
<td style="text-align:right;">
30
</td>
</tr>
<tr>
<td style="text-align:left;">
Chelsea
</td>
<td style="text-align:right;">
20
</td>
<td style="text-align:right;">
6
</td>
<td style="text-align:right;">
12
</td>
<td style="text-align:right;">
69
</td>
<td style="text-align:right;">
54
</td>
<td style="text-align:right;">
15
</td>
</tr>
<tr>
<td style="text-align:left;">
Leicester City
</td>
<td style="text-align:right;">
18
</td>
<td style="text-align:right;">
8
</td>
<td style="text-align:right;">
12
</td>
<td style="text-align:right;">
67
</td>
<td style="text-align:right;">
41
</td>
<td style="text-align:right;">
26
</td>
</tr>
<tr>
<td style="text-align:left;">
Tottenham Hotspur
</td>
<td style="text-align:right;">
16
</td>
<td style="text-align:right;">
11
</td>
<td style="text-align:right;">
11
</td>
<td style="text-align:right;">
61
</td>
<td style="text-align:right;">
47
</td>
<td style="text-align:right;">
14
</td>
</tr>
<tr>
<td style="text-align:left;">
Wolverhampton
</td>
<td style="text-align:right;">
15
</td>
<td style="text-align:right;">
14
</td>
<td style="text-align:right;">
9
</td>
<td style="text-align:right;">
51
</td>
<td style="text-align:right;">
40
</td>
<td style="text-align:right;">
11
</td>
</tr>
<tr>
<td style="text-align:left;">
Arsenal
</td>
<td style="text-align:right;">
14
</td>
<td style="text-align:right;">
14
</td>
<td style="text-align:right;">
10
</td>
<td style="text-align:right;">
56
</td>
<td style="text-align:right;">
48
</td>
<td style="text-align:right;">
8
</td>
</tr>
<tr>
<td style="text-align:left;">
Sheffield United
</td>
<td style="text-align:right;">
14
</td>
<td style="text-align:right;">
12
</td>
<td style="text-align:right;">
12
</td>
<td style="text-align:right;">
39
</td>
<td style="text-align:right;">
39
</td>
<td style="text-align:right;">
0
</td>
</tr>
<tr>
<td style="text-align:left;">
Burnley
</td>
<td style="text-align:right;">
15
</td>
<td style="text-align:right;">
9
</td>
<td style="text-align:right;">
14
</td>
<td style="text-align:right;">
43
</td>
<td style="text-align:right;">
50
</td>
<td style="text-align:right;">
-7
</td>
</tr>
</tbody>
</table>
<p>The sample mean vector is</p>
<p><span class="math display">\[\bar{\mathbf x} =\begin{pmatrix}14.4 \\9.2 \\14.4 \\51.7 \\51.7 \\0 \\\end{pmatrix}.\]</span></p>
<p>Note that the total goals scored must equal the total goals conceded, and that the sum of the goal differences must be <span class="math inline">\(0\)</span>. The sample covariance matrix is</p>
<p><span class="math display" id="eq:PLES">\[\begin{equation}
\mathbf S= \begin{pmatrix}38.3&amp;-9.18&amp;-29.2&amp;103&amp;-57&amp;160 \\-9.18&amp;10.2&amp;-0.98&amp;-27.5&amp;-2.24&amp;-25.2 \\-29.2&amp;-0.98&amp;30.1&amp;-75.3&amp;59.3&amp;-135 \\103&amp;-27.5&amp;-75.3&amp;336&amp;-147&amp;483 \\-57&amp;-2.24&amp;59.3&amp;-147&amp;134&amp;-281 \\160&amp;-25.2&amp;-135&amp;483&amp;-281&amp;764 \\\end{pmatrix}
\tag{3.5}
\end{equation}\]</span></p>
<p>The eigenvalues of <span class="math inline">\(\mathbf S\)</span> are
<span class="math display">\[\boldsymbol \Lambda= \text{diag}\begin{pmatrix}1300&amp;71.9&amp;8.05&amp;4.62&amp;-2.65e-14&amp;-3.73e-14 \\\end{pmatrix}\]</span></p>
<p>Note that we have two zero eigenvalues (which wonât be computed as exactly zero because of numerical rounding errors) because two of our variables are a linear combinations of the other variables, <span class="math inline">\(W+D+L = 38\)</span> and <span class="math inline">\(GD=G-GA\)</span>. The corresponding eigenvectors are
<span class="math display">\[\mathbf V= [\mathbf v_1 \ldots \mathbf v_6] =\begin{pmatrix}-0.166&amp;0.0262&amp;-0.707&amp;0.373&amp;0.222&amp;-0.533 \\0.0282&amp;-0.275&amp;0.661&amp;0.391&amp;0.222&amp;-0.533 \\0.138&amp;0.249&amp;0.0455&amp;-0.764&amp;0.222&amp;-0.533 \\-0.502&amp;0.6&amp;0.202&amp;0.117&amp;0.533&amp;0.222 \\0.285&amp;0.701&amp;0.11&amp;0.286&amp;-0.533&amp;-0.222 \\-0.787&amp;-0.101&amp;0.0915&amp;-0.169&amp;-0.533&amp;-0.222 \\\end{pmatrix}\]</span></p>
<p>The proportion of variability explained by each of the PCs is:
<span class="math display">\[
\begin{pmatrix}0.939&amp;0.052&amp;0.00583&amp;0.00334&amp;-1.92e-17&amp;-2.7e-17 \\\end{pmatrix}
\]</span></p>
<p>There is no point computing the scores for PC 5 and 6, because these do not explain any of the variability in the data. Similarly, there is little value in computing the scores for PCs 3 &amp; 4 because they account for less than 1% of the variability in the data.</p>
<p>We can, therefore, choose to compute only the first two PC scores. We are reducing the dimension of our data set from <span class="math inline">\(p=5\)</span> to <span class="math inline">\(p=2\)</span> while still retaining 99% of the variability. The first PC score/transformed variable is given by:
<span class="math display">\[\begin{align*}
y_{i1} &amp;= -0.17(W_i-\bar{W}) +0.03(D_i-\bar{D}) +0.14(L_i-\bar{L})\\
&amp; \qquad +-0.5(G_i-\bar{G}) +0.28(GA_i-\bar{GA})+-0.79(GD_i-\bar{GD}),
\end{align*}\]</span>
and similarly for PC 2.</p>
The first five rows of our revised âleague tableâ are now
<table class="table" style="width: auto !important; margin-left: auto; margin-right: auto;">
<thead>
<tr>
<th style="text-align:left;">
Team
</th>
<th style="text-align:right;">
PC1
</th>
<th style="text-align:right;">
PC2
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left;">
Liverpool
</td>
<td style="text-align:right;">
-67.6
</td>
<td style="text-align:right;">
0.9
</td>
</tr>
<tr>
<td style="text-align:left;">
Manchester City
</td>
<td style="text-align:right;">
-85.6
</td>
<td style="text-align:right;">
12.3
</td>
</tr>
<tr>
<td style="text-align:left;">
Manchester United
</td>
<td style="text-align:right;">
-36.7
</td>
<td style="text-align:right;">
-7.7
</td>
</tr>
<tr>
<td style="text-align:left;">
Chelsea
</td>
<td style="text-align:right;">
-21.2
</td>
<td style="text-align:right;">
10.9
</td>
</tr>
<tr>
<td style="text-align:left;">
Leicester City
</td>
<td style="text-align:right;">
-32.2
</td>
<td style="text-align:right;">
-1.1
</td>
</tr>
</tbody>
</table>
<p>Now that we have reduced the dimension to <span class="math inline">\(p=2\)</span>, we can visualise the differences between the teams.</p>
<p><img src="04-pca_files/figure-html/unnamed-chunk-26-1.png" width="672" /></p>
<p>We might interpret the PCs as follows. The first PC seems to measure the difference in goals scored and conceded between teams. Low values of PC1 indicate good peformance, and high values poor performance. Teams are rewarded with -0.79 for each positive goal difference, and -0.5 for each goal scored, whilst being penalised by 0.28 for every goal they concede. So a team with a large negative PC1 score tends to score lots of goals and concede few. If we rank teams by their PC1 score, and compare this with the rankings using 3 points for a win and 1 point for a draw we get a different ranking of the teams.</p>
<table>
<thead>
<tr>
<th style="text-align:left;">
</th>
<th style="text-align:right;">
PC1
</th>
<th style="text-align:right;">
PC2
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left;">
Manchester City
</td>
<td style="text-align:right;">
-85.59
</td>
<td style="text-align:right;">
12.35
</td>
</tr>
<tr>
<td style="text-align:left;">
Liverpool
</td>
<td style="text-align:right;">
-67.64
</td>
<td style="text-align:right;">
0.93
</td>
</tr>
<tr>
<td style="text-align:left;">
Manchester United
</td>
<td style="text-align:right;">
-36.66
</td>
<td style="text-align:right;">
-7.73
</td>
</tr>
<tr>
<td style="text-align:left;">
Leicester City
</td>
<td style="text-align:right;">
-32.16
</td>
<td style="text-align:right;">
-1.13
</td>
</tr>
<tr>
<td style="text-align:left;">
Chelsea
</td>
<td style="text-align:right;">
-21.19
</td>
<td style="text-align:right;">
10.90
</td>
</tr>
</tbody>
</table>
<p>The second PC has a strong positive loading for both goals for and against. A team with a large positive PC 2 score was, therefore, involved in matches with lots of goals. We could, therefore, interpret PC 2 as an âentertainmentâ measure, ranking teams according to their involvement in high-scoring games.</p>
<p>The above example raises the question of how many PCs should we use in practice. If we reduce the dimension to <span class="math inline">\(p=1\)</span> then we can rank observations and analyse our new variable with univariate statistics. If we reduce the dimension to <span class="math inline">\(p=2\)</span> then it is still easy to visualise the data. However, reducing the dimension to <span class="math inline">\(p=1\)</span> or <span class="math inline">\(p=2\)</span> may involve losing lots of information and a sensible answer should depend on the objectives of the analysis and the data itself.</p>
<p>The scree graph for the football example is:</p>
<p><img src="04-pca_files/figure-html/unnamed-chunk-29-1.png" width="672" /></p>
<p>There are many possible methods for choosing the number of PCs to retain for analysis, including:</p>
<ul>
<li>retaining enough PCs to explain, say, 90% of the total variation;</li>
<li>retaining PCs where the eigenvalue is above the average.</li>
</ul>
<p>To retain enough PCs to explain 90% of the total variance, would require us to keep just a single PCs in this case.</p>
</div>
<div id="pcawithR" class="section level3">
<h3><span class="header-section-number">3.2.3</span> PCA based on <span class="math inline">\(\mathbf R\)</span> versus PCA based on <span class="math inline">\(\mathbf S\)</span></h3>
<p>Recall the distinction between the sample covariance matrix <span class="math inline">\(\mathbf S\)</span> and the sample correlation matrix <span class="math inline">\(\mathbf R\)</span>.
Note that all correlation matrices are also covariance matrices, but not all covariance matrices are correlation matrices.
Before doing PCA we must decide whether to do PCA based on <span class="math inline">\(\mathbf S\)</span> or <span class="math inline">\(\mathbf R\)</span>? As we will see later</p>
<ul>
<li>PCA based on <span class="math inline">\(\mathbf R\)</span> (but not <span class="math inline">\(\mathbf S\)</span>) is scale invariant, whereas</li>
<li>PCA based on <span class="math inline">\(\mathbf S\)</span> is invariant under orthogonal rotation.</li>
</ul>
<p>If the original <span class="math inline">\(p\)</span> variables represent very different types of quantity or show marked differences in variances, then it will usually be better to use <span class="math inline">\(\mathbf R\)</span> rather than <span class="math inline">\(\mathbf S\)</span>. However, in some circumstances, we may wish to use <span class="math inline">\(\mathbf S\)</span>, such as when the <span class="math inline">\(p\)</span> variables are measuring similar entities and the sample variances are not too different.</p>
<p>Given that the required numerical calculations are easy to perform in R, we might wish to do it both ways and see if it makes much difference. To use the correlation matrix <span class="math inline">\(\mathbf R\)</span>, we just add the option <code>scale=TRUE</code> when using the <code>prcomp</code> command.</p>
<div id="football-example-continued" class="section level4">
<h4><span class="header-section-number">3.2.3.1</span> Football example continued</h4>
<p>If we repeat the analysis of the football data using <span class="math inline">\(\mathbf R\)</span> instead of <span class="math inline">\(\mathbf S\)</span>, we get find principal components:</p>
<p><span class="math display">\[\begin{align*}
\boldsymbol \Lambda&amp;= \text{diag}\begin{pmatrix}4.51&amp;1.25&amp;0.156&amp;0.0863&amp;3.68e-32&amp;2.48e-33 \\\end{pmatrix}\\
\;\\
\mathbf V= [\mathbf v_1 \ldots \mathbf v_6] &amp;=\begin{pmatrix}-0.456&amp;0.149&amp;-0.342&amp;-0.406&amp;0.466&amp;0.52 \\0.143&amp;-0.844&amp;0.344&amp;-0.143&amp;0.24&amp;0.268 \\0.432&amp;0.321&amp;0.186&amp;0.541&amp;0.413&amp;0.461 \\-0.438&amp;0.214&amp;0.7&amp;-0.0181&amp;0.389&amp;-0.348 \\0.419&amp;0.342&amp;0.386&amp;-0.671&amp;-0.245&amp;0.22 \\-0.466&amp;-0.00136&amp;0.302&amp;0.269&amp;-0.586&amp;0.525 \\\end{pmatrix}
\end{align*}\]</span></p>
<p>The effect of using <span class="math inline">\(\mathbf R\)</span> is to standardize each of the original variables to have variance 1.
The first PC now has loadings which are more evenly balanced across the 6 original variables.</p>
<p>Teams will have a small value of PC1 score if they won lots, lost rarely, scored a lot, and conceded rarely. In other words, PC1 is a complete measure of overall performance. If we look at the league table based on ordering according to PC1 we get a table that looks more like the original table.</p>
<table>
<thead>
<tr>
<th style="text-align:left;">
</th>
<th style="text-align:right;">
PC1
</th>
<th style="text-align:right;">
PC2
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left;">
Liverpool
</td>
<td style="text-align:right;">
-4.70
</td>
<td style="text-align:right;">
1.20
</td>
</tr>
<tr>
<td style="text-align:left;">
Manchester City
</td>
<td style="text-align:right;">
-4.38
</td>
<td style="text-align:right;">
1.65
</td>
</tr>
<tr>
<td style="text-align:left;">
Manchester United
</td>
<td style="text-align:right;">
-2.01
</td>
<td style="text-align:right;">
-1.29
</td>
</tr>
<tr>
<td style="text-align:left;">
Chelsea
</td>
<td style="text-align:right;">
-1.29
</td>
<td style="text-align:right;">
1.08
</td>
</tr>
<tr>
<td style="text-align:left;">
Leicester City
</td>
<td style="text-align:right;">
-1.66
</td>
<td style="text-align:right;">
0.12
</td>
</tr>
<tr>
<td style="text-align:left;">
Tottenham Hotspur
</td>
<td style="text-align:right;">
-0.91
</td>
<td style="text-align:right;">
-0.65
</td>
</tr>
<tr>
<td style="text-align:left;">
Wolverhampton
</td>
<td style="text-align:right;">
-0.82
</td>
<td style="text-align:right;">
-1.88
</td>
</tr>
<tr>
<td style="text-align:left;">
Arsenal
</td>
<td style="text-align:right;">
-0.46
</td>
<td style="text-align:right;">
-1.56
</td>
</tr>
<tr>
<td style="text-align:left;">
Sheffield United
</td>
<td style="text-align:right;">
-0.18
</td>
<td style="text-align:right;">
-1.38
</td>
</tr>
<tr>
<td style="text-align:left;">
Burnley
</td>
<td style="text-align:right;">
0.18
</td>
<td style="text-align:right;">
-0.10
</td>
</tr>
</tbody>
</table>
<p>Overall for these data, doing PCA with <span class="math inline">\(\mathbf R\)</span> instead of <span class="math inline">\(\mathbf S\)</span> better summarizes the data (although this is just my subjective opinion - you may feel differently).</p>
<p><img src="04-pca_files/figure-html/unnamed-chunk-32-1.png" width="672" /></p>
</div>
</div>
<div id="population-pca" class="section level3">
<h3><span class="header-section-number">3.2.4</span> Population PCA</h3>
<p>So far we have considered sample PCA based on the sample covariance matrix or sample correlation matrix:
<span class="math display">\[
\mathbf S=\frac{1}{n}\sum_{i=1}^n (\mathbf x_i-\bar{\mathbf x})(\mathbf x_i-\bar{\mathbf x})^\top.
\]</span></p>
<p>We note now that there is a <em>population</em> analogue of PCA based on the population
covariance matrix <span class="math inline">\(\boldsymbol{\Sigma}\)</span>. Although the population version of PCA is not of as much direct practical
relevance as sample PCA, it is nevertheless of conceptual importance.</p>
<p>Let <span class="math inline">\(\mathbf x\)</span> denote a <span class="math inline">\(p \times 1\)</span> random vector with <span class="math inline">\({\mathbb{E}}(\mathbf x)={\pmb \mu}\)</span> and <span class="math inline">\({\mathbb{V}\operatorname{ar}}(\mathbf x)={\pmb \Sigma}\)</span>. As defined,
<span class="math inline">\(\pmb \mu\)</span> is the population mean vector and <span class="math inline">\(\pmb \Sigma\)</span> is the population covariance matrix.</p>
<p>Since <span class="math inline">\(\pmb \Sigma\)</span> is symmetric, the spectral decomposition theorem tells us that
<span class="math display">\[
{\pmb \Sigma}=\sum_{j=1}^p \check{\lambda}_j \check{\mathbf v}_j \check{\mathbf v}_j^\top=\check{\mathbf V} \check{\boldsymbol \Lambda}\check{\mathbf V}^\top
\]</span>
where the âcheckâ symbol <span class="math inline">\(\quad \check{} \quad\)</span> is used to distinguish population quantities from their sample analogues.</p>
<p>Then:</p>
<ul>
<li>the first population PC is defined by <span class="math inline">\(Y_1=\check{\mathbf v}_1^\top (\mathbf x-{\pmb \mu})\)</span>;</li>
<li>the second population PC is defined by <span class="math inline">\(Y_2=\check{\mathbf v}_2^\top (\mathbf x-{\pmb \mu})\)</span>;</li>
<li><span class="math inline">\(\ldots\)</span></li>
<li>the <span class="math inline">\(p\)</span>th population PC is defined by <span class="math inline">\(Y_p=\check{\mathbf v}_p^\top (\mathbf x-{\pmb \mu})\)</span>.</li>
</ul>
<p>The <span class="math inline">\(Y_1, \ldots , Y_p\)</span> are random variables, unlike the sample PCA case, where the <span class="math inline">\(y_{ij}\)</span> are observed quantities.
In the sample PCA case, the <span class="math inline">\(y_{ij}\)</span> can often be regarded as the observed values of random variables.</p>
<p>In matrix form, the above definitions can be summarised by writing
<span class="math display">\[
\mathbf y=\begin{pmatrix} Y_1 \\ Y_2 \\ ... \\...\\Y_p   \end{pmatrix} = \check{\mathbf V}^\top (\mathbf x-{\pmb \mu}).
\]</span></p>
<p>The population PCA analogues of the sample PCA properties listed in Proposition <a href="3-2-pca-a-formal-description-with-proofs.html#prp:pca2">3.2</a> are now given. Note that the
<span class="math inline">\(Y_j\)</span>âs are random variables as opposed to observed values of random variables.</p>

<div class="proposition">
<p><span id="prp:pca3" class="proposition"><strong>Proposition 3.3  </strong></span>The following results hold for the random variables <span class="math inline">\(Y_1, \ldots , Y_p\)</span> defined above.</p>
<ol style="list-style-type: decimal">
<li><p><span class="math inline">\({\mathbb{E}}(Y_j)=0\)</span> for <span class="math inline">\(j=1, \ldots , p\)</span>;</p></li>
<li><p><span class="math inline">\({\mathbb{V}\operatorname{ar}}(Y_j)=\check{\lambda}_j\)</span> for <span class="math inline">\(j=1,\ldots, p\)</span>;</p></li>
<li><p><span class="math inline">\({\mathbb{C}\operatorname{ov}}(Y_j,Y_k)=0\)</span> if <span class="math inline">\(j \neq k\)</span>;</p></li>
<li><p><span class="math inline">\({\mathbb{V}\operatorname{ar}}(Y_1) \geq {\mathbb{V}\operatorname{ar}}(Y_2) \geq \cdots \geq {\mathbb{V}\operatorname{ar}}(Y_p) \geq 0\)</span>;</p></li>
<li><p><span class="math inline">\(\sum_{j=1}^p {\mathbb{V}\operatorname{ar}}(Y_j)=\sum_{j=1}^p \check{\lambda}_j=\text{tr}(\boldsymbol{\Sigma})\)</span>;</p></li>
<li><span class="math inline">\(\prod_{j=1}^p \text{Var}(Y_j)=\prod_{j=1}^p \check{\lambda}_j=\vert \boldsymbol{\Sigma}\vert\)</span>.</li>
</ol>
</div>

<p>Note that, defining <span class="math inline">\(\mathbf y=(Y_1, \ldots , Y_p)^\top\)</span> as before, part 1. implies that <span class="math inline">\({\mathbb{E}}(\mathbf y)={\mathbf 0}_p\)</span> and parts 2. and 3. together imply that
<span class="math display">\[
\text{Var}(\mathbf y)=\boldsymbol \Lambda\equiv \text{diag}(\check{\lambda}_1, \ldots , \check{\lambda}_p).
\]</span></p>
<p>Consider now a repeated sampling framework in which we assume that <span class="math inline">\(\mathbf x_1, \ldots , \mathbf x_n\)</span> are IID random vectors from a population
with mean vector <span class="math inline">\(\pmb \mu\)</span> and covariance matrix <span class="math inline">\(\boldsymbol{\Sigma}\)</span>.</p>
<p>What is the relationship between the sample PCA based on the sample of observed vectors <span class="math inline">\(\mathbf x_1, \ldots , \mathbf x_n\)</span>, and the population PCA based on the unobserved random vector <span class="math inline">\(\mathbf x\)</span>,
from the same population?</p>
<p>If the elements of <span class="math inline">\(\boldsymbol{\Sigma}\)</span> are all finite, then as <span class="math inline">\(n\)</span> increases, the elements of the sample covariance matrix <span class="math inline">\(\mathbf S\)</span> will converge to the corresponding elements
of the population covariance matrix <span class="math inline">\(\boldsymbol{\Sigma}\)</span>. Consequently, we expect the principal components from sample PCA to converge to the population PCA values as <span class="math inline">\(n\)</span> grows large. Justification of this statement comes from the weak law of large numbers applied to the components of <span class="math inline">\(\Sigma\)</span>, but the details are beyond the scope of this module.</p>
</div>
<div id="pca-under-transformations-of-variables" class="section level3">
<h3><span class="header-section-number">3.2.5</span> PCA under transformations of variables</h3>
<p>Weâll now consider what happens to PCA when the data are transformed in various ways.</p>
<p><strong>Addition transformation</strong></p>
<p>Firstly, consider the transformation of addition where, for example, we add a fixed amount to each variable.
We can write this transformation as <span class="math inline">\(\mathbf z_i = \mathbf x_i + \mathbf c\)</span>, where <span class="math inline">\(\mathbf c\)</span> is a fixed vector. Under this transformation the sample mean changes, <span class="math inline">\(\bar{\mathbf z} = \bar{\mathbf x} + \mathbf c\)</span>, but the sample variance remains <span class="math inline">\(\mathbf S\)</span>. Consequently, the eigenvalues and eigenvectors remain the same and, therefore, so do the principal component scores/transformed vasriables,
<span class="math display">\[\mathbf y_i = \mathbf V^\top (\mathbf z_i - \bar{\mathbf z}) = \mathbf V^\top(\mathbf x_i + \mathbf c- (\bar{\mathbf x} + \mathbf c)) = \mathbf V^\top (\mathbf x_i - \bar{\mathbf x}).\]</span>
We say that the principal components are <strong>invariant</strong> under the addition transformation. An important special case is to choose <span class="math inline">\(\mathbf c= -\bar{\mathbf x}\)</span> so that the PC scores are simply <span class="math inline">\(\mathbf y_i = \mathbf V^\top \mathbf z_i\)</span>.</p>
<p><strong>Scale transformation</strong></p>
<p>Secondly, we consider the scale transformation where each variable is multiplied by a fixed amount.
A scale transformation occurs more naturally when we convert units of measurement from, say, metres to kilometres. We can write this transformation as <span class="math inline">\(\mathbf z_i = \mathbf D\mathbf x_i\)</span>, where <span class="math inline">\(\mathbf D\)</span> is a diagonal matrix with positive elements. Under this transformation the sample mean changes from <span class="math inline">\(\bar{\mathbf x}\)</span> to <span class="math inline">\(\bar{\mathbf z} = \mathbf D\bar{\mathbf x}\)</span>, and the sample covariance matrix changes from <span class="math inline">\(\mathbf S\)</span> to <span class="math inline">\(\mathbf D\mathbf S\mathbf D\)</span>. Consequently, the principal components also change.</p>
<p>This lack of scale-invariance is undesirable. For example, if we analysed data that included some information on distances, we donât want the answer to depend upon whether we use km, metres, or miles as the measure of distance.
One solution is to scale the data using
<span class="math display">\[
\mathbf D= \text{diag}(s_{11}^{-1/2}, \ldots , s_{pp}^{-1/2}),
 \]</span>
where <span class="math inline">\(s_{ii}\)</span> is the <span class="math inline">\(i\)</span>th diagonal element of <span class="math inline">\(\mathbf S\)</span>. In effect, we have standardised all the new variables to have variance 1. In this case the sample covariance matrix of the <span class="math inline">\(\mathbf z_i\)</span>âs is simply the sample correlation matrix <span class="math inline">\(\mathbf R\)</span> of the original variables, <span class="math inline">\(\mathbf x_i\)</span>. Therefore, we can carry out PCA on the sample correlation matrix, <span class="math inline">\(\mathbf R\)</span>, which is invariant to changes of scale.</p>
<p>In summary: <span class="math inline">\(\mathbf R\)</span> is scale-invariant while <span class="math inline">\(\mathbf S\)</span> is not. To do PCA on <span class="math inline">\(\mathbf R\)</span> in R we use the option <code>scale=TRUE</code> in the <code>prcomp</code> command.</p>
<p>We saw an example of this in section <a href="3-2-pca-a-formal-description-with-proofs.html#pcawithR">3.2.3</a> with the football data. Because the sample
variances of <span class="math inline">\(G\)</span> and <span class="math inline">\(GA\)</span> are much larger than the sample variances of <span class="math inline">\(W\)</span>, <span class="math inline">\(D\)</span> and <span class="math inline">\(L\)</span>, doing PCA with <span class="math inline">\(\mathbf R\)</span> instead of <span class="math inline">\(\mathbf S\)</span> completely changed the analysis.</p>
<p><strong>Orthogonal transformations</strong></p>
<p>Thirdly, we consider a transformation by an orthogonal matrix, <span class="math inline">\(\stackrel{p \times p}{\mathbf A}\)</span>, such that <span class="math inline">\(\mathbf A\mathbf A^\top = \mathbf A^\top \mathbf A= \mathbf I_p\)</span>, and write <span class="math inline">\(\mathbf z_i = \mathbf A\mathbf x_i\)</span>. This is equivalent to rotating and/or reflecting the original data.</p>
<p>Let <span class="math inline">\(\mathbf S\)</span> be the sample covariance matrix of the <span class="math inline">\(\mathbf x_i\)</span> and let <span class="math inline">\(\mathbf T\)</span> be the sample covariance matrix of the <span class="math inline">\(\mathbf z_i\)</span>. Under this transformation the sample mean changes from <span class="math inline">\(\bar{\mathbf x}\)</span> to <span class="math inline">\(\bar{\mathbf z} = \mathbf A\bar{\mathbf x}\)</span>, and the sample covariance matrix <span class="math inline">\(\mathbf S\)</span> changes from <span class="math inline">\(\mathbf S\)</span> to <span class="math inline">\(\mathbf T= \mathbf A\mathbf S\mathbf A^\top\)</span>.</p>
<p>However, if we write <span class="math inline">\(\mathbf S\)</span> in terms of its spectral decomposition <span class="math inline">\(\mathbf S= \mathbf V\boldsymbol \Lambda\mathbf V^\top\)</span>, then <span class="math inline">\(\mathbf T= \mathbf A\mathbf V\boldsymbol \Lambda\mathbf V^\top \mathbf A^\top = \mathbf B\boldsymbol \Lambda\mathbf B^\top\)</span> where <span class="math inline">\(\mathbf B= \mathbf A\mathbf V\)</span> is also orthogonal. It is therefore apparent that the eigenvalues of <span class="math inline">\(\mathbf T\)</span> are the same as those of <span class="math inline">\(\mathbf S\)</span>; and the eigenvectors of <span class="math inline">\(\mathbf T\)</span> are given by <span class="math inline">\(\mathbf b_j\)</span> where <span class="math inline">\(\mathbf b_j = \mathbf A\mathbf v_j\)</span>, <span class="math inline">\(j=1,\ldots,p\)</span>. The PC scores of the rotated variables are
<span class="math display">\[ \mathbf y_i = \mathbf B^\top (\mathbf z_i - \bar{\mathbf z}) = \mathbf V^\top \mathbf A^\top \mathbf A(\mathbf x_i - \bar{\mathbf x}) = \mathbf V_1^\top (\mathbf x_i - \bar{\mathbf x}),\]</span>
and so they are identical to the PC scores of the original variables.</p>
<p>Therefore, under an orthogonal transformation the eigenvalues and PC scores are unchanged; the PCs are orthogonal transformations of the original PCs. We say that the principal components are <strong>equivariant</strong> with respect to orthogonal transformations.</p>
<!--IS PCA of R EQUIVARAINT?-->
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="3-1-pca-an-informal-introduction.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="3-3-an-alternative-view-of-pca.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["MultivariateStatistics.pdf"],
"toc": {
"collapse": "section"
},
"pandoc_args": "--top-level-division=[chapter|part]"
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
