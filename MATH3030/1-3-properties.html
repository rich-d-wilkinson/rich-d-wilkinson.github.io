<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>1.3 Properties | Multivariate Statistics</title>
  <meta name="description" content="The lecture notes for MATH3030/4068: Multivariate Analysis / Applied Multivariate Statistics" />
  <meta name="generator" content="bookdown 0.21 and GitBook 2.6.7" />

  <meta property="og:title" content="1.3 Properties | Multivariate Statistics" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="The lecture notes for MATH3030/4068: Multivariate Analysis / Applied Multivariate Statistics" />
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="1.3 Properties | Multivariate Statistics" />
  
  <meta name="twitter:description" content="The lecture notes for MATH3030/4068: Multivariate Analysis / Applied Multivariate Statistics" />
  

<meta name="author" content="Prof. Richard Wilkinson" />



  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="1-2-the-full-set-of-canonical-correlations.html"/>
<link rel="next" href="1-4-computer-tasks.html"/>
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />











<style type="text/css">
a.sourceLine { display: inline-block; line-height: 1.25; }
a.sourceLine { pointer-events: none; color: inherit; text-decoration: inherit; }
a.sourceLine:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode { white-space: pre; position: relative; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
code.sourceCode { white-space: pre-wrap; }
a.sourceLine { text-indent: -1em; padding-left: 1em; }
}
pre.numberSource a.sourceLine
  { position: relative; left: -4em; }
pre.numberSource a.sourceLine::before
  { content: attr(data-line-number);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; pointer-events: all; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {  }
@media screen {
a.sourceLine::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Applied multivariate statistics</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Introduction</a></li>
<li class="chapter" data-level="1" data-path="1-cca.html"><a href="1-cca.html"><i class="fa fa-check"></i><b>1</b> Canonical Correlation Analysis (CCA)</a><ul>
<li class="chapter" data-level="1.1" data-path="1-1-cca1.html"><a href="1-1-cca1.html"><i class="fa fa-check"></i><b>1.1</b> The first pair of canonical variables</a><ul>
<li class="chapter" data-level="1.1.1" data-path="1-1-cca1.html"><a href="1-1-cca1.html#the-first-canonical-components"><i class="fa fa-check"></i><b>1.1.1</b> The first canonical components</a></li>
<li class="chapter" data-level="1.1.2" data-path="1-1-cca1.html"><a href="1-1-cca1.html#premcca"><i class="fa fa-check"></i><b>1.1.2</b> Example: Premier league football</a></li>
</ul></li>
<li class="chapter" data-level="1.2" data-path="1-2-the-full-set-of-canonical-correlations.html"><a href="1-2-the-full-set-of-canonical-correlations.html"><i class="fa fa-check"></i><b>1.2</b> The full set of canonical correlations</a><ul>
<li class="chapter" data-level="1.2.1" data-path="1-2-the-full-set-of-canonical-correlations.html"><a href="1-2-the-full-set-of-canonical-correlations.html#example-continued"><i class="fa fa-check"></i><b>1.2.1</b> Example continued</a></li>
</ul></li>
<li class="chapter" data-level="1.3" data-path="1-3-properties.html"><a href="1-3-properties.html"><i class="fa fa-check"></i><b>1.3</b> Properties</a><ul>
<li class="chapter" data-level="1.3.1" data-path="1-3-properties.html"><a href="1-3-properties.html#connection-with-linear-regression-when-q1"><i class="fa fa-check"></i><b>1.3.1</b> Connection with linear regression when <span class="math inline">\(q=1\)</span></a></li>
<li class="chapter" data-level="1.3.2" data-path="1-3-properties.html"><a href="1-3-properties.html#invarianceequivariance-properties-of-cca"><i class="fa fa-check"></i><b>1.3.2</b> Invariance/equivariance properties of CCA</a></li>
</ul></li>
<li class="chapter" data-level="1.4" data-path="1-4-computer-tasks.html"><a href="1-4-computer-tasks.html"><i class="fa fa-check"></i><b>1.4</b> Computer tasks</a></li>
<li class="chapter" data-level="1.5" data-path="1-5-exercises.html"><a href="1-5-exercises.html"><i class="fa fa-check"></i><b>1.5</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="2-lm.html"><a href="2-lm.html"><i class="fa fa-check"></i><b>2</b> Linear Models</a><ul>
<li class="chapter" data-level="" data-path="2-lm.html"><a href="2-lm.html#notation"><i class="fa fa-check"></i>Notation</a></li>
<li class="chapter" data-level="2.1" data-path="2-1-ordinary-least-squares-ols.html"><a href="2-1-ordinary-least-squares-ols.html"><i class="fa fa-check"></i><b>2.1</b> Ordinary least squares (OLS)</a><ul>
<li class="chapter" data-level="2.1.1" data-path="2-1-ordinary-least-squares-ols.html"><a href="2-1-ordinary-least-squares-ols.html#geometry"><i class="fa fa-check"></i><b>2.1.1</b> Geometry</a></li>
<li class="chapter" data-level="2.1.2" data-path="2-1-ordinary-least-squares-ols.html"><a href="2-1-ordinary-least-squares-ols.html#normal-linear-model"><i class="fa fa-check"></i><b>2.1.2</b> Normal linear model</a></li>
<li class="chapter" data-level="2.1.3" data-path="2-1-ordinary-least-squares-ols.html"><a href="2-1-ordinary-least-squares-ols.html#linear-models-in-r"><i class="fa fa-check"></i><b>2.1.3</b> Linear models in R</a></li>
<li class="chapter" data-level="2.1.4" data-path="2-1-ordinary-least-squares-ols.html"><a href="2-1-ordinary-least-squares-ols.html#problems-with-ols"><i class="fa fa-check"></i><b>2.1.4</b> Problems with OLS</a></li>
</ul></li>
<li class="chapter" data-level="2.2" data-path="2-2-principal-component-regression-pcr.html"><a href="2-2-principal-component-regression-pcr.html"><i class="fa fa-check"></i><b>2.2</b> Principal component regression (PCR)</a><ul>
<li class="chapter" data-level="2.2.1" data-path="2-2-principal-component-regression-pcr.html"><a href="2-2-principal-component-regression-pcr.html#pcr-in-r"><i class="fa fa-check"></i><b>2.2.1</b> PCR in R</a></li>
</ul></li>
<li class="chapter" data-level="2.3" data-path="2-3-shrinkage-methods.html"><a href="2-3-shrinkage-methods.html"><i class="fa fa-check"></i><b>2.3</b> Shrinkage methods</a><ul>
<li class="chapter" data-level="2.3.1" data-path="2-3-shrinkage-methods.html"><a href="2-3-shrinkage-methods.html#ridge-regression-in-r"><i class="fa fa-check"></i><b>2.3.1</b> Ridge regression in R</a></li>
</ul></li>
<li class="chapter" data-level="2.4" data-path="2-4-multi-output-linear-model.html"><a href="2-4-multi-output-linear-model.html"><i class="fa fa-check"></i><b>2.4</b> Multi-output Linear Model</a><ul>
<li class="chapter" data-level="2.4.1" data-path="2-4-multi-output-linear-model.html"><a href="2-4-multi-output-linear-model.html#normal-linear-model-1"><i class="fa fa-check"></i><b>2.4.1</b> Normal linear model</a></li>
<li class="chapter" data-level="2.4.2" data-path="2-4-multi-output-linear-model.html"><a href="2-4-multi-output-linear-model.html#reduced-rank-regression"><i class="fa fa-check"></i><b>2.4.2</b> Reduced rank regression</a></li>
</ul></li>
</ul></li>
<li class="divider"></li>
<li> <a href="https://rich-d-wilkinson.github.io/teaching.html" target="blank">University of Nottingham</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Multivariate Statistics</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="properties" class="section level2">
<h2><span class="header-section-number">1.3</span> Properties</h2>
<p>The CC variables have a mean of zero. Their correlation structure is given in the following proposition:</p>

<div class="proposition">
<span id="prp:ccavar" class="proposition"><strong>Proposition 1.6  </strong></span>Assume that <span class="math inline">\(\mathbf S_{\mathbf x\mathbf x}\)</span> and <span class="math inline">\(\mathbf S_{\mathbf y\mathbf y}\)</span> both have full rank. Then for <span class="math inline">\(1 \leq k,\ell \leq t\)</span>,
<span class="math display">\[
{\mathbb{C}\operatorname{or}}(\eta_k,  \psi_{\ell})=\begin{cases} \sigma_k &amp;\text{if} \quad k=\ell\\
0 &amp; \text{if} \quad k \neq \ell, \end{cases}
\]</span>
and
<span class="math display">\[{\mathbb{C}\operatorname{or}}(\eta_j, \eta_k)= {\mathbb{C}\operatorname{or}}(\psi_j, \psi_k)=\begin{cases}
1 &amp;\mbox{ if } j=k\\
0 &amp;\mbox{otherwise}
\end{cases}
\]</span>
</div>


<div class="proof">
 <span class="proof"><em>Proof. </em></span> You will prove this in the exercises.
</div>

<div id="connection-with-linear-regression-when-q1" class="section level3">
<h3><span class="header-section-number">1.3.1</span> Connection with linear regression when <span class="math inline">\(q=1\)</span></h3>
<p>Although CCA is clearly a different technique to linear regression, it turns out that when either <span class="math inline">\(\dim \mathbf x=p=1\)</span> or <span class="math inline">\(\dim \mathbf y=q=1\)</span>, there is a close connection between the two approaches.</p>
<p>Consider the case <span class="math inline">\(q=1\)</span> and <span class="math inline">\(p&gt;1\)</span>. Hence there is only a single <span class="math inline">\(y\)</span>-variable but we still have <span class="math inline">\(p&gt;1\)</span> <span class="math inline">\(x\)</span>-variables.</p>
<p>Let’s make the following assumptions:</p>
<ol style="list-style-type: decimal">
<li>The <span class="math inline">\(\mathbf x_i\)</span> have been centred so that <span class="math inline">\(\bar{\mathbf x}={\mathbf 0}_p\)</span>, the zero vector.</li>
<li>The covariance matrix for the <span class="math inline">\(x\)</span>-variables, <span class="math inline">\(\mathbf S_{\mathbf x\mathbf x}\)</span>, has full rank <span class="math inline">\(p\)</span>.</li>
</ol>
<p>The first assumption means that
<span class="math display">\[\mathbf S_{xx}=\frac{1}{n}\mathbf X^\top \mathbf X\quad \mbox{and}\quad \mathbf S_{xy}=\frac{1}{n}\mathbf X^\top \mathbf y,\]</span> and the second means that <span class="math inline">\((\mathbf X^\top \mathbf X)^{-1}\)</span> exists.</p>
<p>Since <span class="math inline">\(q=1\)</span>, the matrix we decompose in CCA
<span class="math display">\[
\mathbf Q=\mathbf S_{\mathbf x\mathbf x}^{-1/2} \mathbf S_{\mathbf xy}\mathbf S_{yy}^{-1/2}
\]</span>
is a <span class="math inline">\(p \times 1\)</span> vector. Consequently, its
SVD is just
<span class="math display">\[
\mathbf Q=\sigma_1 \mathbf u_1,
\]</span>
where
<span class="math display">\[
\sigma_1=\vert \vert \mathbf Q\vert \vert_F = (\mathbf Q^\top \mathbf Q)^{\frac{1}{2}} \qquad \text{and} \qquad \mathbf u_1=\mathbf Q/\vert \vert \mathbf Q\vert \vert_F.
\]</span>
Note that <span class="math inline">\(\mathbf v=1\)</span> here.
Consequently, the first canoncial correlation vector for <span class="math inline">\(\mathbf x\)</span> is
<span class="math display">\[\begin{align*}
\mathbf a&amp;=\mathbf S_{\mathbf x\mathbf x}^{-1/2}\mathbf u_1 =\mathbf S_{\mathbf x\mathbf x}^{-1/2} \frac{\mathbf Q}{||\mathbf Q||_F}\\
&amp;=\mathbf S_{\mathbf x\mathbf x}^{-1/2} \frac{1}{\vert \vert \mathbf S_{\mathbf x\mathbf x}^{-1/2}\mathbf S_{\mathbf xy}S_{yy}^{-1/2}\vert \vert_F}\mathbf S_{\mathbf x\mathbf x}^{-1/2}\mathbf S_{\mathbf x\mathbf y}S_{yy}^{-1/2}\\
&amp;=\frac{1}{\vert \vert \mathbf S_{\mathbf x\mathbf x}^{-1/2}\mathbf S_{\mathbf xy}\vert \vert_F}\mathbf S_{\mathbf x\mathbf x}^{-1}\mathbf S_{\mathbf xy}\\
&amp;= c (\mathbf X^\top \mathbf X)^{-1}\mathbf X^\top \mathbf y
\end{align*}\]</span>
where <span class="math inline">\(c\)</span> is a scalar constant.</p>
<p>Thus, we can see that the first canonical correlation vector <span class="math inline">\(\mathbf a\)</span> is a scalar multiple of
<span class="math display">\[
\hat{\pmb \beta}=\left ( \mathbf X^\top \mathbf X\right )^{-1} \mathbf X^\top \mathbf y,
\]</span>
the classical least squares estimator. Therefore the least squares estimator <span class="math inline">\(\hat{\pmb \beta}\)</span> solves <a href="1-1-cca1.html#eq:opt26">(1.2)</a>. However, it does not usually solve the constrained optimisation problem <a href="1-1-cca1.html#eq:opt27a">(1.4)</a> because typically <span class="math inline">\(\hat{\pmb \beta}^\top \mathbf S_{\mathbf x\mathbf x}\hat{\pmb \beta} \not= 1\)</span>, so that the constraint in Equation <a href="1-1-cca1.html#eq:opt27a">(1.4)</a> will not be satisfied.</p>
</div>
<div id="invarianceequivariance-properties-of-cca" class="section level3">
<h3><span class="header-section-number">1.3.2</span> Invariance/equivariance properties of CCA</h3>
<p>Suppose we apply orthogonal transformations and translations to the <span class="math inline">\(\mathbf x_i\)</span> and the <span class="math inline">\(\mathbf y_i\)</span> of the form
<span class="math display" id="eq:transformations">\[\begin{equation}
{\mathbf h}_i={\mathbf T}\mathbf x_i + {\pmb \mu} \qquad \text{and} \qquad {\mathbf k}_i={\mathbf R}\mathbf y_i +{\pmb \nu},
\qquad i=1,\ldots , n,
\tag{1.12}
\end{equation}\]</span>
where <span class="math inline">\(\mathbf T\)</span> (<span class="math inline">\(p \times p\)</span>) and <span class="math inline">\(\mathbf R\)</span> (<span class="math inline">\(q \times q\)</span>) are orthogonal matrices, and <span class="math inline">\(\pmb \mu\)</span> (<span class="math inline">\(p \times 1\)</span>) and
<span class="math inline">\(\pmb \nu\)</span> (<span class="math inline">\(q \times 1\)</span>) are fixed vectors.</p>
<p>How do these transformations affect the CC analysis?</p>
<p>Firstly, since CCA depends only on sample covariance matrices, it follows that the translation vectors <span class="math inline">\(\pmb \mu\)</span> and <span class="math inline">\(\pmb \nu\)</span> have no effect on the analysis.</p>
<p>Secondly, let’s consider the effect of the rotations by an orthogonal matrix. We’ve seen that CCA in the original <span class="math inline">\(\mathbf x\)</span> and <span class="math inline">\(\mathbf y\)</span> coordinates depends on
<span class="math display" id="eq:Axy">\[\begin{equation}
\mathbf Q\equiv \mathbf Q_{\mathbf x\mathbf y}=\mathbf S_{\mathbf x\mathbf x}^{-1/2}\mathbf S_{\mathbf x\mathbf y}\mathbf S_{\mathbf y\mathbf y}^{-1/2}.
\tag{1.13}
\end{equation}\]</span>
In the new coordinates we have
<span class="math display">\[
\tilde{\mathbf S}_{\mathbf h\mathbf h}={\mathbf T} \mathbf S_{\mathbf x\mathbf x}{\mathbf T}^\top, \qquad \tilde{\mathbf S}_{\mathbf k\mathbf k}={\mathbf R}\mathbf S_{\mathbf y\mathbf y}{\mathbf R}^\top,
\]</span>
<span class="math display">\[
\tilde{\mathbf S}_{\mathbf h\mathbf k}={\mathbf T}\mathbf S_{\mathbf x\mathbf y}{\mathbf R}^\top =
\tilde{\mathbf S}_{\mathbf k\mathbf h}^\top,
\]</span>
where here and below, a tilde above a symbol is used to indicate that the corresponding term is defined in terms of the new <span class="math inline">\(\mathbf h\)</span>, <span class="math inline">\(\mathbf k\)</span> coordinates, rather
than the old <span class="math inline">\(\mathbf x\)</span>, <span class="math inline">\(\mathbf y\)</span> coordinates.
Due to the fact that <span class="math inline">\(\mathbf T\)</span> and <span class="math inline">\(\mathbf R\)</span> are orthogonal,
<span class="math display">\[
\tilde{\mathbf S}_{\mathbf b\mathbf h}^{ 1/2}={\mathbf T}\mathbf S_{\mathbf x\mathbf x}^{ 1/2}{\mathbf T}^\top, \qquad
\tilde{\mathbf S}_{\mathbf h\mathbf h}^{ -1/2}={\mathbf T}\mathbf S_{\mathbf x\mathbf x}^{ -1/2}{\mathbf T}^\top
\]</span>
<span class="math display">\[
\tilde{\mathbf S}_{\mathbf k\mathbf k}^{ 1/2}={\mathbf R}\mathbf S_{\mathbf y\mathbf y}^{ 1/2}{\mathbf R}^\top \qquad \text{and} \qquad
\tilde{\mathbf S}_{\mathbf k\mathbf k}^{ -1/2}={\mathbf R}\mathbf S_{\mathbf y\mathbf y}^{- 1/2}{\mathbf R}^\top.
\]</span>
The analogue of <a href="1-3-properties.html#eq:Axy">(1.13)</a> in the new coordinates is given by
<span class="math display">\[\begin{align*}
\tilde{\mathbf Q}_{\mathbf h k}&amp;=\tilde{\mathbf S}_{\mathbf h\mathbf h}^{-1/2}\tilde{\mathbf S}_{\mathbf h k}\tilde{\mathbf S}_{\mathbf k\mathbf k}^{-1/2}\\
&amp;={\mathbf T} \mathbf S_{\mathbf x\mathbf x}^{-1/2}{\mathbf T}^\top {\mathbf T}\mathbf S_{\mathbf x\mathbf y}{\mathbf R}^\top {\mathbf R}\mathbf S_{\mathbf y\mathbf y}^{-1/2}{\mathbf R}^\top\\
&amp;={\mathbf T}\mathbf S_{\mathbf x\mathbf x}^{-1/2}\mathbf S_{\mathbf x\mathbf y}\mathbf S_{\mathbf y\mathbf y}^{-1/2}{\mathbf R}^\top\\
&amp;={\mathbf T} \mathbf Q_{\mathbf x\mathbf y}{\mathbf R}^\top.
\end{align*}\]</span>
So, again using the fact that <span class="math inline">\(\mathbf T\)</span> and <span class="math inline">\(\mathbf R\)</span> are orthogonal matrices, if <span class="math inline">\(\mathbf Q_{\mathbf x\mathbf y}\)</span> has SVD <span class="math inline">\(\sum_{j=1}^t \sigma_j {\mathbf u}_j {\mathbf v}_j^\top\)</span>, then <span class="math inline">\(\tilde{\mathbf Q}_{\mathbf h\mathbf k}\)</span> has SVD
<span class="math display">\[\begin{align*}
\tilde{\mathbf Q}_{\mathbf h\mathbf k}&amp;={\mathbf T }\mathbf Q_{\mathbf x\mathbf y}{\mathbf R}^\top
={\mathbf T} \left ( \sum_{j=1}^t \sigma_j {\mathbf u}_j {\mathbf v}_j^\top \right){\mathbf R}^\top\\
&amp;=\sum_{j=1}^t \sigma_j {\mathbf T}{\mathbf u}_j {\mathbf v}_j^\top {\mathbf R}^\top
=\sum_{j=1}^t \sigma_j \left ( {\mathbf T} {\mathbf u}_j \right )\left ({\mathbf R}{\mathbf v}_j  \right )^\top
=\sum_{j=1}^t \sigma_j \tilde{\mathbf u}_j \tilde{\mathbf v}_j^\top,
\end{align*}\]</span>
where, for <span class="math inline">\(j=1, \ldots,t\)</span>, the <span class="math inline">\(\tilde{\mathbf u}_j={\mathbf T}\mathbf u_j\)</span> are mutually orthogonal unit vectors,
and the <span class="math inline">\(\tilde{\mathbf v}_j={\mathbf R}{\mathbf v}_j\)</span> are also mutually orthogonal unit vectors.</p>
<p>Consequently, <span class="math inline">\(\tilde{\mathbf Q}_{\mathbf h k}\)</span> has the same singular values as <span class="math inline">\(\mathbf Q_{\mathbf x\mathbf y}\)</span>, namely <span class="math inline">\(\sigma_1, \ldots , \sigma_t\)</span> in both cases, and so the canonical correlation coefficients are invariant with respect to the transformations <a href="1-3-properties.html#eq:transformations">(1.12)</a>. Moreover, since the optimal linear combinations for the <span class="math inline">\(j\)</span>th CC in the original coordinates are given by <span class="math inline">\(\mathbf a_j =\mathbf S_{\mathbf x\mathbf x}^{-1/2}{\mathbf u}_j\)</span> and <span class="math inline">\(\mathbf b_j=\mathbf S_{\mathbf y\mathbf y}^{-1/2}{\mathbf v}_j\)</span>, the optimal linear combinations in the new coordinates are given by
<span class="math display">\[\begin{align*}
\tilde{\mathbf a}_{j}&amp;=\mathbf S_{\mathbf h\mathbf h}^{-1/2}{\mathbf T}{\mathbf u}_j\\
&amp;={\mathbf T}\mathbf S_{\mathbf x\mathbf x}^{-1/2}{\mathbf T}^\top {\mathbf T}{\mathbf u}_j\\
&amp;={\mathbf T}\mathbf S_{\mathbf x\mathbf x}^{-1/2}{\mathbf u}_j \\
&amp;={\mathbf T}\mathbf a_{j},
\end{align*}\]</span>
and a similar argument shows that <span class="math inline">\(\tilde{\mathbf b}_{j}={\mathbf R}\mathbf b_{j}\)</span>. So under transformations <a href="1-3-properties.html#eq:transformations">(1.12)</a>,
the optimal vectors <span class="math inline">\(\mathbf a_{j}\)</span> and <span class="math inline">\(\mathbf b_{j}\)</span> transform in an equivariant manner to <span class="math inline">\(\tilde{\mathbf a}_{j}\)</span> and <span class="math inline">\(\tilde{\mathbf b}_{j}\)</span>, respectively, <span class="math inline">\(j=1, \ldots , t\)</span>.</p>
<p>If either of <span class="math inline">\(\mathbf T\)</span> or <span class="math inline">\(\mathbf R\)</span> in <a href="1-3-properties.html#eq:transformations">(1.12)</a> is not an orthogonal matrix then the singular values are not invariant and the CC vectors do not transform in an equivariant manner.</p>
<!--## Testing for zero canonical correlation coefficients

So far in Part II of this module we have not considered formal statistical inference (e.g. hypothesis testing, construction of confidence regions).  Inference in various multivariate settings is considered in Part III.  However, before moving on, we briefly explain how to perform tests for zero correlations in the CCA setting, under the assumption that the $\bz_i∂ = (\bx_i^\top , \by_i^\top)^\top$ are IID multivariate normal.

As previously, suppose that the $\bx_i$ are $p \times 1$ vectors and the $\by_i$ are $q \times 1$ vectors and the sample size, i.e.  the number of $\bz_i$ vectors, is $n$.  Let $\bSigma_{\bx \by} =\text{Cov}(\bx,\by)$ denote the population cross-covariance matrix as before and consider the null hypothesis
$$
H_0: \, \bSigma_{\bx \by}={\mathbf 0}_{p,q},
$$
i.e. $\bSigma_{\bx \by}$ is the $p \times q$ matrix of zeros.  Let $H_A$ denote the general alternative
$$
H_A:\, \bSigma_{\bx \by} \quad *unrestricted*.
$$

Then the large-sample log-likelihood ratio test statistic for testing $H_0$ versus $H_A$ is as follows:
$$
W_0=-\left \{n-\frac{1}{2}(p+q+3)  \right \}\sum_{j=1}^{\min(p,q)} \log (1-\sigma_j^2),
$$
where $\sigma_1\geq \sigma_2 \cdots \geq \sigma_{\min(p,q)} \geq 0$ are the sample canonical correlations.
Moreover, when $n$ is large, $W_0$ is approximately $\chi_{pq}^2$ under $H_0$,  and $H_0$ should be rejected
when $W_0$ is sufficiently large.

We now consider a test concerning the rank of $\bSigma_{\bx \by}$.  For $0 \leq t <\min(p,q)$, consider  the hypothesis:
$$
H_t: \,   \text{at most $t$ of the CC coefficients are non-zero}.
$$
It turns out there is a similar statistic to $W_0$ above, for testing $H_t$ against $H_A$, defined by
$$
W_t=-\left \{n-\frac{1}{2}(p+q+3)  \right \}\sum_{j=t+1}^{\min(p,q)} \log (1-\sigma_j^2),
$$
where, under $H_t$ with $n$ large, $W_t$ is approximately $\chi_{(p-t)(q-t)}^2$.  Also, we reject
$H_t$ when $W_t$ is sufficiently large.


**Example \@ref(exm:prem) (continued)**.
  Here $p=q=2$, $n=20$ and $\sigma_1=0.974$ and $\sigma_2=0.508$.
So we should refer $W_0$ to $\chi_4^2$ and refer $W_1$ to $\chi_1^2$.  Here, $W_0=53.92$ and $W_1=4.92$.  So hypothesis $H_0$ is strongly rejected, with $p$-value $<\,<0.001$.  In contrast, $H_1$ is rejected at the $0.05$ level but is not rejected at the $0.01$ level.  So there is only moderate evidence that the 2nd CC coefficient is non-zero.

-->
<!--https://stats.idre.ucla.edu/r/dae/canonical-correlation-analysis/#:~:text=Canonical%20correlation%20analysis%20is%20used,among%20two%20sets%20of%20variables.&text=Canonical%20correlation%20analysis%20determines%20a,both%20within%20and%20between%20sets.


```r
mm <- read.csv("https://stats.idre.ucla.edu/stat/data/mmreg.csv")
colnames(mm) <- c("Control", "Concept", "Motivation", "Read", "Write", "Math",
"Science", "Sex")
psych <- mm[,1:3]
acad <- mm[,4:8]
library(CCA)
matcor(psych, acad)
```

```
## $Xcor
##              Control   Concept Motivation
## Control    1.0000000 0.1711878  0.2451323
## Concept    0.1711878 1.0000000  0.2885707
## Motivation 0.2451323 0.2885707  1.0000000
## 
## $Ycor
##                Read     Write       Math    Science         Sex
## Read     1.00000000 0.6285909  0.6792757  0.6906929 -0.04174278
## Write    0.62859089 1.0000000  0.6326664  0.5691498  0.24433183
## Math     0.67927568 0.6326664  1.0000000  0.6495261 -0.04821830
## Science  0.69069291 0.5691498  0.6495261  1.0000000 -0.13818587
## Sex     -0.04174278 0.2443318 -0.0482183 -0.1381859  1.00000000
## 
## $XYcor
##              Control     Concept Motivation        Read      Write       Math
## Control    1.0000000  0.17118778 0.24513227  0.37356505 0.35887684  0.3372690
## Concept    0.1711878  1.00000000 0.28857075  0.06065584 0.01944856  0.0535977
## Motivation 0.2451323  0.28857075 1.00000000  0.21060992 0.25424818  0.1950135
## Read       0.3735650  0.06065584 0.21060992  1.00000000 0.62859089  0.6792757
## Write      0.3588768  0.01944856 0.25424818  0.62859089 1.00000000  0.6326664
## Math       0.3372690  0.05359770 0.19501347  0.67927568 0.63266640  1.0000000
## Science    0.3246269  0.06982633 0.11566948  0.69069291 0.56914983  0.6495261
## Sex        0.1134108 -0.12595132 0.09810277 -0.04174278 0.24433183 -0.0482183
##                Science         Sex
## Control     0.32462694  0.11341075
## Concept     0.06982633 -0.12595132
## Motivation  0.11566948  0.09810277
## Read        0.69069291 -0.04174278
## Write       0.56914983  0.24433183
## Math        0.64952612 -0.04821830
## Science     1.00000000 -0.13818587
## Sex        -0.13818587  1.00000000
```

```r
cc1 <- cc(psych, acad)
```
-->
<!--## Example: ??????????????

Look at use in the scientific literature?

https://www.researchgate.net/figure/Canonical-correlation-analysis-maximizes-the-correlation-between-the-linear-combination_fig1_268154629


https://towardsdatascience.com/understanding-how-schools-work-with-canonical-correlation-analysis-4c9a88c6b913
-->

</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="1-2-the-full-set-of-canonical-correlations.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="1-4-computer-tasks.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["MultivariateStatistics.pdf"],
"toc": {
"collapse": "section"
},
"pandoc_args": "--top-level-division=[chapter|part]"
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
