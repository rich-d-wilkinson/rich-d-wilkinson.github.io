[["index.html", "Multivariate Statistics Introduction", " Multivariate Statistics Prof. Richard Wilkinson Spring 2021 Introduction This module is concerned with the analysis of multivariate data, in which the response is a vector of random variables rather than a single random variable. FIX FIX CHAPTER REFS Part I of the module describes some basic concepts in Multivariate Analysis and gives some examples of multivariate data (in Chapter 1), and also contains a summary of the matrix algebra that will be important in this module (Chapter 2). A theme running through the module is that of dimension reduction. In Part II we consider three types of dimension reduction: Principal Components Analysis (in Chapter 3), whose purpose is to identify the main modes of variation in a multivariate dataset; Canonical Correlation Analysis (Chapter 4), whose purpose is to describe the association between two sets of variables; and Multidimensional Scaling (Chapter 5), in which the starting point is a set of pairwise distances, suitably defined, between the objects under study. In Part III, we focus on methods of inference for multivariate data whose distribution is multivariate normal. First, in Chapter 6, we develop relevant distribution theory for the multivariate normal distribution. This includes a study of the Wishart distribution, which is a matrix generalisation of the chi-squared distribution, and Hotelling’s \\(T^2\\), which can be thought of as a multivariate generalisation of the Student \\(t\\) distribution. Then in Chapter 7 we focus on inference in multivariate one-sample and two-sample problems in which the underlying distribution is multivariate normal, making use of the distribution theory developed in Chapter 6. In Chapter 8, we focus on the multivariate linear model in which the dependent variable (or \\(y\\) variable) is a vector and the error distribution is multivariate normal. Finally, in Part IV, we focus on different methods of classification, i.e. allocating the observations in a sample to different subsets (or groups). In Chapter 9, we focus on an approach called discriminant analysis, in which we have a training sample available, and we use this training sample to set up a suitable classification rule. In Chapter 10, we consider an alternative approach, known as cluster analysis, in which we allocate the observations into clusters (or similar subsets) when a training sample is not available. If you find any typos or mistakes, please do email me at r.d.wilkinson@nottingham.ac.uk - I would like to fix them! ADD comment on high dimensional SPACE IS BIG! TO DO Miscellaneous topics Centering matrix, ellipses, lines. Is this useful and where should it go? "],["part-i-prerequisites.html", "PART I: Prerequisites", " PART I: Prerequisites Much of modern multivariate statistics (and machine learning) relies upon linear algebra. Consequently, we will spend some time reminding you of the basics of linear algebra (vector spaces, matrices etc), and introducing a few additional concepts that you may not have seen before. It is worth spending time familiarizing yourself with these ideas, as we will rely heavily upon this material in later chapters. In Chapter 1 we explain what we mean by multivariate analysis and give some examples of multivariate data. We also introduce basic definitions and concepts such as the sample covariance matrix, the sample correlation matrix and graphical techniques. We also briefly discuss random vectors and random matrices and derive some elementary properties. In Chapter 2 we summarise the definitions, ideas and results from matrix algebra that will be needed later in the module, most of which will be familiar to you. In particular, we will introduce vector spaces and the concept of a basis for a vector space, discuss the column, row and null space of matrices, and discuss inner product spaces and projections. In Chapter 3 we recap the eigen or spectral decomposition of square symmetric matrices, and introduce the singular value decomposition (SVD) which generalises the concept of eigenvalues for non-square matrices. We will rely upon this material in later chapters. "],["1-stat-prelim.html", "Chapter 1 Statistical Preliminaries", " Chapter 1 Statistical Preliminaries In this chapter we will define some notation, and recap some basic statistical properties and results. NOT YET FILMED VIDEOS. "],["1-1-notation.html", "1.1 Notation", " 1.1 Notation We will think of datasets as consisting of measurements of \\(p\\) different variables for \\(n\\) different cases/subjects. We organise the data into an \\(n \\times p\\) data matrix. Multivariate analysis (MVA) refers to data analysis methods where there are two or more response variables for each case (you are familiar with situations where there is more than one explanatory variable, e.g., multiple linear regression). We shall often write the data matrix as \\(\\mathbf X\\) (\\(n \\times p\\)) where \\[ {\\mathbf X}=\\left[ \\begin{array}{c} \\boldsymbol x_1^\\top\\\\ \\boldsymbol x_2^\\top\\\\ ..\\\\ ..\\\\ ..\\\\ \\boldsymbol x_n^\\top \\end{array}\\right ] \\] The vectors \\(\\boldsymbol x_1, \\ldots , \\boldsymbol x_n \\in \\mathbb{R}^p\\) are the observation vectors for each of the \\(n\\) subjects. The \\(n\\) rows of \\(\\mathbf X\\) are \\(\\boldsymbol x_1^\\top, \\ldots , \\boldsymbol x_n^\\top\\) - each row contains the \\(p\\) observations on a single subject. The \\(p\\) columns of \\(\\mathbf X\\) correspond to the \\(p\\) variables being measured, i.e., they contain the measurements of the same variable across all \\(n\\) subjects. Important remark on notation: Throughout the module we shall use non-bold letters, whether upper or lower case, to indicate scalar (i.e. real-valued) quantities, e.g., \\(x, y\\) lower-case letters in bold to signify column vectors, e.g., \\(\\boldsymbol x, \\boldsymbol y\\) upper case letters in bold to signify matrices, e.g., \\(\\boldsymbol X, \\boldsymbol Y\\). This convention for bold letters will also apply to random quantities. So, in particular, for a random vector we always use (bold) lower case, and for a random matrix we always use bold upper-case, regardless of whether we are referring to (i) the unobserved random quantity or (ii) its observed value. It should always be clear from the context which of these two interpretations (i) or (ii) is appropriate. 1.1.1 Example datasets Example 1.1 The football league table is an example of multivariate data. Here \\(W =\\) number of wins, \\(D =\\) number of draws, \\(F =\\) number of goals scored and \\(A =\\) number of goals conceded for four teams. In this example we have \\(p=4\\) variables \\((W, D, F, A)^\\top\\) measured on \\(n=4\\) cases (teams). Team W D F A USA 1 2 4 3 England 1 2 2 1 Slovenia 1 1 3 3 Algeria 0 1 0 2 The data vector for the USA is \\[x_1=(1,2,4,3)\\] Example 1.2 Exam marks for a set of \\(n\\) students where \\(P =\\) mark in probability and \\(S =\\) mark in statistics. Let \\(x_{ij}\\) denote the \\(j\\)th variable measured on the \\(i\\)th subject. Student P S 1 \\(x_{11}\\) \\(x_{12}\\) 2 \\(x_{21}\\) \\(x_{22}\\) \\(\\vdots\\) \\(\\vdots\\) \\(\\vdots\\) n \\(x_{n1}\\) \\(x_{n2}\\) Example 1.3 The iris dataset is a famous set of measurements collected on the sepal length and width, and the petal length and width, of 50 flowers for each of 3 species of iris (setosa, versicolor, and virginica). The dataset is built into R (try typing iris in R) and is often used to demonstrate multivariate statistical methods. For these data, \\(p=5\\), and \\(n=150\\). Sepal.Length Sepal.Width Petal.Length Petal.Width Species 5.1 3.5 1.4 0.2 setosa 4.9 3.0 1.4 0.2 setosa 4.7 3.2 1.3 0.2 setosa 7.0 3.2 4.7 1.4 versicolor 6.4 3.2 4.5 1.5 versicolor 6.9 3.1 4.9 1.5 versicolor 6.3 3.3 6.0 2.5 virginica 5.8 2.7 5.1 1.9 virginica 7.1 3.0 5.9 2.1 virginica Example 1.4 The MNIST dataset is a collection of handwritten digits that is widely used in statistics and machine learning to test algorithms. It contains 60,000 images of hand-written digits. Each digit has been converted to a grid of \\(28\\times 28\\) pixels, with a grayscale intensity level specified for each pixel. When we store these on a computer, we flatten each grid to a vector of length 784. So for this dataset, \\(n=60,000\\) and \\(p=784\\). 1.1.2 Aims of nultivariate data analysis The aim of multivariate statistical analysis is to answer questions such as: How can we visualise the data? What is the joint distribution of marks? Can we simplify the data? For example, we rank football teams using \\(3W+D\\) and we rank students by their average module mark. Is this fair? Can we reduce the dimension in a better way? Can we use the data to discriminate, for example, between male and female students? Are the different iris species different shapes? Can we build a model to predict the intended digit from an image of someones handwriting? Or predict the species of iris from measurements of its sepal and petal? We could just apply standard univariate techniques to each variable in turn, but this ignores possible dependencies between the variables which we must represent to draw valid conclusions. what is the difference between MVA and standard linear regression? in standard linear regression we have a scalar response variable, \\(y\\) say, and a vector of covariates, \\(\\boldsymbol x\\), say. The focus of interest is on how knowledge of \\(\\boldsymbol x\\) influences the distribution of \\(y\\) (in particular, the mean of \\(y\\)). In contrast, in MVA the focus is a vector \\(\\boldsymbol y\\), in which all the components of \\(\\boldsymbol y\\) are viewed as responses rather than covariates, possibly with additional covariate information \\(\\boldsymbol x\\). We will discuss this further in Chapter ??. "],["1-2-exploratory-data-analysis-eda.html", "1.2 Exploratory data analysis (EDA)", " 1.2 Exploratory data analysis (EDA) A picture is worth a thousand words Figure 1.1: Charles Joseph Minard’s famous map of Napoleon’s 1812 invasion of Russian. It displays six types of data in two dimensions. Before trying any form of statistical analysis, it is always a good idea to do some form of exploratory data analysis to understand the challenges presented by the data. As a minimum, this usually involves finding out whether each variable is continuous, discrete, or categorical, doing some basic visualization (plots), and perhaps computing a few summary statistics such as the mean and variance. 1.2.1 Data visualization Visualising datasets before fitting any models can be extremely useful. It allows us to see obvious patterns and relationships,and may suggest a sensible form of analysis. With multivariate data, finding the right kind of plot is not always simple, and many different approaches have been proposed. When \\(p=1\\) or \\(p=2\\) we can simply draw histograms and scatter plots (respectively) to view the distribution. For \\(p \\geq 3\\) the task is harder. One solution is a matrix of pair-wise scatter plots using the pairs command in R. The graph below shows the relationship between goals scored (F), goals against (A) and points (PT) for 20 teams during a recent Premiership season. Figure 1.2: Scatter plots of goals for (F), goals against (A) and points (PT) for a recent Premier League Season We can instantly see that points and goals scored are positively correlated, and that points and goals conceded (A) are negatively correlated (this is not a surprise of course). R has a good basic plotting functionality. However, we will sometimes use packages that provide additional functionality. The first time you use a package you may need to install it. We can use ggplot2 and GGally (which adds functionality to ggplot2) to add colour and detail to pairs plots. For example data(iris) library(ggplot2) library(GGally) # pairs(iris) # - try the pairs command for comparison ggpairs(iris, columns=1:4, mapping=ggplot2::aes(colour = Species), upper = list(continuous = wrap(&quot;cor&quot;, size = 3))) # fix the font size This plot allows us to instantly see that there are clear differences between the three species of iris, at least when we look at the pairs plots. The benefit of adding colour in this case is that we can see the differences between the different species. Note how the sepal length and width are (weakly) negatively correlated across the entire dataset, but are positively correlated when we look at a single species at a time. We would have missed this information if we only used the pairs command (try it!). Note that it is possible to miss key relationships when looking at marginals plots such as these, as they only show two variables at a time. More complex relationships between three or more variables will not be visible. It is difficult visualize data in three or more dimensions. Many different types of plot have been proposed (e.g. Google Chernoff faces). One approach is to use a parallel line plot ggparcoord(iris, 1:4, groupColumn=5) Each case is represented by a single line, and here we have the information shown for the four continuous variables. The fifth variable Species is a discrete factor, and is shown by colouring the lines. If you not familiar with ggplot2, a nice introduction can be found here. Details about `GGally can be found here. A good way to see the variety of plots that are possible, and to find code to create them, is to browse plot galleries such as those available here and here. 1.2.2 Summary statistics It is often useful to report a small number of numerical summaries of the data. In univariate statistics we define the sample mean and sample variance of samples \\(x_1, \\ldots, x_n\\) to be \\[ \\bar{x} = \\frac{1}{n} \\sum_{i=1}^n x_i \\quad \\text{and} \\quad s_{xx} = \\frac{1}{n} \\sum_{i=1}^n (x_i - \\bar{x})^2 \\] and for two samples, \\(x_1, \\ldots, x_n\\) and \\(y_1, \\ldots, y_n\\), we define the sample covariance to be \\[s_{xy}=\\frac{1}{n}\\sum_{i=1}^n (x_i-\\bar{x})(y_i-\\bar{y}).\\] Analogous multivariate quantities can be defined as follows: Definition 1.1 For a sample of \\(n\\) points, each containing \\(p\\) variables, \\(\\boldsymbol x_1, \\boldsymbol x_2, \\ldots, \\boldsymbol x_n \\in \\mathbb{R}^p\\), the sample mean and sample covariance matrix are \\[\\begin{align} \\bar{\\boldsymbol x} &amp;= \\frac{1}{n} \\sum_{i=1}^n \\boldsymbol x_i \\tag{1.1}\\\\ \\boldsymbol S&amp;= \\frac{1}{n} \\sum_{i=1}^n (\\boldsymbol x_i - \\bar{\\boldsymbol x}) (\\boldsymbol x_i - \\bar{\\boldsymbol x})^\\top \\tag{1.2} \\end{align}\\] where \\(\\boldsymbol x_i\\in \\mathbb{R}^p\\) denotes the \\(p\\) variables observed on the \\(i\\)th subject. Note that \\(\\bar{\\boldsymbol x} \\in \\mathbb{R}^p\\). The \\(j\\)th entry in \\(\\bar{\\boldsymbol x}\\) is simply the (univariate) sample mean of the \\(j\\)th variable. \\(\\boldsymbol S\\in \\mathbb{R}^{p\\times p}\\). Note that the \\(ij^{th}\\) entry of \\(\\boldsymbol S\\) is \\(s_{ij}\\), the sample covariance between variable \\(i\\) and variable \\(j\\). The \\(i^{th}\\) diagonal element is the (univariate) sample variance of the \\(i\\)th variable. \\(\\boldsymbol S\\) is symmetric since \\(s_{ij}=s_{ji}\\). an alternative formula for \\(\\boldsymbol S\\) is \\[\\boldsymbol S= \\frac{1}{n} \\left(\\sum_{i=1}^n \\boldsymbol x_i \\boldsymbol x_i^\\top \\right)- \\bar{\\boldsymbol x} \\bar{\\boldsymbol x}^\\top.\\] We have divided by \\(n\\) rather than \\(n-1\\) here, which gives the maximum likelihood estimator of the variance, rather than the unbiased variance estimator that is often used. Definition 1.2 The sample correlation matrix, \\(\\boldsymbol R\\), is the matrix with \\(ij^{th}\\) entry \\(r_{ij}\\) equal to the sample correlation between variables \\(i\\) and \\(j\\), that is \\[ r_{ij} = \\frac{s_{ij}}{\\sqrt{s_{ii}s_{jj}}}. \\] Note that If \\(\\boldsymbol D= \\text{diag}(\\sqrt{s_{11}}, \\dots, \\sqrt{s_{pp}})\\), then \\[ \\boldsymbol R= \\boldsymbol D^{-1} \\boldsymbol S\\boldsymbol D^{-1} \\] \\(\\boldsymbol R\\) is symmetric the diagonal entries of \\(\\boldsymbol R\\) are exactly 1 (each variable is perfectly correlated with itself) \\(|r_{ij}| \\leq 1\\) for all \\(i, j\\) Note that if we change the unit of measurement for the \\(\\boldsymbol x_i\\)’s then \\(\\boldsymbol S\\) will change but \\(\\boldsymbol R\\) will not. Definition 1.3 The total variation in a data set is usually measured by \\(\\text{tr}(\\boldsymbol S)\\) where \\(\\text{tr}()\\) is the trace function that sums the diagonal elements of the matrix. That is, \\[\\text{tr}(\\boldsymbol S) = s_{11} + s_{22} + \\ldots + s_{pp}.\\] In other words, it is the sum of the univariate variances of each of the \\(p\\) variables. "],["1-3-random-vectors-and-matrices.html", "1.3 Random vectors and matrices", " 1.3 Random vectors and matrices Definition 1.4 The population mean vector of the random vector \\(\\boldsymbol x\\) is \\[\\boldsymbol \\mu= {\\mathbb{E}}(\\boldsymbol x).\\] The population covariance matrix of \\(\\boldsymbol x\\) is \\[ \\boldsymbol \\Sigma= {\\mathbb{V}\\operatorname{ar}}(\\boldsymbol x) = {\\mathbb{E}}\\left((\\boldsymbol x-{\\mathbb{E}}(\\boldsymbol x))(\\boldsymbol x-{\\mathbb{E}}(\\boldsymbol x))^\\top \\right).\\] The covariance between \\(\\boldsymbol x\\) (\\(p \\times 1\\)) and \\(\\boldsymbol y\\) (\\(q \\times 1\\)) is \\[ {\\mathbb{C}\\operatorname{ov}}(\\boldsymbol x,\\boldsymbol y) = {\\mathbb{E}}\\left((\\boldsymbol x- {\\mathbb{E}}(\\boldsymbol x))(\\boldsymbol y- {\\mathbb{E}}(\\boldsymbol y))^\\top \\right). \\] Let \\(\\boldsymbol A\\) denote a \\(q \\times p\\) constant matrix, and let \\(\\boldsymbol b\\) a constant vector of size \\(q \\times 1\\). Expectation is a linear operator in the sense that \\[{\\mathbb{E}}(\\boldsymbol A\\boldsymbol x+ \\boldsymbol b) = \\boldsymbol A{\\mathbb{E}}(\\boldsymbol x) + \\boldsymbol b=\\boldsymbol A\\boldsymbol \\mu+\\boldsymbol b.\\] The following properties follow: \\({\\mathbb{V}\\operatorname{ar}}(\\boldsymbol x) = {\\mathbb{E}}(\\boldsymbol x\\boldsymbol x^\\top) - \\boldsymbol \\mu\\boldsymbol \\mu^\\top\\). \\({\\mathbb{V}\\operatorname{ar}}(\\boldsymbol A\\boldsymbol x+ \\boldsymbol b) = \\boldsymbol A\\boldsymbol \\Sigma\\boldsymbol A^\\top\\) \\({\\mathbb{C}\\operatorname{ov}}(\\boldsymbol x,\\boldsymbol y) = {\\mathbb{E}}(\\boldsymbol x\\boldsymbol y^\\top) - {\\mathbb{E}}(\\boldsymbol x) {\\mathbb{E}}(\\boldsymbol y)^\\top\\). \\({\\mathbb{C}\\operatorname{ov}}(\\boldsymbol x,\\boldsymbol x) = \\boldsymbol \\Sigma\\). \\({\\mathbb{C}\\operatorname{ov}}(\\boldsymbol x,\\boldsymbol y) = {\\mathbb{C}\\operatorname{ov}}(\\boldsymbol y,\\boldsymbol x)^\\top\\). \\({\\mathbb{C}\\operatorname{ov}}(\\boldsymbol A\\boldsymbol x,\\boldsymbol B\\boldsymbol y) = \\boldsymbol A{\\mathbb{C}\\operatorname{ov}}(\\boldsymbol x,\\boldsymbol y)\\boldsymbol B^\\top\\) If \\(p=q\\) then \\[ {\\mathbb{V}\\operatorname{ar}}(\\boldsymbol x+ \\boldsymbol y) = {\\mathbb{V}\\operatorname{ar}}(\\boldsymbol x) + {\\mathbb{V}\\operatorname{ar}}(\\boldsymbol y) + {\\mathbb{C}\\operatorname{ov}}(\\boldsymbol x,\\boldsymbol y) + {\\mathbb{C}\\operatorname{ov}}(\\boldsymbol y,\\boldsymbol x). \\] Finally, note that if \\(\\boldsymbol x\\) and \\(\\boldsymbol y\\) are independent (in which case I will write \\(\\boldsymbol x\\perp \\!\\!\\! \\perp\\boldsymbol y\\)) then \\({\\mathbb{C}\\operatorname{ov}}(\\boldsymbol x,\\boldsymbol y) = {\\mathbf 0}_{p,q}\\), i.e., a \\(p\\times q\\) matrix of zeros. 1.3.1 Estimators The population mean vector \\(\\boldsymbol \\mu\\) and population covariance matrix \\(\\boldsymbol \\Sigma\\) will usually be unknown. We can use data to estimate these quantities. The sample mean \\(\\bar{\\boldsymbol x}\\) is often used as an estimator of \\(\\boldsymbol \\mu\\). The sample covariance matrix \\(\\boldsymbol S\\) is often used as an estimator of \\(\\boldsymbol \\Sigma\\). Equation (1.1) gives an unbiased estimator of the sample mean. The sample covariance matrix (1.2) is a biased estimator of the population covariance matrix. An unbiased estimate is obtained by dividing by \\(n-1\\) rather than \\(n\\) in Equation (1.2). "],["1-4-computer-tasks.html", "1.4 Computer tasks", " 1.4 Computer tasks DUMPED HERE. Example 1.5 The table below shows the module marks for 5 students on the modules G11PRB (\\(P\\)) and G11STA (\\(S\\)). Student P S A 41 63 B 72 82 C 46 38 D 77 57 E 59 85 As an exercise, calculate the sample mean, sample covariance, sample correlation and total variation by hand. The sample mean is \\(\\bar{\\boldsymbol x} = \\begin{pmatrix} 59 \\\\ 65 \\end{pmatrix}\\). The sample covariance matrix is \\(\\boldsymbol S= \\begin{pmatrix} 197.2 &amp; 92.8 \\\\ 92.8 &amp; 297.2 \\end{pmatrix}\\). The sample correlation matrix is \\[\\begin{align*} \\boldsymbol R&amp;= \\boldsymbol D^{-1} \\boldsymbol S\\boldsymbol D^{-1}\\\\ &amp;= \\begin{pmatrix} 14.0 &amp; 0 \\\\ 0 &amp; 17.2 \\end{pmatrix}^{-1} \\begin{pmatrix} 197.2 &amp; 92.8 \\\\ 92.8 &amp; 297.2 \\end{pmatrix} \\begin{pmatrix} 14.0 &amp; 0 \\\\ 0 &amp; 17.2 \\end{pmatrix}^{-1} \\\\ &amp;= \\begin{pmatrix} 0.071 &amp; 0 \\\\ 0 &amp; 0.058 \\end{pmatrix} \\begin{pmatrix} 197.2 &amp; 92.8 \\\\ 92.8 &amp; 297.2 \\end{pmatrix} \\begin{pmatrix} 0.071 &amp; 0 \\\\ 0 &amp; 0.058 \\end{pmatrix} \\\\ &amp;= \\begin{pmatrix} 1.000 &amp; 0.383 \\\\ 0.383 &amp; 1.000 \\end{pmatrix}. \\end{align*}\\] The total variation is \\(\\text{tr}(\\boldsymbol S) = 197.2 + 297.2 = 494.4\\). To calculate these in R use, ‘colMeans’, ‘cov’, and ‘cor’. These assume each column is a different variable, and each row a different observation. library(dplyr) Ex1 &lt;- data.frame( Student=LETTERS[1:5], P = c(41,72,46,77,59), S = c(63,82,38,57,85) ) Ex1 %&gt;% knitr::kable(booktabs = TRUE) %&gt;% kable_styling(full_width = F) Student P S A 41 63 B 72 82 C 46 38 D 77 57 E 59 85 Ex1 %&gt;% select_if(is.numeric) %&gt;% colMeans ## P S ## 59 65 Ex1 %&gt;% select_if(is.numeric) %&gt;% cov ## P S ## P 246.5 116.0 ## S 116.0 371.5 Ex1 %&gt;% select_if(is.numeric) %&gt;% cov*4/5 ## P S ## P 197.2 92.8 ## S 92.8 297.2 Ex1 %&gt;% select_if(is.numeric) %&gt;% cor ## P S ## P 1.0000000 0.3833276 ## S 0.3833276 1.0000000 Ex1 %&gt;% select_if(is.numeric) %&gt;% cov %&gt;% diag %&gt;% sum*4/5 ## [1] 494.4 Note that by default R uses \\(n-1\\) in the denominator, whereas we used \\(n\\) in our calculation, hence the multiple of $4/5=(n-1)/n introduced in the covariance calculations above. We will be using the dplyr R package to perform basic data manipulation in R. If you are unfamiliar with dplyr, you can read about it at https://dplyr.tidyverse.org/. The pipe command %&gt;% is particularly useful for chaining together multiple commands. "],["1-5-exercises.html", "1.5 Exercises", " 1.5 Exercises Prove? \\({\\mathbb{V}\\operatorname{ar}}(\\boldsymbol x) = {\\mathbb{E}}(\\boldsymbol x\\boldsymbol x^\\top) - \\boldsymbol \\mu\\boldsymbol \\mu^\\top\\). \\({\\mathbb{V}\\operatorname{ar}}(\\boldsymbol A\\boldsymbol x+ \\boldsymbol b) = \\boldsymbol A\\boldsymbol \\Sigma\\boldsymbol A^\\top\\) \\({\\mathbb{C}\\operatorname{ov}}(\\boldsymbol x,\\boldsymbol y) = {\\mathbb{E}}(\\boldsymbol x\\boldsymbol y^\\top) - {\\mathbb{E}}(\\boldsymbol x) {\\mathbb{E}}(\\boldsymbol y)^\\top\\). \\({\\mathbb{C}\\operatorname{ov}}(\\boldsymbol x,\\boldsymbol x) = \\boldsymbol \\Sigma\\). \\({\\mathbb{C}\\operatorname{ov}}(\\boldsymbol x,\\boldsymbol y) = {\\mathbb{C}\\operatorname{ov}}(\\boldsymbol y,\\boldsymbol x)^\\top\\). \\({\\mathbb{C}\\operatorname{ov}}(\\boldsymbol A\\boldsymbol x,\\boldsymbol B\\boldsymbol y) = \\boldsymbol A{\\mathbb{C}\\operatorname{ov}}(\\boldsymbol x,\\boldsymbol y)\\boldsymbol B^\\top\\) If \\(p=q\\) then \\[ {\\mathbb{V}\\operatorname{ar}}(\\boldsymbol x+ \\boldsymbol y) = {\\mathbb{V}\\operatorname{ar}}(\\boldsymbol x) + {\\mathbb{V}\\operatorname{ar}}(\\boldsymbol y) + {\\mathbb{C}\\operatorname{ov}}(\\boldsymbol x,\\boldsymbol y) + {\\mathbb{C}\\operatorname{ov}}(\\boldsymbol y,\\boldsymbol x). \\] "],["2-linalg-prelim.html", "Chapter 2 Review of linear algebra", " Chapter 2 Review of linear algebra Modern statistics and machine learning rely heavily upon linear algebra, nowhere more so than in multivariate statistics. In the first part of this chapter (sections 2.1 and 2.2) we review some concepts from linear algebra that will be needed throughout the module, including vector spaces, row and column spaces, the rank of a matrix, etc. Hopefully most of this will be familiar to you. We then cover some basic details on inner-product or normed spaces in 2.3, which are vector spaces equipped with a concept of distance and angle. Section 3 is perhaps the most important section. Here we provide a reminder about eigenvalues and the spectral decomposition of square symmetric matrices, before introducing the singular value decomposition (SVD) in Section 3.4. The SVD is one of the most important concepts in this module, and is the key linear algebra technique behind many of the methods we will study. Finally, in Section 2.4 we will cover some miscellaneous topics that will be needed in later chapters. I do not provide proofs of all the results stated in this chapter, but instead prove a small selection which I think it is useful to see. For a complete treatment of the linear algebra needed for this module, see the excellent book “Linear algebra and learning from data” by Gilbert Strang. I have recorded videos on some (but not all) of the topics in these notes: Vector spaces Matrices Inner product spaces Orthogonal matrices Projection matrices NOT DONE A VIDEO ON CENTERING MATRIX, OR ELLIPSES, or VECTOR DIFFERENTIATION. "],["2-1-linalg-basics.html", "2.1 Basics", " 2.1 Basics In this section, we recap some basic definitions and notation. Hopefully this material will largely be familiar to you. 2.1.1 Notation The matrix \\({\\mathbf A}\\) will be referred to in the following equivalent ways: \\[\\begin{eqnarray*} {\\mathbf A}=\\stackrel{n\\times p}{\\mathbf A} &amp;=&amp; \\left[\\begin{array}{cccc} a_{11}&amp;a_{12}&amp;\\dots&amp;a_{1p}\\\\ a_{21}&amp;a_{22}&amp;\\dots&amp;a_{2p}\\\\ \\vdots&amp;\\vdots&amp;&amp;\\vdots\\\\ a_{n1}&amp;a_{n2}&amp;\\dots&amp;a_{np} \\end{array} \\right] \\\\ &amp;=&amp;[a_{ij}: i=1, \\ldots , m; j=1, \\ldots , n]\\\\ &amp;=&amp;(a_{ij})\\\\ &amp;=&amp; \\left[ \\begin{array}{c}\\boldsymbol a_1^\\top\\\\ \\vdots\\\\ \\boldsymbol a_n^\\top\\end{array}\\right] \\end{eqnarray*}\\] where the \\(a_{ij}\\) are the individual entries, and \\(\\boldsymbol a_i^\\top=(a_{i1}, a_{i2}, \\ldots, a_{ip})\\) is the \\(i^{th}\\) row. A matrix of order \\(1\\times 1\\) is called a scalar. A matrix of order \\(n\\times 1\\) is called a (column) vector. A matrix of order \\(1\\times p\\) is called a (row) vector. e.g. \\(\\stackrel{n\\times 1}{\\mathbf a}=\\left( \\begin{array}{c} a_1\\\\\\vdots\\\\a_n \\end{array} \\right)\\)is a column vector. The \\(n\\times n\\) identity matrix \\({\\mathbf I}_n\\) has diagonal elements equal to 1 and off-diagonal elements equal to zero. A diagonal matrix is an \\(n \\times n\\) matrix whose off-diagonal elements are zero. Sometimes we denote a diagonal matrix by \\(\\text{diag}\\{a_1,\\ldots, a_n\\}\\). \\[\\mathbf I_3 = \\left(\\begin{array}{ccc} 1&amp;0&amp;0\\\\ 0&amp;1&amp;0\\\\ 0&amp;0&amp;1\\end{array}\\right),\\quad \\text{diag}\\{1,2,3\\}=\\left(\\begin{array}{ccc} 1&amp;0&amp;0\\\\ 0&amp;2&amp;0\\\\ 0&amp;0&amp;3\\end{array}\\right)\\quad\\] 2.1.2 Elementary matrix operations Addition/Subtraction. If \\(\\stackrel{n\\times p}{\\mathbf A}=[a_{ij}]\\) and \\(\\stackrel{n\\times p}{\\mathbf B}=[b_{ij}]\\) are given matrices then \\[ {\\mathbf A}+{\\mathbf B}=[a_{ij}+b_{ij}] \\qquad \\text{and} \\qquad {\\mathbf A}-{\\mathbf B}=[a_{ij}-b_{ij}].\\] Scalar Multiplication. If \\(\\lambda\\) is a scalar and \\({\\mathbf A}=[a_{ij}]\\) then \\[\\lambda {\\mathbf A}=[\\lambda a_{ij}].\\] Matrix Multiplication. If \\(\\stackrel{n\\times p}{\\mathbf A}\\) and \\(\\stackrel{p\\times q}{\\mathbf B}\\) are matrices then \\(\\boldsymbol A\\boldsymbol B=\\stackrel{n\\times q}{\\mathbf C}=[c_{ij}]\\) where \\[c_{ij}=\\sum _{k=1}^p a_{ik}b_{kj}, \\qquad i=1,\\dots,n, \\qquad j=1,\\dots ,q.\\] Matrix Transpose. If \\(\\stackrel{m \\times n}{\\boldsymbol A}=[a_{ij}: i=1, \\ldots , m; j=1, \\ldots , n]\\), then the transpose of \\(\\boldsymbol A\\), written \\(\\boldsymbol A^\\top\\), is given by the \\(n \\times m\\) matrix \\[ \\boldsymbol A^\\top =[a_{ji}: j=1, \\ldots , n; i=1, \\ldots, m]. \\] Note from the definitions that \\((\\boldsymbol A\\boldsymbol B)^\\top={\\mathbf B}^\\top {\\mathbf A}^\\top\\). Matrix Inverse. The inverse of a matrix \\(\\stackrel{n\\times n}{\\mathbf A}\\) (if it exists) is a matrix \\(\\stackrel{n\\times n}{\\mathbf B}\\) such that \\({\\mathbf A}\\boldsymbol B=\\boldsymbol B\\boldsymbol A={\\mathbf I}_n.\\) We denote the inverse by \\({\\mathbf A}^{-1}\\). Note that if \\({\\mathbf A}_1\\) and \\({\\mathbf A}_2\\) are both invertible, then \\(({\\mathbf A}_1 {\\mathbf A}_2)^{-1}={\\mathbf A}_2^{-1}{\\mathbf A}_1^{-1}\\). Trace. The trace of a matrix \\(\\stackrel{n\\times n}{\\mathbf A}\\) is given by \\[ \\text{tr}({\\mathbf A})=\\sum _{i=1}^n a_{ii}.\\] Lemma 2.1 For any matrices \\(\\boldsymbol A\\) (\\(n \\times m\\)) and \\(\\boldsymbol B\\) (\\(m \\times n\\)), \\[ \\text{tr}(\\boldsymbol A\\boldsymbol B) = \\text{tr}(\\boldsymbol B\\boldsymbol A). \\] The determinant of a square matrix \\(\\stackrel{n\\times n}{\\mathbf A}\\) is defined as \\[ \\text{det}({\\mathbf A})=\\sum (-1)^{|\\tau |} a_{1\\tau(1)}\\dots a_{n\\tau (n)} \\] where the summation is taken over all permutations \\(\\tau\\) of \\(\\{1,2,\\dots ,n\\}\\), and we define \\(|\\tau |=0\\) or \\(1\\) depending on whether \\(\\tau\\) can be written as an even or odd number of transpositions. E.g. If \\({\\mathbf A}=\\left[ \\begin{array}{cc} a_{11}&amp;a_{12}\\\\ a_{21}&amp;a_{22} \\end{array} \\right]\\), then \\(\\text{det}({\\mathbf A})=a_{11}a_{22}-a_{12}a_{21}\\). Proposition 2.1 Matrix \\(\\stackrel{n\\times n}{\\mathbf A}\\) is invertible if and only if \\(\\det(\\boldsymbol A)\\not = 0\\). If \\(\\boldsymbol A^{-1}\\) exists then \\[\\det(\\boldsymbol A)=\\frac{1}{\\det(\\boldsymbol A^{-1})}\\] Proposition 2.2 For any matrices \\(\\stackrel{n\\times n}{\\mathbf A}\\), \\(\\stackrel{n\\times n}{\\mathbf B}\\), \\(\\stackrel{n\\times n}{\\mathbf C}\\) such that \\({\\mathbf C}={\\mathbf{AB}}\\), \\[ \\text{det}({\\mathbf C})=\\text{det}({\\mathbf A}) \\cdot \\text{det}({\\mathbf B}).\\] 2.1.3 Special matrices Definition 2.1 An \\(n\\times n\\) matrix \\(\\boldsymbol A\\) is symmetric if \\[\\boldsymbol A= \\boldsymbol A^\\top.\\] An \\(n\\times n\\) symmetric matrix \\(\\boldsymbol A\\) is positive-definite if \\[\\boldsymbol x^\\top \\boldsymbol A\\boldsymbol x&gt;0 \\mbox{ for all } \\boldsymbol x\\in \\mathbb{R}^n, \\boldsymbol x\\not = \\boldsymbol 0\\] and is positive semi-definite if \\[\\boldsymbol x^\\top \\boldsymbol A\\boldsymbol x\\geq 0 \\mbox{ for all } \\boldsymbol x\\in \\mathbb{R}^n.\\] \\(\\boldsymbol A\\) is idempotent if \\(\\boldsymbol A^2=\\boldsymbol A\\). 2.1.4 Vector Differentiation Consider a real-valued function \\(f: \\mathbb{R}^p \\rightarrow \\mathbb{R}\\) of a vector variable \\(\\boldsymbol x=(x_1, \\ldots , x_p)^\\top\\). Sometimes we will want to differentiate \\(f\\). We define the partial derivative of \\(f(\\boldsymbol x)\\) with respect to \\(\\boldsymbol x\\) to be the vector of partial derivatives, i.e. \\[\\begin{equation} \\frac{\\partial f}{\\partial \\boldsymbol x}(\\boldsymbol x)=\\left [ \\begin{array}{c} \\frac{\\partial f}{\\partial x_1}(\\boldsymbol x)\\\\ ..\\\\ ..\\\\ ..\\\\ \\frac{\\partial f}{\\partial x_p}(\\boldsymbol x) \\end{array} \\right ] \\tag{2.1} \\end{equation}\\] The following examples can be worked out directly from the definition (2.1), using the chain rule in some cases. Example 2.1 If \\(f(\\boldsymbol x)=\\boldsymbol a^\\top \\boldsymbol x\\) where \\(\\boldsymbol a\\in \\mathbb{R}^p\\) is a constant vector, then \\[ \\frac{\\partial f}{\\partial \\boldsymbol x}(\\boldsymbol x)=\\boldsymbol a. \\] Example 2.2 If \\(f(\\boldsymbol x)=(\\boldsymbol x-\\boldsymbol a)^\\top \\boldsymbol A(\\boldsymbol x-\\boldsymbol a)\\) for a fixed vector \\(\\boldsymbol a\\in \\mathbb{R}^p\\) and \\(\\boldsymbol A\\) is a symmetric constant \\(p \\times p\\) matrix, then \\[ \\frac{\\partial f}{\\partial \\boldsymbol x}(\\boldsymbol x)=2\\boldsymbol A(\\boldsymbol x-\\boldsymbol a). \\] Example 2.3 Suppose that \\(g: \\, \\mathbb{R} \\rightarrow \\mathbb{R}\\) is a differentiable function with derivative \\(g^\\prime\\). Then, using the chain rule for partial derivatives, \\[ \\frac{\\partial g(\\boldsymbol a^\\top \\boldsymbol x)}{\\partial \\boldsymbol x}=g^{\\prime}(\\boldsymbol a^\\top\\boldsymbol x)\\frac{\\partial}{\\partial \\boldsymbol x}\\left \\{\\boldsymbol a^\\top \\boldsymbol x\\right \\}=g^{\\prime}(\\boldsymbol a^\\top\\boldsymbol x) \\boldsymbol a. \\] Example 2.4 If \\(f\\) is defined as in Example 2.2 and \\(g\\) is as in Example 2.3 then, using the chain rule again, \\[ \\frac{\\partial }{\\partial \\boldsymbol x} g\\{f(\\boldsymbol x)\\}=g^{\\prime} \\{f(\\boldsymbol x)\\}\\frac{\\partial f}{\\partial \\boldsymbol x}(\\boldsymbol x) =2 g^{\\prime}\\{(\\boldsymbol x- \\boldsymbol a)^\\top \\boldsymbol A(\\boldsymbol x- \\boldsymbol a)\\}\\boldsymbol A(\\boldsymbol x-\\boldsymbol a). \\] If we wish to find a maximum or minimum of \\(f(\\boldsymbol x)\\) we should search for stationary points of \\(f\\), i.e. solutions to the system of equations \\[ \\frac{\\partial f}{\\partial \\boldsymbol x}(\\boldsymbol x)\\equiv \\left [ \\begin{array}{c} \\frac{\\partial f}{\\partial x_1}(\\boldsymbol x)\\\\ ..\\\\ ..\\\\ ..\\\\ \\frac{\\partial f}{\\partial x_p}(\\boldsymbol x) \\end{array} \\right ]={\\mathbf 0}_p. \\] Definition 2.2 The Hessian matrix of \\(f\\) is the \\(p \\times p\\) matrix of second derivatives. \\[ \\frac{\\partial^2f}{\\partial \\boldsymbol x\\partial \\boldsymbol x^\\top}(\\boldsymbol x) =\\left \\{ \\frac{\\partial^2 f(\\boldsymbol x)}{\\partial x_j \\partial x_k}\\right \\}_{j,k=1}^p. \\] The nature of a stationary point is determined by the Hessian If the Hessian is positive (negative) definite at a stationary point \\(\\boldsymbol x\\), then the stationary point is a minimum (maximum). If the Hessian has both positive and negative eigenvalues at \\(\\boldsymbol x\\) then the stationary point will be a saddle point. "],["2-2-linalg-vecspaces.html", "2.2 Vector spaces", " 2.2 Vector spaces It will be useful to talk about vector spaces. These are sets of vectors that can be added together, or multiplied by a scalar. You should be familiar with these from your undergraduate degree. We don’t provide a formal definition here, but you can think of a real vector space \\(V\\) as a set of vectors such that for any \\(\\boldsymbol v_1, \\boldsymbol v_2 \\in V\\) and \\(\\alpha_1, \\alpha_2 \\in \\mathbb{R}\\), we have \\[\\alpha_1 \\boldsymbol v_1 + \\alpha_2 \\boldsymbol v_2 \\in V\\] i.e., vector spaces are closed under addition and scalar multiplication. Example 2.5 Euclidean space in \\(p\\) dimensions, \\(\\mathbb{R}^p\\), is a vector space. If we add any two vectors in \\(\\mathbb{R}^p,\\) or multiply a vector by a real scalar, then the resulting vector also lies in \\(\\mathbb{R}^p\\). A subset \\(U \\subset V\\) of a vector space \\(V\\) is called a vector subspace if \\(U\\) is also a vector space. Example 2.6 Let \\(V=\\mathbb{R}^2\\). Then the sets \\[U_1 = \\left\\{\\left( \\begin{array}{c} a\\\\ 0 \\end{array} \\right): a\\in \\mathbb{R}\\right\\}, \\mbox{ and}\\quad U_2 = \\left\\{a\\left( \\begin{array}{c} 1 \\\\ 1 \\end{array} \\right): a\\in \\mathbb{R}\\right\\}\\] are both subspaces of \\(V\\). 2.2.1 Linear independence Definition 2.3 Vectors \\(\\stackrel{n\\times 1}{\\mathbf x}_1 ,\\dots , \\stackrel{n\\times 1}{\\mathbf x}_p\\) are said to be linearly dependent if there exist scalars \\(\\lambda _1, \\dots ,\\lambda _p\\) not all zero such that \\[ \\lambda _1 {\\mathbf x}_1+\\lambda _2 {\\mathbf x}_2+ \\dots + \\lambda _p {\\mathbf x}_p={\\mathbf 0}.\\] Otherwise, these vectors are said to be linearly independent. Definition 2.4 Given a set of vectors \\(S=\\{s_1, \\ldots, s_n\\}\\), the span of \\(S\\) is the smallest vector space containing \\(S\\) or equivalently, is the set of all linear combinations of vectors from \\(S\\) \\[\\operatorname{span}(S) = \\left\\{ \\sum_{i=1}^k \\alpha_i s_i \\mid k \\in \\mathbb{N}, \\alpha_i \\in \\mathbb{R}, s_i \\in S\\right\\}\\] Definition 2.5 A basis of a vector space \\(V\\) is a set of linearly independent vectors in \\(V\\) that span \\(V\\). Example 2.7 Consider \\(V=\\mathbb{R}^2\\). Then the following are both bases for \\(V\\): \\[B_1=\\left\\{\\left(\\begin{array}{c}1\\\\0\\end{array}\\right), \\left(\\begin{array}{c}0\\\\1\\end{array}\\right)\\right\\} \\] \\[B_2=\\left\\{\\left(\\begin{array}{c}1\\\\1\\end{array}\\right), \\left(\\begin{array}{c}1\\\\2\\end{array}\\right)\\right\\} \\] Definition 2.6 The dimension of a vector space is the number of vectors in its basis. 2.2.2 Row and column spaces We can think about the matrix-vector multiplication \\(\\boldsymbol A\\boldsymbol x\\) in two ways. The usual way is as the inner product between the rows of \\(A\\) and \\(x\\). \\[ \\left( \\begin{array}{cc} 1 &amp; 2\\\\ 3&amp;4\\\\5&amp;6\\end{array}\\right) \\left(\\begin{array}{c}x_1\\\\ x_2\\end{array}\\right) = \\left(\\begin{array}{c} x_1+2x_2\\\\3x_1+4x_2\\\\5x_1+6x_2\\end{array}\\right)\\] But a better way to think of \\(\\boldsymbol A\\boldsymbol x\\) is as a linear combination of the columns of \\(A\\). \\[ \\left( \\begin{array}{cc} 1 &amp; 2\\\\ 3&amp;4\\\\5&amp;6\\end{array}\\right) \\left(\\begin{array}{c}x_1\\\\ x_2\\end{array}\\right) = x_1\\left(\\begin{array}{c}1\\\\3\\\\5 \\end{array}\\right)+x_2\\left(\\begin{array}{c}2\\\\4\\\\6 \\end{array}\\right)\\] Definition 2.7 The column space of a \\(n\\times p\\) matrix \\(\\boldsymbol A\\) is the set of all linear combinations of the columns of \\(\\boldsymbol A\\): \\[\\mathcal{C}(\\boldsymbol A) = \\{\\boldsymbol A\\boldsymbol x: \\boldsymbol x\\in \\mathbb{R}^p\\}\\subset \\mathbb{R}^n\\] For \\[A=\\left( \\begin{array}{cc} 1 &amp; 2\\\\ 3&amp;4\\\\5&amp;6\\end{array}\\right) \\] we can see that the column space is a 2-dimensional plane in \\(\\mathbb{R}^3\\). The matrix \\(\\boldsymbol B\\) has the same column space as \\(\\boldsymbol A\\) \\[\\boldsymbol B=\\left( \\begin{array}{cccc} 1 &amp; 2&amp;3 &amp;4\\\\ 3&amp;4 &amp;7&amp;10\\\\5&amp;6&amp;11&amp;16\\end{array}\\right) \\] The number of linearly independent columns of \\(\\boldsymbol A\\) is called the column rank of \\(\\boldsymbol A\\), and is equal to the dimension of the column space of \\(\\mathcal{C}(\\boldsymbol A)\\). The column rank of \\(\\boldsymbol A\\) and \\(\\boldsymbol B\\) is 2. The row space of \\(\\boldsymbol A\\) is defined to be the column space of \\(\\boldsymbol A^\\top\\), and the row rank is the number of linearly independent rows of \\(\\boldsymbol A\\). Theorem 2.1 The row rank of of a matrix equals the column rank. Thus we can simply refer to the rank of the matrix. Proof. The proof of this theorem is very simple. Let \\(\\boldsymbol C\\) be an \\(n \\times r\\) matrix (where \\(r=\\operatorname{rank}(\\boldsymbol A)\\)) with columns chosen to be a set of \\(r\\) linearly independent columns from \\(A\\). Then we know each column of \\(\\boldsymbol A\\) can be written as a linear combination of the columns of \\(\\boldsymbol C\\), i.e. \\[\\boldsymbol A= \\boldsymbol C\\boldsymbol R.\\] The dimension of \\(\\boldsymbol R\\) must be \\(r \\times p\\). But now we can see that the rows of \\(\\boldsymbol A\\) are formed by a linear combination of the rows of \\(\\boldsymbol R\\). Thus the row rank of \\(\\boldsymbol A\\) is at most \\(r\\) (=the column rank of \\(\\boldsymbol A\\)). This holds for any matrix, so is true for \\(\\boldsymbol A^\\top\\): namely \\(\\operatorname{row-rank}(A^\\top)\\leq \\operatorname{column-rank}(A^\\top)\\). But the row space of \\(\\boldsymbol A^\\top\\) equals \\(\\mathcal{C}(\\boldsymbol A)\\), thus proving the theorem! Corollary 2.1 The rank of an \\(n\\times p\\) matrix is at most \\(\\min(n,p)\\). Example 2.8 \\[B = \\left( \\begin{array}{cccc} 1 &amp; 2\\\\ 3&amp;4 \\\\5&amp;6\\end{array}\\right)\\left(\\begin{array}{cccc}1&amp;0&amp;1&amp;2\\\\0&amp;1&amp;1&amp;1\\end{array}\\right) \\] Example 2.9 \\[ D=\\left( \\begin{array}{ccc} 1 &amp; 2&amp;3\\\\ 2&amp;4&amp;6 \\end{array}\\right)= \\left( \\begin{array}{c} 1 \\\\ 2 \\end{array}\\right)\\left(\\begin{array}{ccc}1&amp;2&amp;3\\end{array}\\right) \\] So the rank of \\(D\\) is \\(1\\). 2.2.3 Linear transformations We can view an \\(n\\times p\\) matrix \\(\\boldsymbol A\\) as a linear map between two vector spaces: \\[\\begin{align*} \\boldsymbol A: \\;\\mathbb{R}^p &amp;\\rightarrow \\mathbb{R}^n\\\\ \\boldsymbol x&amp;\\mapsto \\boldsymbol A\\boldsymbol x \\end{align*}\\] The image of \\(\\boldsymbol A\\) is precisely the column space of \\(\\boldsymbol A\\): \\[\\operatorname{Im}(\\boldsymbol A) = \\{\\boldsymbol A\\boldsymbol x: \\boldsymbol x\\in \\mathbb{R}^p\\}=\\mathcal{C}(\\boldsymbol A) \\subset \\mathbb{R}^n\\] The kernel of \\(A\\) is the set of vectors mapped to zero: \\[\\operatorname{Ker}(\\boldsymbol A)=\\{\\boldsymbol x: \\boldsymbol A\\boldsymbol x=\\boldsymbol 0\\}\\subset \\mathbb{R}^p\\] and is sometimes called the null-space of \\(\\boldsymbol A\\) and denoted \\(\\mathcal{N}(\\boldsymbol A)\\). Theorem 2.2 The rank-nullity theorem says if \\(V\\) and \\(W\\) are vector spaces, and \\(A: V\\rightarrow W\\) is a linear map, then \\[\\dim \\operatorname{Im}(A)+\\dim \\operatorname{Ker}(A) =\\dim V\\] If we’re thinking about matrices, then \\(\\dim \\mathcal{C}(\\boldsymbol A)+\\dim \\mathcal{N}(\\boldsymbol A)=p\\), or equivalently that \\(\\operatorname{rank}(\\boldsymbol A)+\\dim \\mathcal{N}(\\boldsymbol A)=p\\). We’ve already said that the row space of \\(\\boldsymbol A\\) is \\(\\mathcal{C}(\\boldsymbol A^\\top)\\). The left-null space is \\(\\{\\boldsymbol x\\in \\mathbb{R}^n: \\boldsymbol x^\\top \\boldsymbol A=0\\}\\) or equivalently \\(\\{x \\in \\mathbb{R}^n: \\boldsymbol A^\\top \\boldsymbol x=0\\}=\\mathcal{N}(\\boldsymbol A^\\top)\\). And so by the rank-nullity theorem we must have \\[n=\\dim \\mathcal{C}(\\boldsymbol A^\\top) + \\dim \\mathcal{N}(\\boldsymbol A^\\top)= \\operatorname{rank}(\\boldsymbol A)+\\dim \\operatorname{Ker}(\\boldsymbol A^\\top).\\] Example 2.10 Consider again the matrix \\(D: \\mathbb{R}^3\\rightarrow \\mathbb{R}^2\\) \\[ D=\\left( \\begin{array}{ccc} 1 &amp; 2&amp;3\\\\ 2&amp;4&amp;6 \\end{array}\\right)= \\left( \\begin{array}{c} 1 \\\\ 2 \\end{array}\\right)\\left(\\begin{array}{ccc}1&amp;2&amp;3\\end{array}\\right) \\] We have already seen that \\[\\mathcal{C}(D)=\\operatorname{span}\\left\\{\\left(\\begin{array}{c}1\\\\2\\end{array}\\right)\\right\\}\\] and so \\(\\dim \\mathcal{C}(D)=\\operatorname{rank}(D)=1\\). The kernel, or null-space, of \\(\\boldsymbol D\\) is the set of vectors for which \\(\\boldsymbol D\\boldsymbol x=\\boldsymbol 0\\), i.e., \\[x_1+2x_2+3x_3=0\\] This is a single equation with three unknowns, and so there must be a plane of solutions. We need two linearly independent vectors in this plane to describe it. Convince yourself that \\[\\mathcal{N}(D) = \\operatorname{span}\\left\\{\\left(\\begin{array}{c}0\\\\3\\\\-2\\end{array}\\right), \\left(\\begin{array}{c}2\\\\-1\\\\0\\end{array}\\right)\\right\\}\\] So we have \\[\\dim \\mathcal{C}(D)+\\dim \\mathcal{N}(D)=1+2=3\\] as required by the rank-nullity theorem. If we consider \\(D^\\top\\), we already know \\(\\dim \\mathcal{C}(D)=1\\) (as row-rank=column rank), and the rank-nullity theorem tells us that the dimension of the null space of \\(D^\\top\\) must be \\(2-1=1\\). This is easy to confirm as \\(D^\\top x=0\\) implies \\[x_1+2x_2=0\\] which is a line in \\(\\mathbb{R}^2\\) \\[\\mathcal{N}(D^\\top) = \\operatorname{span}\\left\\{ \\left(\\begin{array}{c}-2\\\\1\\end{array}\\right)\\right\\}\\] Question: When does a square matrix \\(\\boldsymbol A\\) have an inverse? Precisely when the kernel of \\(\\boldsymbol A\\) contains only the zero vector, i.e., has dimension 0. In this case the column space of \\(\\boldsymbol A\\) is the original space, and \\(\\boldsymbol A\\) is surjective and so must have an inverse. A simpler way to determine if \\(\\boldsymbol A\\) has an inverse is to consider its determinant. Question: Suppose we are given a \\(n\\times p\\) matrix \\(\\boldsymbol A\\), and a n-vector \\(\\boldsymbol y\\). When does \\[\\boldsymbol A\\boldsymbol x= \\boldsymbol y\\] have a solution? When \\(\\boldsymbol y\\) is in the column space of \\(\\boldsymbol A\\), \\[\\boldsymbol y\\in \\mathcal{C}(\\boldsymbol A)\\] Question: When is the answer unique? Suppose \\(\\boldsymbol x\\) and \\(\\boldsymbol x&#39;\\) are both solutions with \\(\\boldsymbol x\\not =\\boldsymbol x&#39;\\). We can write \\(\\boldsymbol x&#39;=\\boldsymbol x+\\boldsymbol u\\) for some vector \\(\\boldsymbol u\\) and note that \\[\\boldsymbol y=\\boldsymbol A\\boldsymbol x&#39; = \\boldsymbol A\\boldsymbol x+\\boldsymbol A\\boldsymbol u= \\boldsymbol y+\\boldsymbol A\\boldsymbol u\\] and so \\(\\boldsymbol A\\boldsymbol u=\\boldsymbol 0\\), i.e., \\(\\boldsymbol u\\in \\mathcal{N}(A)\\). So there are multiple solutions when the null-space of \\(\\boldsymbol A\\) contains more than the zero vector. If the dimension of \\(\\mathcal{N}(A)\\) is one, there is a line of solutions. If the dimension is two, there is a plane of solutions, etc. "],["2-3-linalg-innerprod.html", "2.3 Inner product spaces", " 2.3 Inner product spaces 2.3.1 Distances, and angles Vector spaces are not particularly interesting from a statistical point of view until we equip them with a sense of geometry, i.e. distance and angle. Definition 2.8 A real inner product space \\((V, \\langle\\cdot,\\cdot\\rangle)\\) is a real vector space \\(V\\) equipped with a map \\[ \\langle\\cdot,\\cdot\\rangle : V \\times V \\rightarrow \\mathbb{R}\\] such that \\(\\langle\\cdot,\\cdot\\rangle\\) is a linear map in both arguments: \\[\\langle \\alpha \\boldsymbol v_1+\\beta \\boldsymbol v_2, \\boldsymbol u\\rangle = \\alpha \\langle \\boldsymbol v_1, \\boldsymbol u\\rangle + \\beta \\langle \\boldsymbol v_2, \\boldsymbol u\\rangle\\] for all \\(\\boldsymbol v_1, \\boldsymbol v_2, \\boldsymbol u\\in V\\) and \\(\\alpha, \\beta \\in \\mathbb{R}\\). \\(\\langle\\cdot,\\cdot\\rangle\\) is symmetric in its arguments: \\(\\langle \\boldsymbol v, \\boldsymbol u\\rangle = \\langle \\boldsymbol u, \\boldsymbol v\\rangle\\) for all \\(\\boldsymbol u,\\boldsymbol v\\in V\\) \\(\\langle\\cdot,\\cdot\\rangle\\) is positive definite: \\(\\langle \\boldsymbol v, \\boldsymbol v\\rangle \\geq 0\\) for all \\(\\boldsymbol v\\in V\\) with equality if and only if \\(\\boldsymbol v={\\mathbf 0}\\). An inner product provides a vector space with the concepts of distance: for all \\(v\\in V\\) define the norm of \\(v\\) to be \\[||\\boldsymbol v|| = \\langle \\boldsymbol v, \\boldsymbol v\\rangle ^{\\frac{1}{2}}\\] Thus any inner-product space \\((V, \\langle\\cdot,\\cdot\\rangle)\\) is also a normed space \\((V, ||\\cdot||)\\), and a metric space \\((V, d(\\boldsymbol x,\\boldsymbol y)=||\\boldsymbol x-\\boldsymbol y||)\\). angle: for \\(\\boldsymbol u, \\boldsymbol v\\in V\\) we define the angle between \\(\\boldsymbol u\\) and \\(\\boldsymbol v\\) to be \\(\\theta\\) where \\[\\begin{align*} \\langle \\boldsymbol u,\\boldsymbol v\\rangle &amp;= ||\\boldsymbol u||.||\\boldsymbol v||\\cos \\theta\\\\ \\implies \\theta &amp;= \\cos^{-1}\\left( \\frac{\\langle \\boldsymbol u, \\boldsymbol v\\rangle}{||\\boldsymbol u|| \\;||\\boldsymbol v||}\\right) \\end{align*}\\] We will primarily be interested in the concept of orthogonality. We say \\(\\boldsymbol u, \\boldsymbol v\\in V\\) are orthogonal if \\[\\langle \\boldsymbol u, \\boldsymbol v\\rangle =0\\] i.e., the angle between them is \\(\\frac{\\pi}{2}\\). If you have done any functional analysis, you may recall that a Hilbert space is a complete inner-product space, and a Banach space is a complete normed space. This is an applied module, so we will skirt much of the technical detail, but note that some of the proofs formally require us to be working in a Banach or Hilbert space. We will not concern ourselves with such detail. Example 2.11 We will mostly be working with the Euclidean vector spaces \\(V=\\mathbb{R}^n\\), in which we use the Euclidean inner product \\[\\langle \\boldsymbol u, \\boldsymbol v\\rangle = \\boldsymbol u^\\top \\boldsymbol v\\] sometimes called the scalar or dot product of \\(\\boldsymbol u\\) and \\(\\boldsymbol v\\). Sometimes this gets weighted by a matrix so that \\[\\langle \\boldsymbol u, \\boldsymbol v\\rangle_Q = \\boldsymbol u^\\top \\boldsymbol Q\\boldsymbol v.\\] The norm associated with the dot product is the square root of the sum of squared errors, denoted by \\(|| \\cdot ||_2\\). The length of \\(\\boldsymbol u\\) is then \\[||\\boldsymbol u||_2=\\sqrt{\\boldsymbol u^\\top \\boldsymbol u} =\\left( \\sum_{i=1}^n u_i^2\\right)^\\frac{1}{2}\\geq 0.\\] Note that \\(||\\boldsymbol u||_2=0\\) if and only if \\(\\boldsymbol u={\\mathbf 0}_n\\) where \\(\\stackrel{n\\times 1}{\\mathbf 0}_n=(0,0,\\dots ,0)^\\top\\). We say \\(\\boldsymbol u\\) is orthogonal to \\(\\boldsymbol v\\) if \\(\\boldsymbol u^\\top \\boldsymbol v=0\\). For example, if \\[\\boldsymbol u=\\left(\\begin{array}{c}1\\\\2\\end{array}\\right) \\mbox{ and } \\boldsymbol v=\\left(\\begin{array}{c}-2\\\\1\\end{array}\\right)\\] then \\[||\\boldsymbol u||_2 = \\sqrt{5}\\mbox{ and } \\boldsymbol u^\\top \\boldsymbol v=0.\\] We will write \\(\\boldsymbol u\\perp \\boldsymbol v\\) if \\(\\boldsymbol u\\) is orthogonal to \\(\\boldsymbol v\\). Definition 2.9 p-norm: The subscript \\(2\\) hints at a wider family of norms. We define the \\(L_p\\) norm to be \\[|| \\boldsymbol v||_p = \\left(\\sum_{i=1}^n |v_i|^p\\right)^\\frac{1}{p}.\\] 2.3.2 Orthogonal matrices Definition 2.10 A unit vector \\(\\mathbf v\\) is a vector satisfying \\(||{\\mathbf v}||=1\\), i.e., it is a vector of length \\(1\\). Vectors \\(\\boldsymbol u\\) and \\(\\boldsymbol v\\) are orthonormal if \\[||\\boldsymbol u||=||\\boldsymbol v|| = 1 \\mbox{ and } \\langle \\boldsymbol u, \\boldsymbol v\\rangle =0.\\] An \\(n\\times n\\) matrix \\({\\mathbf Q}\\) is an orthogonal matrix if \\[{\\mathbf Q}\\boldsymbol Q^\\top = {\\mathbf Q}^\\top {\\mathbf Q}={\\mathbf I}_n.\\] Equivalently, a matrix \\(\\mathbf Q\\) is orthogonal if \\({\\mathbf Q}^{-1}={\\mathbf Q}^\\top.\\) If \\({\\mathbf Q}=[\\boldsymbol q_1,\\ldots, \\boldsymbol q_n]\\) is an orthogonal matrix, then the columns \\(\\boldsymbol q_1, \\ldots, \\boldsymbol q_n\\) are mutually orthonormal vectors, i.e. \\[ \\boldsymbol q_j^\\top \\boldsymbol q_k=\\begin{cases} 1 &amp;\\hbox{ if } j=k\\\\ 0 &amp;\\hbox{ if } j \\neq k. \\\\ \\end{cases} \\] Lemma 2.2 Let \\(\\boldsymbol Q\\) be a \\(n\\times p\\) matrix and suppose \\(\\boldsymbol Q^\\top \\boldsymbol Q=\\mathbf I_p\\), where \\(\\mathbf I_p\\) is the \\(p \\times p\\) identity matrix. If \\(\\boldsymbol Q\\) is a square matrix (\\(n=p\\)), then \\(\\boldsymbol Q\\boldsymbol Q^\\top = \\mathbf I_p\\). If \\(\\boldsymbol Q\\) is not square (\\(n\\not =p\\)), then \\(\\boldsymbol Q\\boldsymbol Q^\\top \\not = I_n\\). Proof. Suppose \\(n=p\\), and think of \\(\\boldsymbol Q\\) as a linear map&quot;&quot; \\[\\begin{align*} \\boldsymbol Q: &amp;\\mathbb{R}^n \\rightarrow \\mathbb{R}^n\\\\ &amp;\\boldsymbol v\\mapsto \\boldsymbol Q\\boldsymbol v \\end{align*}\\] By the rank-nullity theorem, \\[\\dim \\operatorname{Ker}(\\boldsymbol Q) + \\dim \\operatorname{Im}(\\boldsymbol Q) =n\\] and because \\(\\boldsymbol Q\\) has a left-inverse, we must have \\(\\dim \\operatorname{Ker}(\\boldsymbol Q)=0\\), as otherwise \\(\\boldsymbol Q^\\top\\) would have to map from a vector space of dimension less than \\(n\\) to \\(\\mathbb{R}^n\\). So \\(\\boldsymbol Q\\) is of full rank, and thus must also have a right inverse, \\(\\boldsymbol B\\) say, with \\(\\boldsymbol Q\\boldsymbol B=\\mathbf I_n\\). If we left multiply by \\(\\boldsymbol Q^\\top\\) we get \\[\\begin{align*} \\boldsymbol Q\\boldsymbol B&amp;=\\mathbf I_n\\\\ \\boldsymbol Q^\\top\\boldsymbol Q\\boldsymbol B&amp;=\\boldsymbol Q^\\top\\\\ \\mathbf I_n \\boldsymbol B&amp;= \\boldsymbol Q^\\top\\\\ \\boldsymbol B&amp;= \\boldsymbol Q^\\top\\\\ \\end{align*}\\] and so we have that \\(\\boldsymbol Q^{-1}=\\boldsymbol Q^\\top\\). Now suppose \\(\\boldsymbol Q\\) is \\(n \\times p\\) with \\(n\\not = p\\). Then as \\(\\boldsymbol Q^\\top \\boldsymbol Q=\\mathbf I_{p\\times p}\\), we must have \\(\\operatorname{tr}(\\boldsymbol Q^\\top \\boldsymbol Q)=p\\). This implies that \\[\\operatorname{tr}(\\boldsymbol Q\\boldsymbol Q^\\top)=\\operatorname{tr}(\\boldsymbol Q^\\top \\boldsymbol Q)=m\\] and so we cannot have \\(\\boldsymbol Q\\boldsymbol Q^\\top=\\mathbf I_{n}\\) as \\(\\operatorname{tr}{\\mathbf I_{n}}=n\\). Corollary 2.2 If \\(\\boldsymbol q_1, \\ldots , \\boldsymbol q_n\\) are mutually orthogonal \\(n \\times 1\\) unit vectors then \\[ \\sum_{i=1}^n \\boldsymbol q_i \\boldsymbol q_i^\\top = {\\mathbf I}_n. \\] Proof. Let \\(\\boldsymbol Q\\) be the matrix with \\(i^{th}\\) column \\(\\boldsymbol q_i\\) \\[\\boldsymbol Q=\\left( \\begin{array}{ccc} | &amp;&amp;|\\\\ \\boldsymbol q_1&amp; \\ldots&amp; \\boldsymbol q_n\\\\ | &amp;&amp;| \\end{array}\\right).\\] Then \\(\\boldsymbol Q^\\top \\boldsymbol Q=\\mathbf I_n\\), and \\(\\boldsymbol Q\\) is \\(n\\times n\\). Thus by Lemma 2.2, we must also have \\(\\boldsymbol Q\\boldsymbol Q^\\top=\\mathbf I_n\\) and if we think about matrix-matrix multiplication as columns times rows (c.f. section 3.1), we get \\[\\mathbf I_n=\\boldsymbol Q\\boldsymbol Q^\\top=\\left( \\begin{array}{ccc} | &amp;&amp;|\\\\ \\boldsymbol q_1&amp; \\ldots&amp; \\boldsymbol q_n\\\\ | &amp;&amp;| \\end{array}\\right) \\left( \\begin{array}{ccc} - &amp;\\boldsymbol q_1^\\top&amp;-\\\\ &amp; \\vdots&amp; \\\\ - &amp;\\boldsymbol q_n^\\top&amp;- \\end{array}\\right) = \\sum_{i=1}^n \\boldsymbol q_i \\boldsymbol q_i^\\top\\] as required. 2.3.3 Projections Definition 2.11 \\(\\stackrel{n \\times n}{\\boldsymbol P}\\) is a projection matrix if \\[\\boldsymbol P^2 =\\boldsymbol P\\] i.e., if it is idempotent. View \\(\\boldsymbol P\\) as a map from a vector space \\(W\\) to itself. Let \\(U=\\operatorname{Im}(\\boldsymbol P)\\) and \\(V=\\operatorname{Ker}(\\boldsymbol P)\\) be the image and kernel of \\(\\boldsymbol P\\). Proposition 2.3 We can write \\(\\boldsymbol w\\in W\\) as the sum of \\(\\boldsymbol u\\in U\\) and \\(\\boldsymbol v\\in V\\). Proof. Let \\(\\boldsymbol w\\in W\\). Then \\[\\boldsymbol w= \\mathbf I_n \\boldsymbol w=(\\mathbf I-\\boldsymbol P)\\boldsymbol w+ \\boldsymbol P\\boldsymbol w\\] Now \\(\\boldsymbol P\\boldsymbol w\\in \\operatorname{Im}(\\boldsymbol P)\\) and \\((\\mathbf I-\\boldsymbol P)\\boldsymbol w\\in \\operatorname{Ker}(\\boldsymbol P)\\) as \\[\\boldsymbol P(\\mathbf I-\\boldsymbol P)\\boldsymbol w= (\\boldsymbol P-\\boldsymbol P^2)\\boldsymbol w=\\boldsymbol 0.\\] Proposition 2.4 If \\(\\stackrel{n \\times n}{\\boldsymbol P}\\) is a projection matrix then \\({\\mathbf I}_n - \\boldsymbol P\\) is also a projection matrix. The kernel and image of \\(\\mathbf I-\\boldsymbol P\\) are the image and kernel (respectively) of \\(\\boldsymbol P\\): \\[\\begin{align*} \\operatorname{Ker}(\\mathbf I-\\boldsymbol P) &amp;= U=\\operatorname{Im}(\\boldsymbol P)\\\\ \\operatorname{Im}(\\mathbf I-\\boldsymbol P) &amp;= V=\\operatorname{Ker}(\\boldsymbol P). \\end{align*}\\] 2.3.3.1 Orthogonal projection We are mostly interested in orthogonal projections. Definition 2.12 If \\(W\\) is an inner product space, and \\(U\\) is a subspace of \\(W\\), then the orthogonal projection of \\(\\boldsymbol w\\in W\\) onto \\(U\\) is the unique element \\(\\boldsymbol u\\in U\\) that minimizes \\[||\\boldsymbol w-\\boldsymbol u||.\\] In other words, the orthogonal projection of \\(\\boldsymbol w\\) onto \\(U\\) is the best possible approximation of \\(\\boldsymbol w\\) in \\(U\\). As above, we can split \\(W\\) into \\(U\\) and its orthogonal compliment \\[U^\\perp = \\{\\boldsymbol x\\in W: \\langle \\boldsymbol x,\\boldsymbol u\\rangle = 0\\}\\] i.e., \\(W=U \\oplus U^\\perp\\) so that any \\(\\boldsymbol w\\in W\\) can be written as \\(\\boldsymbol w=\\boldsymbol u+\\boldsymbol v\\) with \\(\\boldsymbol u\\in U\\) and \\(\\boldsymbol v\\in U^\\perp\\). Proposition 2.5 If \\(\\{\\boldsymbol u_1, \\ldots, \\boldsymbol u_k\\}\\) is a basis for \\(U\\), then the orthogonal projection matrix (i.e., the matrix that projects \\(\\boldsymbol w\\in W\\) onto \\(U\\)) is \\[\\boldsymbol P_U = \\boldsymbol A(\\boldsymbol A^\\top \\boldsymbol A)^{-1}\\boldsymbol A^\\top\\] where \\(\\boldsymbol A=[\\boldsymbol u_1\\; \\ldots\\; \\boldsymbol u_k]\\) is the matrix with columns given by the basis vectors. Proof. We need to find \\(\\boldsymbol u= \\sum \\lambda_i \\boldsymbol u_i = \\boldsymbol A\\boldsymbol \\lambda\\) that minimizes \\(||\\boldsymbol w-\\boldsymbol u||\\). \\[\\begin{align*} ||\\boldsymbol w-\\boldsymbol u||^2 &amp;= \\langle \\boldsymbol w-\\boldsymbol u, \\boldsymbol w-\\boldsymbol u\\rangle\\\\ &amp;= \\boldsymbol w^\\top \\boldsymbol w- 2\\boldsymbol u^\\top \\boldsymbol w+ \\boldsymbol u^\\top \\boldsymbol u\\\\ &amp;= \\boldsymbol w^\\top \\boldsymbol w-2\\boldsymbol \\lambda^\\top \\boldsymbol A^\\top \\boldsymbol w+ \\boldsymbol \\lambda^\\top \\boldsymbol A^\\top \\boldsymbol A\\boldsymbol \\lambda. \\end{align*}\\] Differentiating with respect to \\(\\boldsymbol \\lambda\\) and setting equal to zero gives \\[\\boldsymbol 0=-2 \\boldsymbol A^\\top \\boldsymbol w+2 \\boldsymbol A^\\top \\boldsymbol A\\boldsymbol \\lambda\\] and hence \\[ \\boldsymbol \\lambda= (\\boldsymbol A^\\top \\boldsymbol A)^{-1}\\boldsymbol A^\\top \\boldsymbol w.\\] The orthogonal projection of \\(\\boldsymbol w\\) is hence \\[ \\boldsymbol A\\boldsymbol \\lambda= \\boldsymbol A(\\boldsymbol A^\\top \\boldsymbol A)^{-1}\\boldsymbol A^\\top \\boldsymbol w\\] and the projection matrix is \\[\\boldsymbol P_U = \\boldsymbol A(\\boldsymbol A^\\top \\boldsymbol A)^{-1}\\boldsymbol A^\\top. \\] Notes: If \\(\\{\\boldsymbol u_1, \\ldots, \\boldsymbol u_k\\}\\) is an orthonormal basis for \\(U\\) then \\(\\boldsymbol A^\\top \\boldsymbol A= \\mathbf I\\) and \\(\\boldsymbol P_U = \\boldsymbol A\\boldsymbol A^\\top\\). We can then write \\[\\boldsymbol P_U\\boldsymbol w= \\sum_i (\\boldsymbol u_i^\\top \\boldsymbol w) \\boldsymbol u_i\\] and \\[\\boldsymbol P_U = \\sum_{i=1}^k \\boldsymbol u_i\\boldsymbol u_i^\\top.\\] Note that if \\(U=W\\) (so that \\(\\boldsymbol P_U\\) is a projection from \\(W\\) onto \\(W\\), i.e., the identity), then \\(\\boldsymbol A\\) is a square matrix (\\(n\\times n\\)) and thus \\(\\boldsymbol A^\\top\\boldsymbol A=\\mathbf I_n \\implies \\boldsymbol A\\boldsymbol A^\\top\\) and thus \\(\\boldsymbol P_U=\\mathbf I_n\\) as required. The coordinates (with respect to the orthonormal basis \\(\\{\\boldsymbol u_1, \\ldots, \\boldsymbol u_k\\}\\)) of a point \\(\\boldsymbol w\\) projected onto \\(U\\) are \\(\\boldsymbol A^\\top \\boldsymbol w\\). \\(\\boldsymbol P_U^2=\\boldsymbol P_U\\), so \\(\\boldsymbol P_U\\) is a projection matrix in the sense of definition 2.11. \\(\\boldsymbol P_U\\) is symmetric (\\(\\boldsymbol P_U^\\top=\\boldsymbol P_U\\)). This is true for orthogonal projection matrices, but not in general for projection matrices. Example 2.12 Consider the vector space \\(\\mathbb{R}^2\\) and let \\(\\boldsymbol u=\\frac{1}{\\sqrt{2}}\\left(\\begin{array}{c}1\\\\1\\end{array}\\right)\\). The projection of \\(\\boldsymbol v\\in \\mathbb{R}^2\\) onto \\(\\boldsymbol u\\) is given by \\((\\boldsymbol v^\\top \\boldsymbol u) \\boldsymbol u\\). So for example, if \\(\\boldsymbol v= (2, \\; 1)^\\top\\), then its projection onto \\(\\boldsymbol u\\) is \\[\\boldsymbol P_U \\boldsymbol v= \\frac{3}{\\sqrt{2}}\\left(\\begin{array}{c}1\\\\1\\end{array}\\right).\\] Alternatively, if we treat \\(\\boldsymbol u\\) as a basis for \\(U\\), then the coordinate of \\(\\boldsymbol P_U \\boldsymbol v\\) with respect to the basis is \\(3\\). To check this, draw a picture! 2.3.3.2 Geometric interpretation of linear regresssion Consider the linear regression model \\[\\boldsymbol y= \\boldsymbol X\\boldsymbol \\beta+\\boldsymbol e\\] where \\(\\boldsymbol y\\in\\mathbb{R}^n\\) is the vector of observations, \\(\\boldsymbol X\\) is the \\(n\\times p\\) design matrix, \\(\\boldsymbol \\beta\\) is the \\(p\\times 1\\) vector of parameters that we wish to estimate, and \\(\\boldsymbol e\\) is a \\(n\\times 1\\) vector of zero-mean errors. Least-squares regression tries to find the value of \\(\\boldsymbol \\beta\\in \\mathbb{R}^p\\) that minimizes the sum of squared errors, i.e., we try to find \\(\\boldsymbol \\beta\\) to minimize \\[||\\boldsymbol y- \\boldsymbol X\\boldsymbol \\beta||_2\\] We know that \\(\\boldsymbol X\\boldsymbol \\beta\\) is in the column space of \\(\\boldsymbol X\\), and so we can see that linear regression aims to find the orthogonal projection onto \\(\\mathcal{C}(X)\\). \\[\\boldsymbol P_U\\boldsymbol y=\\arg \\min_{\\boldsymbol y&#39;: \\boldsymbol y&#39; \\in \\mathcal{C}(X)} ||\\boldsymbol y-\\boldsymbol y&#39;||_2.\\] By Proposition 2.5 this is \\[\\boldsymbol P_U\\boldsymbol y= \\boldsymbol X(\\boldsymbol X^\\top \\boldsymbol X)^{-1}\\boldsymbol X^\\top \\boldsymbol y=\\hat{\\boldsymbol y}\\] which equals the usual prediction obtained in linear regression (\\(\\hat{\\boldsymbol y}\\) are often called the fitted values). We can also see that the choice of \\(\\boldsymbol \\beta\\) that specifies this point in \\(\\mathcal{C}(X)\\) is \\[\\hat{\\boldsymbol \\beta}=(\\boldsymbol X^\\top \\boldsymbol X)^{-1}\\boldsymbol X^\\top \\boldsymbol y\\] which is the usual least-squares estimator. "],["2-4-linalg-misc.html", "2.4 Miscellaneous topics", " 2.4 Miscellaneous topics 2.4.1 The Centering Matrix The centering matrix will be play an important role in this module, as we will use it to remove the column means from a matrix (so that each column has mean zero), centering the matrix. Definition 2.13 The centering matrix is \\[\\begin{equation} \\boldsymbol H=\\mathbf I_n - \\frac{1}{n} {\\mathbf 1}_n {\\mathbf 1}_n^\\top. \\tag{2.2} \\end{equation}\\] where \\(\\mathbf I_n\\) is the \\(n \\times n\\) identity matrix, and \\({\\mathbf 1}_n\\) is an \\(n \\times 1\\) column vector of ones. You will be asked to prove the following results about \\(\\boldsymbol H\\) in the example sheets: The matrix \\(\\boldsymbol H\\) is a projection matrix, i.e. \\(\\boldsymbol H^\\top =\\boldsymbol H\\) and \\(\\boldsymbol H^2=\\boldsymbol H\\). Writing \\({\\mathbf 0}_n\\) for the \\(n \\times 1\\) vector of zeros, we have \\(\\boldsymbol H{\\mathbf 1}_n={\\mathbf 0}_n\\) and \\({\\mathbf 1}_n^\\top \\boldsymbol H={\\mathbf 0}_n^\\top.\\) In words: the sum of each row and each column of \\(\\boldsymbol H\\) is \\(0\\). If \\(\\boldsymbol x=(x_1, \\ldots , x_n)^\\top\\), then \\(\\boldsymbol H\\boldsymbol x= \\boldsymbol x- \\bar{x}{\\mathbf 1}_n\\) where \\(\\bar{x}=n^{-1}\\sum_{i=1}^n x_i\\). I.e., \\(H\\) subtracts the mean \\(\\bar{x}\\) from \\(\\boldsymbol x\\). With \\(\\boldsymbol x\\) as in 3., we have \\[ \\boldsymbol x^\\top \\boldsymbol H\\boldsymbol x= \\sum_{i=1}^n (x_i-\\bar{x})^2, \\] and so \\[ \\frac{1}{n}\\boldsymbol x^\\top \\boldsymbol H\\boldsymbol x=\\frac{1}{n}\\sum_{i=1}^n (x_i-\\bar{x})^2 = \\hat{\\sigma}^2, \\] where \\(\\hat{\\sigma}^2\\) is the sample variance. If \\[\\boldsymbol X=\\left[\\begin{array}{ccc}-&amp;\\boldsymbol x_1^\\top&amp;-\\\\ &amp;\\vdots&amp; \\\\ -&amp;\\boldsymbol x_n^\\top&amp;-\\end{array}\\right] = [\\boldsymbol x_1, \\ldots, \\boldsymbol x_n]^\\top\\] is an \\(n \\times p\\) data matrix containing data points \\(\\boldsymbol x_1, \\ldots, \\boldsymbol x_n\\in \\mathbb{R}^p\\), then \\[ \\boldsymbol H\\boldsymbol X=\\left[ \\begin{array}{ccc} -&amp;(\\boldsymbol x_1-\\bar{\\boldsymbol x})^\\top&amp;-\\\\ -&amp;(\\boldsymbol x_2 -\\bar{\\boldsymbol x})^\\top&amp;-\\\\ &amp;\\vdots&amp;\\\\ -&amp;(\\boldsymbol x_n - \\bar{\\boldsymbol x})^\\top&amp;- \\end{array}\\right ]= \\left[ \\boldsymbol x_1 -\\bar{\\boldsymbol x}, \\ldots , \\boldsymbol x_n-\\bar{\\boldsymbol x}\\right]^\\top \\] where \\[\\bar{\\boldsymbol x} = \\frac{1}{n} \\sum_{i=1}^n \\boldsymbol x_i \\in \\mathbb{R}^p\\] is the p-dimensional sample mean of \\(\\boldsymbol x_1, \\ldots, \\boldsymbol x_n\\in \\mathbb{R}^p\\). In words, \\(\\boldsymbol H\\) has subtracted the column mean from each column of \\(\\boldsymbol X\\). With \\(\\boldsymbol X\\) as in 5. \\[ \\frac{1}{n}\\boldsymbol X^\\top \\boldsymbol H\\boldsymbol X=\\frac{1}{n} \\sum_{i=1}^n (\\boldsymbol x_i -\\bar{\\boldsymbol x})(\\boldsymbol x_i -\\bar{\\boldsymbol x})^\\top =\\boldsymbol S, \\] where \\(\\boldsymbol S\\) is the sample covariance matrix. If \\(\\boldsymbol A=(a_{ij})_{i,j=1}^n\\) is a symmetric \\(n \\times n\\) matrix, then \\[ \\boldsymbol B=\\boldsymbol H\\boldsymbol A\\boldsymbol H= \\boldsymbol A- {\\mathbf 1}_n \\bar{\\boldsymbol a}_+^\\top -\\bar{\\boldsymbol a}_+{\\mathbf 1}_n^\\top +\\bar{a}_{++}{\\mathbf 1}_n {\\mathbf 1}_n^\\top, \\] or, equivalently, \\[ b_{ij}=a_{ij}-\\bar{a}_{i+}-\\bar{a}_{+j}+\\bar{a}_{++}, \\qquad i,j=1, \\ldots , n, \\] where \\[ \\bar{\\boldsymbol a}_{+}\\equiv (\\bar{a}_{1+}, \\ldots , \\bar{a}_{n+})^\\top=\\frac{1}{n}\\boldsymbol A{\\mathbf 1}_n, \\] \\(\\bar{a}_{+j}=\\bar{a}_{j+}\\) , for , \\(j=1, \\ldots , n\\),, and , \\(\\bar{a}_{++}=n^{-2}\\sum_{i,j=1}^n a_{ij}\\). Note that Property 3. is a special case of Property 5., and Property 4. is a special case of Property 6. However, it is useful to see these results in the simpler scalar case before moving onto the general matrix case. 2.4.2 Quadratic forms and ellipses POSSIBLY MOVE OR ADD PICTURES - DECIDE ONCE I KNOW WHERE It IS USED. A standard ellipse in \\(\\mathbb{R}^2\\) is given by the equation \\[ \\frac{x^2}{a^2}+\\frac{y^2}{b^2}=1 \\quad (a&gt;b&gt;0). \\] The interior (the shaded region) is given by \\[\\begin{equation} \\frac{x^2}{a^2}+\\frac{y^2}{b^2}\\leq 1. \\tag{2.3} \\end{equation}\\] Note that a standard ellipse has axes of symmetry given by the \\(x\\)-axis and \\(y\\)-axis (if \\(a&gt;b\\), the former is the major axis and the latter the minor axis). If we define \\({\\mathbf A}=\\left[ \\begin{array}{cc} a^2&amp;0\\\\ 0&amp;b^2 \\end{array} \\right]\\) then Equation (2.3) can be written in the form \\[ \\binom{x}{y}^\\top {\\mathbf A}^{-1} \\binom{x}{y}\\leq 1. \\] If we write \\({\\mathbf x}=\\binom{x_1}{x_2}\\) and generalise to an arbitrary symmetric positive definite matrix \\(\\stackrel{2 \\times 2}{\\mathbf A}\\), what is the set \\[ \\{ {\\mathbf x} \\in \\mathbb{R}^2 : {\\mathbf x}^\\top {\\mathbf A}^{-1} {\\mathbf x} \\leq 1\\} ? \\] We get a rotated ellipse with axes of symmetry given by the eigenvectors of \\(\\mathbf A\\), with the major axis determined by the eigenvector corresponding to the larger eigenvalue of \\(\\mathbf A\\), and the minor axis determined by the eigenvector corresponding to the smaller eigenvalue of \\(\\mathbf A\\). Note that, for \\(c&gt;0\\), \\[{\\mathbf x}^\\top {\\mathbf A}^{-1} {\\mathbf x}\\leq c \\qquad \\Leftrightarrow \\qquad {\\mathbf x}^\\top (c{\\mathbf A})^{-1}{\\mathbf x}\\leq 1 ,\\] where \\(c{\\mathbf A}\\) is a scalar multiple of \\(\\mathbf A\\). If \\({\\mathbf m}\\) is a fixed 2-vector, then what is the set \\[ \\{ {\\mathbf x} \\in \\mathbb{R}^2 : ({\\mathbf x}-{\\mathbf m})^\\top {\\mathbf A}^{-1}({\\mathbf x}-{\\mathbf m})\\leq 1\\} ? \\] Since \\[ \\{ {\\mathbf x} : ({\\mathbf x}-{\\mathbf m})^\\top {\\mathbf A}^{-1}({\\mathbf x}-{\\mathbf m})\\leq 1 \\}=\\{ {\\mathbf z}+ {\\mathbf m} : {\\mathbf z}^\\top {\\mathbf A}^{-1} {\\mathbf z}\\leq 1 \\} , \\] it follows that \\[ \\{ {\\mathbf x} : ({\\mathbf x}-{\\mathbf m})^\\top {\\mathbf A}^{-1}({\\mathbf x}-{\\mathbf m})\\leq 1\\} \\] is just the ellipse \\(\\{ {\\mathbf z}:{\\mathbf z}^\\top {\\mathbf A}^{-1}{\\mathbf z}\\leq 1\\}\\) translated by \\({\\mathbf m}\\). Analogous results for ellipsoids and quadratic forms hold in three and higher dimensions. 2.4.3 Lines and Hyperplanes in \\(\\mathbb{R}^p\\) For any \\(\\boldsymbol a, \\boldsymbol b\\in \\mathbb{R}^p\\), the set \\[\\begin{equation} \\mathcal{L}=\\mathcal{L}(\\boldsymbol a, \\boldsymbol b)=\\{\\boldsymbol a+\\gamma \\boldsymbol b: \\gamma \\in \\mathbb{R}\\} \\tag{2.4} \\end{equation}\\] is a straight line in \\(\\mathbb{R}^p\\). If \\(\\boldsymbol a^\\top \\boldsymbol b=0\\), i.e. \\(\\boldsymbol a\\) and \\(\\boldsymbol b\\) are orthogonal, then \\(\\boldsymbol a\\) is the perpendicular from the origin \\({\\mathbf 0}_p\\) to the line \\(\\mathcal{L}(\\boldsymbol a,\\boldsymbol b).\\) PICTURE?? For fixed \\(\\boldsymbol a\\in \\mathbb{R}^p\\) and \\(\\gamma \\in \\mathbb{R}\\), \\[ \\mathcal{H}=\\mathcal{H}(\\boldsymbol a, \\gamma) =\\{\\boldsymbol x\\in \\mathbb{R}^p:\\, \\boldsymbol a^\\top \\boldsymbol x=\\gamma\\} \\] is a hyperplane of dimension \\(p-1\\) in \\(\\mathbb{R}^p\\). The vector \\(\\boldsymbol a\\) is the perpendicular from the origin \\({\\mathbf 0}_p\\) to the hyperplane \\(\\mathcal{H}(\\boldsymbol a, \\gamma)\\). I DON&quot;T KNOW WHY THIS IS HERE? THINK ABOUT There is an alternative way to define hyperplanes in \\(\\mathbb{R}^P\\). Suppose that, for \\(1 \\leq r &lt;p\\), \\(\\stackrel{p \\times 1}{\\boldsymbol a}_1, \\ldots , \\stackrel{p \\times 1}{\\boldsymbol a}_r, \\stackrel{p \\times 1}{\\boldsymbol a}_{r+1}\\) are linearly independent. Then \\[ \\mathcal{H}=\\left \\{ \\sum_{j=1}^{r+1} \\gamma_j \\boldsymbol a_j: \\, \\sum_{j=1}^{r+1}\\gamma_j =1 \\right \\} \\] is an \\(r\\)-dimensional hyperplane in \\(\\mathbb{R}^p\\). When \\(r=1\\), using the fact that \\(\\gamma_1+\\gamma_2=1\\), we may write \\[ \\gamma_1 \\boldsymbol a_1 + \\gamma_2 \\boldsymbol a_2=(1-\\gamma_2)\\boldsymbol a_1 + \\gamma_2 \\boldsymbol a_2 = \\boldsymbol a_1 +\\gamma_2(\\boldsymbol a_2-\\boldsymbol a_1), \\] which agrees with \\(\\boldsymbol a+\\gamma \\boldsymbol b\\) in (2.4) when \\(\\boldsymbol a= \\boldsymbol a_1\\), \\(\\boldsymbol b= \\boldsymbol a_2 -\\boldsymbol a_1\\) and \\(\\gamma=\\gamma_2\\). So we have shown that the two definitions agree in the case of a straight line. "],["3-linalg-decomp.html", "Chapter 3 Matrix decompositions", " Chapter 3 Matrix decompositions This chapter focusses on two ways to decompose a matrix into smaller parts. We can then think about which are the most important parts of the matrix, and that will be useful when we think about dimension reduction. The highlight of the chapter is the singular value decomposition (SVD), which is one of the most useful mathematical concepts from the past century, and is relied upon throughout statistics and machine learning. The SVD extends the idea of the eigen (or spectral) decomposition of symmetric square matrices to any matrix. Matrix-matrix products Eigenvalues and the spectral decomposition Introduction to the singular value decomposition SVD optimization results Low-rank approximation "],["3-1-matrix-matrix.html", "3.1 Matrix-matrix products", " 3.1 Matrix-matrix products Before we get to the SVD, we first need to recap some basic material on matrix multiplication and eigenvalues. We saw in section 2.2.2 that we can think about matrix-vector products in two ways: \\(\\boldsymbol A\\boldsymbol x\\) is rows of \\(\\boldsymbol A\\) times \\(\\boldsymbol x\\); or as a linear combination of the columns of \\(\\boldsymbol A\\). We can similarly think about matrix-matrix products in two ways. The usual way to think about the matrix product \\(\\boldsymbol A\\boldsymbol B\\) is as the rows of \\(\\boldsymbol A\\) times the columns of \\(\\boldsymbol B\\): \\[\\left[ \\begin{array}{ccc} \\cdot &amp; \\cdot &amp;\\cdot\\\\ a_{21}&amp;a_{22}&amp;a_{23}\\\\ \\cdot &amp; \\cdot &amp;\\cdot \\end{array} \\right]\\left[\\begin{array}{ccc} \\cdot &amp; b_{12} &amp;\\cdot\\\\ \\cdot&amp;b_{22}&amp;\\cdot\\\\ \\cdot &amp; b_{32} &amp;\\cdot \\end{array} \\right]\\] A better way (for this module) to think of \\(\\boldsymbol A\\boldsymbol B\\) is as the columns of \\(\\boldsymbol A\\) times the rows of \\(\\boldsymbol B\\). If we let \\(\\boldsymbol a_i\\) denote the columns of \\(\\boldsymbol A\\), and \\(\\boldsymbol b^*_i\\) the rows of \\(\\boldsymbol B\\) then \\[\\left[ \\begin{array}{ccc} | &amp; | &amp;|\\\\ \\boldsymbol a_{1}&amp;\\boldsymbol a_{2}&amp;\\boldsymbol a_{3}\\\\ | &amp; | &amp;| \\end{array} \\right]\\left[\\begin{array}{ccc} - &amp; \\boldsymbol b_{1}^* &amp;-\\\\ -&amp;\\boldsymbol b_{2}^*&amp;-\\\\ - &amp; \\boldsymbol b_{3}^* &amp;- \\end{array} \\right]=\\sum_{i=1}^3\\boldsymbol a_i \\boldsymbol b_i^*\\] i.e., \\(\\boldsymbol A\\boldsymbol B\\) is a sum of the columns of \\(\\boldsymbol A\\) times the rows of \\(\\boldsymbol B\\). Note that if \\(\\boldsymbol a\\) is a vector of length \\(n\\) and \\(\\boldsymbol b\\) is a vector of length \\(p\\) then \\(\\boldsymbol a\\boldsymbol b^\\top\\) is an \\(n\\times p\\) matrix. Example 3.1 \\[\\left( \\begin{array}{c} 1\\\\ 2\\end{array} \\right) \\left(2 \\; 3\\; 1\\right)= \\left(\\begin{array}{ccc} 2&amp;3&amp;1\\\\ 4&amp;6&amp;2 \\end{array} \\right). \\] Note that \\(\\boldsymbol a\\boldsymbol b^\\top\\) is a rank-1 matrix as its columns are all multiples of \\(\\boldsymbol a\\), or in other words, its column space is just multiples of \\(\\boldsymbol a\\). \\[\\mathcal{C}(\\boldsymbol a\\boldsymbol b^\\top) = \\{\\lambda \\boldsymbol a: \\lambda \\in \\mathbb{R}\\}.\\] We sometimes call \\(\\boldsymbol a\\boldsymbol b^\\top\\) the outer product of \\(\\boldsymbol a\\) with \\(\\boldsymbol b\\). By thinking of matrix-matrix multiplication in this way \\[\\boldsymbol A\\boldsymbol B=\\sum_{i=1}^k \\boldsymbol a_i \\boldsymbol b_i^*\\] (where \\(k\\) is the number of columns of \\(\\boldsymbol A\\) and the number of rows of \\(\\boldsymbol B\\)) we can see that the product is a sum of rank-1 matrices. We can think of rank-1 matrices as the building blocks of matrices. This chapter is about ways of decomposing matrices into their most important parts, and we will do this by thinking about the most important rank-1 building blocks. Firstly though, we need a recap on eigenvectors. "],["3-2-eigenvalues-and-eigenvectors.html", "3.2 Eigenvalues and eigenvectors", " 3.2 Eigenvalues and eigenvectors Consider the \\(n\\times n\\) matrix \\(\\boldsymbol A\\). We say that vector \\(\\boldsymbol x\\in \\mathbb{R}^n\\) is an eigenvector corresponding to eigenvalue \\(\\lambda\\) of \\(\\boldsymbol A\\) if \\[{\\mathbf A} {\\mathbf x} = \\lambda {\\mathbf x}.\\] To find the eigenvalues of a matrix, we note that if \\(\\lambda\\) is an eigenvalue, then \\((\\boldsymbol A-\\lambda \\mathbf I_n)\\boldsymbol x=\\boldsymbol 0\\), i.e., the kernel of \\(\\boldsymbol A-\\lambda \\mathbf I_n\\) has dimension at least 1, so \\(\\boldsymbol A-\\lambda \\mathbf I_n\\) is not invertible, and so we must have \\(\\text{det}({\\mathbf A}-\\lambda {\\mathbf I}_n)=0\\). Let \\(R(\\lambda )=\\text{det}({\\mathbf A}-\\lambda {\\mathbf I}_n)\\), which is an \\(n^{\\text{th}}\\) order polynomial in \\(\\lambda\\). To find the eigenvalues of \\(\\boldsymbol A\\) we find the \\(n\\) roots \\(\\lambda _1, \\dots , \\lambda _n\\) of \\(R(\\lambda )\\). We will always consider ordered eigenvalues so that \\(\\lambda _1\\geq \\dots \\geq \\lambda _n\\) Proposition 3.1 If \\(\\mathbf A\\) is symmetric (i.e. \\({\\mathbf A}^\\top ={\\mathbf A}\\)) then the eigenvalues and eigenvectors of \\(\\mathbf A\\) are real (in \\(\\mathbb{R}\\)). Proposition 3.2 If \\(\\stackrel{n\\times n}{\\mathbf A}\\) is a symmetric matrix then its determinant is the product of its eigenvalues, i.e. \\(\\text{det}({\\mathbf A})=\\lambda _1 \\dots \\lambda _n\\). Thus, \\[\\boldsymbol A\\mbox{ is invertible } \\iff \\det(\\boldsymbol A)\\not=0 \\iff \\lambda_i\\not=0 \\forall i \\iff \\boldsymbol A\\mbox{ is of full rank}\\] "],["3-3-spectraleigen-decomposition.html", "3.3 Spectral/eigen decomposition", " 3.3 Spectral/eigen decomposition The key to much of dimension reduction is finding matrix decompositions. The first decomposition we will consider is the spectral decomposition (also called an eigen-decomposition). Proposition 3.3 (Spectral decomposition). Any symmetric matrix \\(\\stackrel{n\\times n}{\\mathbf A}\\) can be written as \\[ {\\mathbf A}={\\mathbf Q} {\\mathbf\\Lambda} \\boldsymbol Q^\\top = \\sum _{i=1}^{n} \\lambda _i {\\mathbf q}_i {\\mathbf q}_i^\\top ,\\] where \\(\\stackrel{n\\times n}{\\mathbf \\Lambda}=\\text{diag}\\{ \\lambda _1, \\dots , \\lambda _n \\}\\) is a diagonal matrix consisting of the eigenvalues of \\(\\mathbf A\\) and \\(\\stackrel{n\\times n}{\\mathbf Q}\\) is an orthogonal matrix (\\(\\boldsymbol Q\\boldsymbol Q^\\top=\\boldsymbol Q^\\top \\boldsymbol Q=\\mathbf I_n\\)) whose columns are unit eigenvectors \\({\\mathbf q}_1, \\dots , {\\mathbf q}_n\\) of \\(\\mathbf A\\). Because \\(\\boldsymbol \\Lambda\\) is a diagonal matrix, we sometimes refer to the spectral decomposition as diagonalizing the matrix \\(\\boldsymbol A\\) as \\(\\boldsymbol Q^\\top\\boldsymbol A\\boldsymbol Q=\\boldsymbol \\Lambda\\) is a diagonal matrix. This will be useful at various points throughout the module. Note that it relies upon the fact that the eigenvectors of \\(\\boldsymbol A\\) can be chosen to be mutually orthogonal, and as there are \\(n\\) of them, they form an orthonormal basis for \\(\\mathbb{R}^n\\). Corollary 3.1 The rank of a symmetric matrix is equal to the number of non-zero eigenvalues (counting according to their multiplicities). Proof. If \\(r\\) is the number of non-zero eigenvalues of \\(\\boldsymbol A\\), then we have (after possibly reordering the \\(\\lambda_i\\)) \\[{\\mathbf A}= \\sum _{i=1}^{r} \\lambda _i {\\mathbf q}_i {\\mathbf q}_i^\\top. \\] Each \\({\\mathbf q}_i {\\mathbf q}_i^\\top\\) is a rank 1 matrix, with column space equal to the span of \\(\\boldsymbol q_i\\). As the \\(\\boldsymbol q_i\\) are orthogonal, the columns spaces \\(\\mathcal{C}(\\boldsymbol q_i \\boldsymbol q_i^\\top)\\) are orthogonal, and their union is a vector space of dimension \\(r\\). Hence the rank of \\(\\boldsymbol A\\) is \\(r\\). Lemma 3.1 Let \\(\\stackrel{n\\times n}{\\mathbf A}\\) be a symmetric matrix with (necessarily real) eigenvalues \\(\\lambda _1 \\geq \\lambda _2 \\geq \\dots \\geq \\lambda _n\\). Then \\(\\mathbf A\\) is positive definite if and only if \\(\\lambda _n &gt;0\\). It is positive semi-definite if and only if \\(\\lambda_n \\geq 0\\). Proof. If \\(\\boldsymbol A\\) is positive definite, and if \\(\\boldsymbol x\\) is a unit-eigenvalue of \\(\\boldsymbol A\\) corresponding to \\(\\lambda_n\\), then \\[0\\leq \\boldsymbol x^\\top \\boldsymbol A\\boldsymbol x= \\lambda_n \\boldsymbol x^\\top \\boldsymbol x= \\lambda_n.\\] Conversely, suppose \\(\\boldsymbol A\\) has positive eigenvalues. Because \\(\\boldsymbol A\\) is real and symmetric, we can write it as \\(\\boldsymbol A=\\boldsymbol Q{\\mathbf\\Lambda} \\boldsymbol Q^\\top\\). Now if \\(\\boldsymbol x\\) is a non-zero vector, then \\(\\boldsymbol y= \\boldsymbol Q^\\top \\boldsymbol x\\not= \\boldsymbol 0\\), (as \\(\\boldsymbol Q^\\top\\) has inverse \\(\\boldsymbol Q\\) and hence \\(\\dim \\operatorname{Ker}(\\boldsymbol Q)=0\\)). Thus \\[\\boldsymbol x^\\top \\boldsymbol A\\boldsymbol x= \\boldsymbol y^\\top {\\mathbf\\Lambda}\\boldsymbol y= \\sum_{i=1}^n \\lambda_i y_i^2 &gt;0\\] and thus \\(\\boldsymbol A\\) is positive definite. Note: A covariance matrix \\(\\boldsymbol \\Sigma\\) is always positive semi- definite (and thus always has non-negative eigenvalues). To see this, recall that if \\(\\boldsymbol x\\) is a random vector with \\({\\mathbb{V}\\operatorname{ar}}(\\boldsymbol x)=\\boldsymbol \\Sigma\\), then for any constant vector \\(\\boldsymbol a\\), the random variable \\(\\boldsymbol a^\\top \\boldsymbol x\\) has variance \\({\\mathbb{V}\\operatorname{ar}}(\\boldsymbol a^\\top \\boldsymbol x)=\\boldsymbol a^\\top \\boldsymbol \\Sigma\\boldsymbol a\\). Because variances are positive, we must have \\[\\boldsymbol a^\\top \\boldsymbol \\Sigma\\boldsymbol a\\geq 0 \\;\\forall \\;\\boldsymbol a.\\] Moreover, if \\(\\boldsymbol \\Sigma\\) is positive definite (so that its eigenvalues are positive), then its determinant will be positive (so that \\(\\boldsymbol \\Sigma\\) is non-singular) and we can find an inverse \\(\\boldsymbol \\Sigma^{-1}\\) matrix, which is called the precision matrix. Proposition 3.4 The eigenvalues of a projection matrix \\(\\boldsymbol P\\) are all \\(0\\) or \\(1\\). 3.3.1 Matrix square roots From the spectral decomposition theorem, we can see that if \\(\\boldsymbol A\\) is a symmetric positive semi-definite matrix, then for any integer \\(p\\) \\[\\boldsymbol A^p = \\boldsymbol Q{\\mathbf\\Lambda}^p \\boldsymbol Q^\\top.\\] If in addition \\(\\boldsymbol A\\) is positive definite (rather than just semi-definite), then \\[\\boldsymbol A^{-1} = \\boldsymbol Q{\\mathbf\\Lambda}^{-1} \\boldsymbol Q^\\top\\] where \\({\\mathbf\\Lambda}^{-1} = \\text{diag}\\{ \\frac{1}{\\lambda _1}, \\dots , \\frac{1}{\\lambda _n} \\}\\). The spectral decomposition also gives us a way to define a matrix square root. If we assume \\(\\boldsymbol A\\) is positive semi-definite, then its eigenvalues are non-negative, and the diagonal elements of \\(\\mathbf \\Lambda\\) are all non-negative. We then define \\({\\boldsymbol A}^{1/2}\\), a matrix square root of \\(\\boldsymbol A\\), to be \\({\\boldsymbol A}^{1/2}=\\boldsymbol Q\\boldsymbol \\Lambda^{1/2} \\boldsymbol Q^\\top\\) where \\(\\boldsymbol \\Lambda^{1/2}=\\text{diag}\\{\\lambda_1^{1/2}, \\ldots , \\lambda_n^{1/2}\\}\\). This definition makes sense because \\[\\begin{align*} \\boldsymbol A^{1/2}\\boldsymbol A^{1/2}&amp;=\\boldsymbol Q\\boldsymbol \\Lambda^{1/2}\\boldsymbol Q^\\top \\boldsymbol Q\\boldsymbol \\Lambda^{1/2} \\boldsymbol Q^\\top\\\\ &amp;=\\boldsymbol Q\\boldsymbol \\Lambda^{1/2}\\boldsymbol \\Lambda^{1/2}\\boldsymbol Q^\\top\\\\ &amp;=\\boldsymbol Q\\boldsymbol \\Lambda\\boldsymbol Q^\\top\\\\ &amp;=\\boldsymbol A, \\end{align*}\\] where \\(\\boldsymbol Q^\\top \\boldsymbol Q=\\mathbf I_n\\) and \\(\\boldsymbol \\Lambda^{1/2}\\boldsymbol \\Lambda^{1/2}=\\boldsymbol \\Lambda\\). The matrix \\(\\boldsymbol A^{1/2}\\) is not the only matrix square root of \\(\\boldsymbol A\\), but it is the only symmetric, positive semi-definite square root of \\(\\boldsymbol A\\). If \\(\\boldsymbol A\\) is positive definite (as opposed to just positive semi-definite), then all the \\(\\lambda_i\\) are positive and so we can also define \\(\\boldsymbol A^{-1/2}=\\boldsymbol Q\\boldsymbol \\Lambda^{-1/2}\\boldsymbol Q^\\top\\) where \\(\\boldsymbol \\Lambda^{-1/2}=\\text{diag}\\{\\lambda_1^{-1/2},\\ldots , \\lambda_n^{-1/2}\\}\\). Note that \\[ \\boldsymbol A^{-1/2}\\boldsymbol A^{-1/2}=\\boldsymbol Q\\boldsymbol \\Lambda^{-1/2}\\boldsymbol Q^\\top \\boldsymbol Q\\boldsymbol \\Lambda^{-1/2}\\boldsymbol Q^\\top =\\boldsymbol Q\\boldsymbol \\Lambda^{-1}\\boldsymbol Q^\\top=\\boldsymbol A^{-1}, \\] so that, as defined above, \\(\\boldsymbol A^{-1/2}\\) is the matrix square root of \\(\\boldsymbol A^{-1}\\). Furthermore, similar calculations show that \\[ \\boldsymbol A^{1/2}\\boldsymbol A^{-1/2}=\\boldsymbol A^{-1/2}\\boldsymbol A^{1/2}=\\mathbf I_n, \\] so that \\(\\boldsymbol A^{-1/2}\\) is the matrix inverse of \\(\\boldsymbol A^{1/2}\\). "],["3-4-linalg-SVD.html", "3.4 Singular Value Decomposition (SVD)", " 3.4 Singular Value Decomposition (SVD) The spectral decomposition theorem (Proposition 3.3) gives a decomposition of any symmetric matrix. We now give a generalisation of this result which applies to all matrices. If matrix \\(\\boldsymbol A\\) is not a square matrix, then it cannot have eigenvectors. Instead, it has singular vectors corresponding to singular values. Suppose \\(\\bf\\) is a \\(n\\times p\\) matrix. Then we say \\(\\sigma\\) is a singular value with corresponding left and right singular vectors \\(\\boldsymbol u\\) and \\(\\boldsymbol v\\) (respectively) if \\[\\boldsymbol A\\boldsymbol v= \\sigma \\boldsymbol u\\quad \\mbox{ and }\\quad \\boldsymbol A^\\top \\boldsymbol u= \\sigma \\boldsymbol v\\] If \\(\\boldsymbol A\\) if a symmetric matrix then \\(\\boldsymbol u=\\boldsymbol v\\) is a eigenvector and \\(\\sigma\\) is an eigenvalue. The singular value decomposition (SVD) diagonalizes \\(\\boldsymbol A\\) into a product of a matrix of left singular vectors \\(\\boldsymbol U\\), a diagonal matrix of singular values \\(\\boldsymbol \\Sigma\\), and a matrix of right singular vectors \\(\\boldsymbol V\\). \\[\\boldsymbol A= \\boldsymbol U\\boldsymbol \\Sigma\\boldsymbol V^\\top.\\] Proposition 3.5 (Singular value decomposition). Let \\(\\boldsymbol A\\) be a \\(n \\times p\\) matrix of rank \\(r\\), where \\(1 \\leq r \\leq \\min(n,p)\\). Then there exists a \\(n \\times r\\) matrix \\(\\boldsymbol U=[\\boldsymbol u_1,\\ldots , \\boldsymbol u_r]\\), a \\(p \\times r\\) matrix \\(\\boldsymbol V=[{\\mathbf v}_1,\\ldots ,{ \\mathbf v}_r],\\) and a \\(r \\times r\\) diagonal matrix \\(\\boldsymbol \\Sigma=\\text{diag}\\{\\sigma_1,\\ldots , \\sigma_r\\}\\) such that \\[ \\boldsymbol A=\\boldsymbol U{\\mathbf \\Sigma} \\boldsymbol V^\\top =\\sum_{i=1}^r \\sigma_i \\boldsymbol u_i {\\mathbf v}_i^\\top, \\] where \\(\\boldsymbol U^\\top \\boldsymbol U= \\mathbf I_r = \\boldsymbol V^\\top \\boldsymbol V\\) and the \\(\\sigma_1 \\geq \\ldots \\geq \\sigma_r &gt;0\\). Note that the \\(\\boldsymbol u_i\\) and the \\({\\mathbf v}_i\\) are necessarily unit vectors, and that we have ordered the singular values from largest to smallest. The scalars \\(\\sigma_1, \\ldots , \\sigma_r\\) are called the singular values of \\(\\boldsymbol A\\), the columns of \\(\\boldsymbol U\\) are the left singular vectors, and the columns of \\(\\boldsymbol V\\) are the right singular vectors. The form of the SVD given above is called the compact singular value decomposition. Sometimes we write it in a non-compact form \\[\\boldsymbol A= \\boldsymbol U\\boldsymbol \\Sigma\\boldsymbol V^\\top\\] where \\(\\boldsymbol U\\) is a \\(n \\times n\\) orthogonal matrix \\((\\boldsymbol U^\\top\\boldsymbol U=\\boldsymbol U\\boldsymbol U^\\top = \\mathbf I_n)\\), \\(\\boldsymbol V\\) is a \\(p \\times p\\) orthogonal matrix \\((\\boldsymbol V^\\top\\boldsymbol V=\\boldsymbol V\\boldsymbol V^\\top = \\mathbf I_p)\\), and \\(\\boldsymbol \\Sigma\\) is a \\(n \\times p\\) diagonal matrix \\[\\begin{equation} \\boldsymbol \\Sigma= \\left(\\begin{array}{cccccccc} \\sigma_1&amp;0&amp;\\ldots&amp;&amp;&amp;&amp;0\\\\ 0&amp;\\sigma_2&amp;0&amp;\\ldots&amp;&amp;&amp;\\\\ \\vdots\\\\ 0&amp;0&amp;&amp;\\ldots&amp;\\sigma_r&amp;&amp;\\\\ 0&amp;0&amp;&amp;\\ldots&amp;&amp;0&amp;\\ldots\\\\ \\vdots\\\\ 0&amp;0&amp;&amp;\\ldots&amp;&amp;&amp;0\\\\ \\end{array} \\right). \\tag{3.1} \\end{equation}\\] The columns of \\(\\boldsymbol U\\) and \\(\\boldsymbol V\\) form an orthonormal basis for \\(\\mathbb{R}^n\\) and \\(\\mathbb{R}^p\\) respectively. We can see that we recover the compact form of the SVD by only using the first \\(r\\) columns of \\(\\boldsymbol U\\) and \\(\\boldsymbol V\\), and truncating \\(\\boldsymbol \\Sigma\\) to a \\(r\\times r\\) matrix with non-zero diagonal elements. When \\(\\boldsymbol A\\) is symmetric, we take \\({\\mathbf U}=\\boldsymbol V\\), and the spectral decomposition theorem is recovered, and in this case (but not in general) the singular values of \\(\\boldsymbol A\\) are eigenvalues of \\(\\boldsymbol A\\). Proof. \\(\\boldsymbol A^\\top \\boldsymbol A\\) is a \\(p\\times p\\) symmetric matrix, and so by the spectral decomposition theorem we can write it as \\[\\boldsymbol A^\\top \\boldsymbol A= \\boldsymbol V\\boldsymbol \\Lambda\\boldsymbol V^\\top\\] where \\(\\boldsymbol V\\) is a \\(p \\times p\\) orthogonal matrix containing the orthonormal eigenvectors of \\(\\boldsymbol A^\\top \\boldsymbol A\\), and \\(\\boldsymbol \\Lambda=\\operatorname{diag}(\\lambda_1, \\ldots, \\lambda_r)\\) is a diagonal matrix of eigenvalues with \\(\\lambda_1\\geq \\ldots \\geq\\lambda_r&gt;0\\) (by Corollary 3.1). For \\(i=1,\\dots, r\\), let \\(\\sigma_i =\\sqrt{\\lambda_i}\\) and let \\(\\boldsymbol u_i = \\frac{1}{\\sigma_i} \\boldsymbol A\\boldsymbol v_i\\). Then the vectors \\(\\boldsymbol u_i\\) are orthonormal: \\[\\begin{align*} \\boldsymbol u_i^\\top \\boldsymbol u_j &amp;=\\frac{1}{\\sigma_i\\sigma_j} \\boldsymbol v_i^\\top \\boldsymbol A^\\top\\boldsymbol A\\boldsymbol v_j\\\\ &amp;=\\frac{\\sigma_j^2}{\\sigma_i\\sigma_j} \\boldsymbol v_i^\\top\\boldsymbol v_j \\quad \\mbox{ as }\\boldsymbol v_j \\mbox{ is an eigenvector of } \\boldsymbol A^\\top\\boldsymbol A\\\\ &amp;=\\begin{cases} 0 &amp;\\mbox{ if } i\\not=j\\\\ 1 &amp;\\mbox{ if } i=j \\end{cases}\\quad \\mbox{ as the } \\boldsymbol v_i \\mbox{ are orthonormal vectors.} \\end{align*}\\] In addition \\[\\boldsymbol A^\\top\\boldsymbol u_i = \\frac{1}{\\sigma_i}\\boldsymbol A^\\top\\boldsymbol A\\boldsymbol v_i = \\frac{\\sigma^2_i}{\\sigma_i}\\boldsymbol v_i = \\sigma_i\\boldsymbol v_i\\] and so \\(\\boldsymbol u_i\\) and \\(\\boldsymbol v_i\\) are left and right singular vectors. Let \\(\\boldsymbol U=[\\boldsymbol u_1 \\; \\boldsymbol u_2 \\; \\ldots \\; \\boldsymbol u_r\\; \\ldots \\; \\boldsymbol u_n]\\), where \\(\\boldsymbol u_{r+1}, \\ldots, \\boldsymbol u_n\\) are chosen to complete the orthonormal basis for \\(\\mathbb{R}^n\\) given \\(\\boldsymbol u_1, \\ldots, \\boldsymbol u_r\\), and let \\(\\Sigma\\) be the \\(n\\times p\\) diagonal matrix in Equation (3.1). Then we have shown that \\[\\boldsymbol U= \\boldsymbol A\\boldsymbol V\\boldsymbol \\Sigma^{-1}\\] Thus \\[\\begin{align*} \\boldsymbol U&amp;= \\boldsymbol A\\boldsymbol V\\boldsymbol \\Sigma^{-1}\\\\ \\boldsymbol U\\boldsymbol \\Sigma&amp;= \\boldsymbol A\\boldsymbol V\\\\ \\boldsymbol U\\boldsymbol \\Sigma\\boldsymbol V^\\top &amp;= \\boldsymbol A. \\end{align*}\\] Note that by construction we’ve shown that \\(\\boldsymbol A^\\top\\boldsymbol A\\) has eigenvalues \\(\\sigma^2_i\\) with corresponding eigenvectors \\(\\boldsymbol v_i\\). We also can also show that \\(\\boldsymbol A\\boldsymbol A^\\top\\) has eigenvalues \\(\\sigma^2_i\\), but with corresponding eigenvectors \\(\\boldsymbol u_i\\). \\[\\boldsymbol A\\boldsymbol A^\\top \\boldsymbol u_i = \\sigma_i\\boldsymbol A\\boldsymbol v_i = \\sigma^2_i \\boldsymbol u_i\\] Proposition 3.6 Let \\(\\boldsymbol A\\) be any matrix of rank \\(r\\). Then the non-zero eigenvalues of both \\(\\boldsymbol A\\boldsymbol A^\\top\\) and \\(\\boldsymbol A^\\top \\boldsymbol A\\) are \\(\\sigma_1^2, \\ldots , \\sigma_r^2\\). The corresponding unit eigenvectors of \\(\\boldsymbol A\\boldsymbol A^\\top\\) are given by the columns of \\(\\boldsymbol U\\), and the corresponding unit eigenvectors of \\(\\boldsymbol A^\\top \\boldsymbol A\\) are given by the columns of \\(\\mathbf V\\). Notes: The SVD expresses a matrix as a sum of rank-1 matrices \\[\\boldsymbol A= \\sum_{i=1}^r \\sigma_i \\boldsymbol u_i \\boldsymbol v_i^\\top.\\] We can think of these as a list of the building blocks of \\(\\boldsymbol A\\) ordered by their importance (\\(\\sigma_1\\geq \\sigma_2\\geq\\ldots\\)). The singular value decomposition theorem shows that every matrix is diagonal, provided one uses the proper bases for the domain and range spaces. We can diagonalize \\(\\boldsymbol A\\) by \\[ \\boldsymbol U^\\top\\boldsymbol A\\boldsymbol V=\\boldsymbol \\Sigma.\\] The SVD reveals a great deal about a matrix. Firstly, the rank of \\(\\boldsymbol A\\) is the number of non-zero singular values. The left singular vectors \\(\\boldsymbol u_1, \\ldots, \\boldsymbol u_r\\) are an orthonormal basis for the columns space of \\(\\boldsymbol A\\), \\(\\mathcal{C}(\\boldsymbol A)\\), and the right singular vectors \\(\\boldsymbol v_1, \\ldots, \\boldsymbol v_r\\) are an orthonormal basis for \\(\\mathcal{C}(\\boldsymbol A^\\top)\\), the row space of \\(\\boldsymbol A\\). The vectors \\(\\boldsymbol v_{r+1}, \\ldots, \\boldsymbol v_p\\) from the non-compact SVD are a basis for the kernel of \\(\\boldsymbol A\\) (sometimes called the null space \\(\\mathcal{N}(\\boldsymbol A)\\)), and \\(\\boldsymbol u_{r+1}, \\ldots, \\boldsymbol u_n\\) are a basis for \\(\\mathcal{N}(\\boldsymbol A^\\top)\\). The SVD has many uses in mathematics. One is as a generalized inverse of a matrix. If \\(\\boldsymbol A\\) is \\(n \\times p\\) with \\(n\\not = p\\), or if it is square but not of full rank, then \\(\\boldsymbol A\\) cannot have an inverse. However, we say \\(\\boldsymbol A^+\\) is a generalized inverse if \\(\\boldsymbol A\\boldsymbol A^+\\boldsymbol A=\\boldsymbol A\\). One such generalized inverse can be obtained from the SVD by \\(\\boldsymbol A^+ = \\boldsymbol V\\boldsymbol \\Sigma^{-1}\\boldsymbol U^\\top\\) - this is known as the Moore-Penrose pseudo-inverse. 3.4.1 Examples In practice, we don’t compute SVDs of a matrix by hand: in R you can use the command SVD(A) to compute the SVD of matrix A. However, it is informative to do the calculation yourself a few times to help fix the ideas. Example 3.2 Consider the matrix \\(\\boldsymbol A= \\boldsymbol x\\boldsymbol y^\\top\\). We can see this is a rank-1 matrix, so it only has one non-zero singular value which is \\(\\sigma_1 = ||\\boldsymbol x||.||\\boldsymbol y||\\). Its SVD is given by \\[\\boldsymbol U= \\frac{1}{||\\boldsymbol x|| }\\boldsymbol x,\\quad \\boldsymbol V= \\frac{1}{||\\boldsymbol y|| }\\boldsymbol y, \\quad \\mbox{ and } \\Sigma = ||\\boldsymbol x||.||\\boldsymbol y||.\\] Example 3.3 Let \\[\\boldsymbol A= \\left(\\begin{array}{ccc}3&amp;2&amp;2\\\\ 2&amp;3&amp;-2\\end{array}\\right).\\] Let’s try to find the SVD of \\(\\boldsymbol A\\). We know the singular values are the square roots of the eigenvalues of \\(\\boldsymbol A\\boldsymbol A^\\top\\) and \\(\\boldsymbol A^\\top\\boldsymbol A\\). We’ll work with the former as it is only \\(2\\times 2\\). \\[\\boldsymbol A\\boldsymbol A^\\top = \\left(\\begin{array}{cc}17&amp;8\\\\ 8&amp;17\\end{array}\\right) \\quad \\mbox{ and so } \\det(\\boldsymbol A\\boldsymbol A^\\top-\\lambda \\mathbf I)=(17-\\lambda)^2-64 \\] Solving \\(\\det(\\boldsymbol A\\boldsymbol A^\\top-\\lambda \\mathbf I)=0\\) gives the eigenvalues to be \\(\\lambda=25\\) or \\(9\\). Thus the singular values of \\(\\boldsymbol A\\) are \\(\\sigma_1=5\\) and \\(\\sigma_2=3\\), and \\[\\boldsymbol \\Sigma=\\left(\\begin{array}{cc}5&amp;0\\\\ 0&amp;3\\end{array}\\right).\\] The columns of \\(\\boldsymbol U\\) are the unit eigenvectors of \\(\\boldsymbol A\\boldsymbol A^\\top\\) which we can find by solving \\[\\begin{align*}(\\boldsymbol A-25\\mathbf I_2)\\boldsymbol u&amp;=\\left(\\begin{array}{cc}-8&amp;8\\\\ 8&amp;-8\\end{array}\\right)\\left(\\begin{array}{c}u_1\\\\u_2\\\\\\end{array}\\right)=\\left(\\begin{array}{c}0\\\\0\\\\\\end{array}\\right) \\quad \\mbox{ and }\\\\ (\\boldsymbol A-9\\mathbf I_2)\\boldsymbol u&amp;=\\left(\\begin{array}{cc}8&amp;8\\\\ 8&amp;8\\end{array}\\right)\\left(\\begin{array}{c}u_1\\\\u_2\\\\\\end{array}\\right)=\\left(\\begin{array}{c}0\\\\0\\\\\\end{array}\\right).\\end{align*}\\] And so, remembering that the eigenvectors used to form \\(\\boldsymbol V\\) need to be unit vectors, we can see that \\[\\boldsymbol U=\\frac{1}{\\sqrt{2}}\\left(\\begin{array}{cc}1&amp;1\\\\ 1&amp;-1\\end{array}\\right).\\] Finally, to compute \\(\\boldsymbol V\\) recall that \\(\\sigma_i \\boldsymbol v_i = \\boldsymbol A^\\top \\boldsymbol u_i\\) and so \\[\\boldsymbol V= \\boldsymbol A^\\top\\boldsymbol U\\boldsymbol \\Sigma^{-1} = \\frac{1}{\\sqrt{2}}\\left(\\begin{array}{cc}1&amp;\\frac{1}{3}\\\\ 1&amp;\\frac{-1}{3}\\\\ 0&amp;\\frac{4}{3}\\end{array}\\right). \\] This completes the calculation, and we can see that we can express \\(\\boldsymbol A\\) as \\[\\boldsymbol A= \\left(\\begin{array}{cc}\\frac{1}{\\sqrt{2}}&amp;\\frac{1}{\\sqrt{2}}\\\\ \\frac{1}{\\sqrt{2}}&amp;\\frac{-1}{\\sqrt{2}}\\end{array}\\right) \\left(\\begin{array}{cc}5&amp;0\\\\ 0&amp;3\\end{array}\\right)\\left(\\begin{array}{ccc}\\frac{1}{\\sqrt{2}}&amp;\\frac{1}{\\sqrt{2}}&amp;0\\\\ \\frac{1}{3\\sqrt{2}}&amp; \\frac{-1}{3 \\sqrt{2}} &amp;\\frac{4}{3\\sqrt{2}}\\end{array}\\right)\\] or as the sum of rank-1 matrices: \\[\\boldsymbol A= 5\\left(\\begin{array}{c}\\frac{1}{\\sqrt{2}}\\\\ \\frac{1}{\\sqrt{2}}\\end{array}\\right) \\left(\\begin{array}{cc}\\frac{1}{\\sqrt{2}}&amp;\\frac{1}{3\\sqrt{2}} \\end{array}\\right)+ 3\\left(\\begin{array}{c}\\frac{1}{\\sqrt{2}}\\\\ \\frac{-1}{\\sqrt{2}}\\end{array}\\right) \\left(\\begin{array}{ccc}\\frac{1}{3\\sqrt{2}}&amp; \\frac{-1}{3 \\sqrt{2}} &amp;\\frac{4}{3\\sqrt{2}}\\end{array}\\right)\\] This is the compact form of the SVD. To find the non-compact form we need \\(\\boldsymbol V\\) to be a \\(3 \\times 3\\) matrix, which requires us to find a 3rd column that is orthogonal to the first two columns (thus completing an orthonormal basis for \\(\\mathbb{R}^3\\)). We can do that with the vector \\(\\boldsymbol v_3 = \\frac{1}{\\sqrt{17}}(2\\; -2\\; -3)\\) giving the non-compact SVD for \\(\\boldsymbol A\\). \\[\\boldsymbol A= \\left(\\begin{array}{cc}\\frac{1}{\\sqrt{2}}&amp;\\frac{1}{\\sqrt{2}}\\\\ \\frac{1}{\\sqrt{2}}&amp;\\frac{-1}{\\sqrt{2}}\\end{array}\\right) \\left(\\begin{array}{ccc}5&amp;0&amp;0\\\\ 0&amp;3&amp;0\\end{array}\\right)\\left(\\begin{array}{ccc}\\frac{1}{\\sqrt{2}}&amp; \\frac{1}{3\\sqrt{2}}&amp;\\frac{2}{\\sqrt{17}} \\\\ \\frac{1}{\\sqrt{2}}&amp; \\frac{-1}{3 \\sqrt{2}} &amp;\\frac{-2}{\\sqrt{17}}\\\\ 0&amp;\\frac{4}{3\\sqrt{2}}&amp;\\frac{-3}{\\sqrt{17}}\\end{array}\\right)^\\top\\] Let’s check our answer in R. A&lt;- matrix(c(3,2,2,2,3,-2), nr=2, byrow=T) svd(A) ## $d ## [1] 5 3 ## ## $u ## [,1] [,2] ## [1,] -0.7071068 -0.7071068 ## [2,] -0.7071068 0.7071068 ## ## $v ## [,1] [,2] ## [1,] -7.071068e-01 -0.2357023 ## [2,] -7.071068e-01 0.2357023 ## [3,] -5.551115e-17 -0.9428090 The eigenvectors are only defined upto multiplication by \\(-1\\) and so we can multiply any pair of left and right singular vectors by \\(-1\\) and it is still a valid SVD. Note: In practice this is a terrible way to compute the SVD as it is prone to numerical error. In practice an efficient iterative method is used in most software implementations (including R). "],["3-5-optimization-results.html", "3.5 Optimization results", " 3.5 Optimization results Why are eigenvalues and singular values useful in statistics? It is because they appear as the result of some important optimization problems. We’ll see more about this in later chapters, but we’ll prove a few preliminary results here. For example, suppose \\(\\boldsymbol x\\in\\mathbb{R}^n\\) is a random variable with \\({\\mathbb{C}\\operatorname{ov}}(\\boldsymbol x)=\\boldsymbol \\Sigma\\) (an \\(n \\times n\\) matrix), then can we find a projection of \\(\\boldsymbol x\\) that has either maximum or minimum variance? I.e., can we find \\(\\boldsymbol a\\) such that \\[{\\mathbb{V}\\operatorname{ar}}(\\boldsymbol a^\\top\\boldsymbol x)=\\boldsymbol a^\\top \\boldsymbol \\Sigma\\boldsymbol a\\] is maximized or minimized? To make the question interesting we need to constrain the length of \\(\\boldsymbol a\\) so lets assume that \\(||\\boldsymbol a||_2 = \\sqrt{\\boldsymbol a^\\top \\boldsymbol a}=1\\), otherwise we could just take \\(\\boldsymbol a=\\boldsymbol 0\\) to obtain a projection with variance zero. So we want to solve the optimization problems involving the quadratic form \\(\\boldsymbol a^\\top \\boldsymbol \\Sigma\\boldsymbol a\\): \\[\\begin{equation} \\max_{\\boldsymbol a: \\;\\boldsymbol a^\\top \\boldsymbol a=1}{\\mathbf a}^\\top {\\mathbf \\Sigma} {\\mathbf a}, \\quad \\mbox{and}\\quad \\min_{\\boldsymbol a: \\;\\boldsymbol a^\\top \\boldsymbol a=1}{\\mathbf a}^\\top {\\mathbf \\Sigma} {\\mathbf a}. \\tag{3.2} \\end{equation}\\] Given that \\(\\boldsymbol \\Sigma\\) is symmetric, we can write it as \\[\\boldsymbol \\Sigma= \\boldsymbol V\\boldsymbol \\Lambda\\boldsymbol V^\\top \\] where \\(\\boldsymbol \\Lambda\\) is the diagonal matrix of eigenvalues of \\(\\boldsymbol \\Sigma\\), and \\(\\boldsymbol V\\) is an orthogonal matrix of eigenvectors. If we let \\(\\boldsymbol b=\\boldsymbol V^\\top \\boldsymbol a\\) then \\[\\boldsymbol a^\\top \\boldsymbol \\Sigma\\boldsymbol a= \\boldsymbol b^\\top \\boldsymbol \\Lambda\\boldsymbol b= \\sum_{i=1}^n \\lambda_i b_i^2\\] and given that the eigenvalues are ordered \\(\\lambda_1\\geq \\lambda_2 \\geq \\ldots\\) and that \\[\\sum_{i=1}^n b_i^2=\\boldsymbol b^\\top\\boldsymbol b=\\boldsymbol a^\\top \\boldsymbol V\\boldsymbol V^\\top\\boldsymbol a=\\boldsymbol a^\\top\\boldsymbol a=1,\\] we can see that the maximumn is \\(\\lambda_1\\) obtained by setting \\(\\boldsymbol b=(1\\;0\\;0 \\ldots)^\\top\\). Then \\[\\begin{align*} \\boldsymbol V^\\top \\boldsymbol a&amp;= \\boldsymbol b\\\\ \\boldsymbol V\\boldsymbol V^\\top \\boldsymbol a&amp;=\\boldsymbol V\\boldsymbol b\\\\ \\boldsymbol a&amp;= \\boldsymbol v_1 \\end{align*}\\] so we can see that the maximum is obtained when \\(\\boldsymbol a=\\boldsymbol v_1\\), the eigenvector of \\(\\boldsymbol \\Sigma\\) corresponding to the largest eigenvalue \\(\\lambda_1\\). Similarly, the minimum is \\(\\lambda_n\\), which obtained by setting \\(\\boldsymbol b=(\\;0\\;0 \\ldots\\;0\\;1)^\\top\\) which corresponds to \\(\\boldsymbol a=\\boldsymbol v_n\\). Proposition 3.7 For any symmetric \\(n \\times n\\) matrix \\(\\boldsymbol \\Sigma\\), \\[\\max_{\\boldsymbol a: \\boldsymbol a^\\top \\boldsymbol a=1} \\boldsymbol a^\\top\\boldsymbol \\Sigma\\boldsymbol a=\\lambda_1,\\] where the maximum occurs at \\(\\boldsymbol a=\\pm \\boldsymbol v_1\\), and \\[\\min_{\\boldsymbol a: \\boldsymbol a^\\top \\boldsymbol a=1} \\boldsymbol a^\\top\\boldsymbol \\Sigma\\boldsymbol a=\\lambda_n\\] where the minimum occurs at \\(\\boldsymbol a= \\pm \\boldsymbol v_n\\), where \\(\\lambda_i, \\boldsymbol v_i\\) are the ordered eigenpairs of \\(\\boldsymbol \\Sigma\\). Note that \\[\\frac{\\boldsymbol a^\\top \\boldsymbol \\Sigma\\boldsymbol a}{\\boldsymbol a^\\top\\boldsymbol a}=\\frac{\\boldsymbol a^\\top \\boldsymbol \\Sigma\\boldsymbol a}{||\\boldsymbol a||^\\top} = (\\frac{\\boldsymbol a}{||\\boldsymbol a||})^\\top \\boldsymbol \\Sigma(\\frac{\\boldsymbol a}{||\\boldsymbol a||})\\] and so another way to write the maximization problems (3.2) is as unconstrained optimization problems: \\[\\max_{\\boldsymbol a}\\frac{\\boldsymbol a^\\top \\boldsymbol \\Sigma\\boldsymbol a}{\\boldsymbol a^\\top\\boldsymbol a}\\quad \\mbox{ and } \\quad \\min_{\\boldsymbol a}\\frac{\\boldsymbol a^\\top \\boldsymbol \\Sigma\\boldsymbol a}{\\boldsymbol a^\\top\\boldsymbol a}.\\] We obtain a similar result for non-square matrices using the singular value decomposition. Proposition 3.8 For any matrix \\(\\boldsymbol A\\) \\[\\max_{\\boldsymbol x: ||\\boldsymbol x||_2=1}||\\boldsymbol A\\boldsymbol x||_2=\\max_{\\boldsymbol x}\\frac{||\\boldsymbol A\\boldsymbol x||_2}{||\\boldsymbol x||_2}=\\sigma_1\\] the first singular value of \\(\\boldsymbol A\\), with the maximum achieved at \\(\\boldsymbol x=\\boldsymbol v_1\\) (the first right singular vector). Proof. This is follows from 3.7 as \\[||\\boldsymbol A\\boldsymbol x||_2^2=\\boldsymbol x^\\top \\boldsymbol A^\\top\\boldsymbol A\\boldsymbol x.\\] Finally, we will need the following result when we study canonical correlation analysis: Proposition 3.9 For any matrix \\(\\boldsymbol A\\), we have \\[ \\max_{\\boldsymbol a, \\boldsymbol b:\\, \\vert \\vert \\boldsymbol a\\vert \\vert=\\vert \\vert \\boldsymbol b\\vert \\vert =1} \\boldsymbol a^\\top \\boldsymbol A\\boldsymbol b=\\sigma_1. \\] with the maximum obtained at \\(\\boldsymbol a=\\boldsymbol u_1\\) and \\(\\boldsymbol b=\\boldsymbol v_1\\), the first left and right singular vectors of \\(\\boldsymbol A\\). Proof. We’ll see much more of this kind of thing in Chapters 4 and 5. "],["3-6-best-approximating-matrices.html", "3.6 Best approximating matrices", " 3.6 Best approximating matrices One of the reasons the SVD is so widely used is that it can be used to find the best low rank approximation to a matrix. Before we discuss this, we need to define what it means for some matrix \\(\\boldsymbol B\\) to be a good approximation to \\(\\boldsymbol A\\). To do that, we need the concept of a matrix norm. 3.6.1 Matrix norms In Section 2.3.1 we described norms on vectors. Here will extend this idea to include norms on matrices, so that we can discuss the size of a matrix \\(||\\boldsymbol A||\\), and the distance between two matrices \\(||\\boldsymbol A-\\boldsymbol B||\\). There are two particular norms we will focus on. The first is called the Frobenious norm (or sometimes the Hilbert-Schmidt norm). Definition 3.1 Let \\(\\boldsymbol A\\in \\mathbb{R}^{n\\times p}\\). The Frobenius norm of \\(\\boldsymbol A\\) is \\[||\\boldsymbol A||_F = \\left(\\sum_{i=1}^n\\sum_{j=1}^p |a_{ij}|^2\\right)^{\\frac{1}{2}}=(\\operatorname{tr}\\boldsymbol A^\\top\\boldsymbol A)^{\\frac{1}{2}} \\] where \\(a_{ij}\\) are the individual entries of \\(\\boldsymbol A\\). Note that the Frobenius norm is invariant to rotation by an orthogonal matrix \\(\\boldsymbol U\\): \\[\\begin{align*} ||\\boldsymbol A\\boldsymbol U||_F^2 &amp;= \\operatorname{tr}(\\boldsymbol U^\\top \\boldsymbol A^\\top \\boldsymbol A\\boldsymbol U)\\\\ &amp;=\\operatorname{tr}(\\boldsymbol U\\boldsymbol U^\\top \\boldsymbol A^\\top \\boldsymbol A)\\\\ &amp;= \\operatorname{tr}(\\boldsymbol A^\\top\\boldsymbol A)\\\\ &amp;= ||\\boldsymbol A||_F^2. \\end{align*}\\] Proposition 3.10 \\[||\\boldsymbol A||_F = \\left(\\sum_{i=1}^r \\sigma_i^2\\right)^{\\frac{1}{2}}\\] where \\(\\sigma_i\\) are the singular values of \\(\\boldsymbol A\\), and \\(r = \\operatorname{rank}(\\boldsymbol A)\\). Proof. Using the (non-compact) SVD \\(\\boldsymbol A= \\boldsymbol U\\boldsymbol \\Sigma\\boldsymbol V^\\top\\) we have \\[||\\boldsymbol A||_F=||\\boldsymbol U^\\top \\boldsymbol A||_F = ||\\boldsymbol U^\\top \\boldsymbol A\\boldsymbol V||_F = ||\\boldsymbol \\Sigma||_F=\\operatorname{tr}(\\boldsymbol \\Sigma^\\top\\boldsymbol \\Sigma)^\\frac{1}{2}=\\left(\\sum \\sigma_i^2 \\right)^\\frac{1}{2}.\\] We previously defined the p-norms for vectors in \\(\\mathbb{R}^p\\) to be \\[||\\boldsymbol x||_p = \\left(\\sum |x_i|^p\\right)^{\\frac{1}{p}}.\\] These vector norms induce matrix norms, sometimes also called operator norms: Definition 3.2 The p-norms for matrices are defined by \\[||\\boldsymbol A||_p = \\sup_{\\boldsymbol x\\not=0} \\frac{||\\boldsymbol A\\boldsymbol x||_p}{||\\boldsymbol x||_p} = \\sup_{\\boldsymbol x: ||\\boldsymbol x||_p=1} ||\\boldsymbol A\\boldsymbol x||_p\\] Proposition 3.11 \\[||\\boldsymbol A||_2 = \\sigma_1\\] where \\(\\sigma_1\\) is the first singular value of \\(\\boldsymbol A\\). Proof. By Proposition 3.8. 3.6.2 Eckart-Young-Mirsky Theorem Now that we have defined a norm (i.e., a distance) on matrices, we can think about approximating a matrix \\(\\boldsymbol A\\) by a matrix that is easier to work with. We have shown that any matrix can be split into the sum of rank-1 component matrices \\[\\boldsymbol A= \\sum_{i=1}^r \\sigma_i \\boldsymbol u_i \\boldsymbol v_i^\\top\\] We’ll now consider a family of approximations of the form \\[\\begin{equation} \\boldsymbol A_k = \\sum_{i=1}^k \\sigma_i \\boldsymbol u_i \\boldsymbol v_i^\\top \\tag{3.3} \\end{equation}\\] where \\(k&lt;=r=\\operatorname{rank}(\\boldsymbol A)\\). This is a rank-k matrix, and as we’ll now show, it is the best possible rank-k approximation to \\(\\boldsymbol A\\). Theorem 3.1 (Eckart-Young-Mirsky) For either the 2-norm \\(||\\cdot||_2\\) or the Frobenious norm \\(||\\cdot||_F\\) \\[||\\boldsymbol A-\\boldsymbol A_k|| \\leq ||\\boldsymbol A-\\boldsymbol B|| \\mbox{ for all rank-k matrices }\\boldsymbol B.\\] Moreover, \\[||\\boldsymbol A-\\boldsymbol A_k|| =\\begin{cases} \\sigma_{k+1} &amp;\\mbox{for the }||\\cdot||_2 \\mbox{ norm}\\\\ \\left(\\sum_{i=k+1}^r \\sigma_{i}^2\\right)^{\\frac{1}{2}} &amp;\\mbox{for the }||\\cdot||_F \\mbox{ norm.}\\\\ \\end{cases}\\] Proof. The last part follows from Propositions 3.11 and 3.10. Non-examinable: this is quite a tricky proof, but I’ve included it as its interesting to see. We’ll just prove it for the 2-norm. Let \\(\\boldsymbol B\\) be an \\(n\\times p\\) matrix of rank \\(k\\). The null space \\(\\mathcal{N}(\\boldsymbol B)\\subset\\mathbb{R}^p\\) must be of dimension \\(p-k\\) by the rank nullity theorem. Consider the \\(p \\times (k+1)\\) matrix \\(\\boldsymbol V_{k+1}=[\\boldsymbol v_1\\; \\ldots \\;\\boldsymbol v_{k+1}]\\). This has rank \\(k+1\\), and has column space \\(\\mathcal{C}(\\boldsymbol V_{k+1})\\subset \\mathbb{R}^{p}\\). Because \\[\\dim \\mathcal{N}(\\boldsymbol B)+\\dim \\mathcal{C}(\\boldsymbol V_{k+1})=p-k+k+1=p+1\\] we can see that \\(\\mathcal{N}(B)\\) and \\(\\mathcal{C}(V_{k+1})\\) cannot be disjoint spaces (as they are both subsets of the p-dimensional space \\(\\mathbb{R}^p\\)). Thus we can find \\(\\boldsymbol w\\in \\mathcal{N}(B)\\cap\\mathcal{C}(V_{k+1})\\), and moreover we can choose \\(\\boldsymbol w\\) so that \\(||\\boldsymbol w||_2=1\\). Because \\(\\boldsymbol w\\in \\mathcal{C}(\\boldsymbol V_{k+1})\\) we can write \\(\\boldsymbol w= \\sum_{i=1}^{k+1}w_i \\boldsymbol v_i\\) with \\(\\sum_{i=1}^{k+1}w_i^2=1\\). Then \\[\\begin{align*} ||\\boldsymbol A-\\boldsymbol B||_2^2 &amp;\\geq ||(\\boldsymbol A-\\boldsymbol B)\\boldsymbol w||_{2}^2 \\quad \\mbox{ by definition of the matrix 2-norm}\\\\ &amp;=||\\boldsymbol A\\boldsymbol w||_2^2 \\quad \\mbox{ as } \\boldsymbol w\\in \\mathcal{N}(\\boldsymbol B)\\\\ &amp;=\\boldsymbol w^\\top \\boldsymbol V\\boldsymbol \\Sigma^2\\boldsymbol V^\\top \\boldsymbol w\\quad\\mbox{ using the SVD} \\boldsymbol A=\\boldsymbol U\\boldsymbol \\Sigma\\boldsymbol V^\\top\\\\ &amp;=\\sum_{i=1}^{k+1}\\sigma_i^2 w_i^2 \\quad\\mbox{ by substituting }\\boldsymbol w= \\sum_{i=1}^{k+1}w_i \\boldsymbol v_i\\\\ &amp;\\geq \\sigma_{k+1}^2 \\sum_{i=1}^{k+1} w_i^2\\quad\\mbox{ as } \\sigma_1\\geq\\sigma_2\\geq\\ldots\\\\ &amp;= \\sigma_{k+1}^2 \\quad\\mbox{ as } \\sum_{i=1}^{k+1}w_i^2=1\\\\ &amp;=||\\boldsymbol A-\\boldsymbol A_k||_2^2 \\end{align*}\\] as required This best-approximation property is what makes the SVD so useful in applications. 3.6.3 Example: image compression As an example, lets consider the image of some peppers from the USC-SIPI image database. library(tiff) library(rasterImage) peppers&lt;-readTIFF(&quot;figs/Peppers.tiff&quot;) plot(as.raster(peppers)) This is a \\(512 \\times 512\\) colour image, meaning that there are three matrices \\(\\boldsymbol R, \\boldsymbol B,\\boldsymbol G\\) of dimension \\(512\\times 512\\)) giving the intensity of red, green, and blue for each pixel. Naively storing this matrix requires 5.7Mb. We can compute the SVD of the three colour intensity matrices, and the view the image that results from using reduced rank versions \\(\\boldsymbol B_k, \\boldsymbol G_k, \\boldsymbol R_k\\) instead (as in Equation (3.3)). The image below is formed using \\(k=5, 30, 100\\), and \\(300\\) basis vectors. svd_image &lt;- function(im,k){ s &lt;- svd(im) Sigma_k &lt;- diag(s$d[1:k]) U_k &lt;- s$u[,1:k] V_k &lt;- s$v[,1:k] im_k &lt;- U_k %*% Sigma_k %*% t(V_k) ## the reduced rank SVD produces some intensities &lt;0 and &gt;1. # Let&#39;s truncate these im_k[im_k&gt;1]=1 im_k[im_k&lt;0]=0 return(im_k) } par(mfrow=c(2,2), mar=c(1,1,1,1)) pepprssvd&lt;- peppers for(k in c(4,30,100,300)){ svds&lt;-list() for(ii in 1:3) { pepprssvd[,,ii]&lt;-svd_image(peppers[,,ii],k) } plot(as.raster(pepprssvd)) } You can see that for \\(k=30\\) we have a reasonable approximation, but with some errors. With \\(k=100\\) it is hard to spot the difference with the original. The size of the four compressed images is 45Kb, 345Kb, 1.1Mb and 3.4Mb. You can see further demonstrations of image compression with the SVD here. We will see much more of the SVD in later chapters. "],["part-ii-dimension-reduction-methods.html", "PART II: Dimension reduction methods", " PART II: Dimension reduction methods In many applications, a large number of variables are recorded for each experimental unit under study. For example, if we think of individual people as the experimental units, then in a health check-up we might collect data on age, blood pressure, cholesterol level, blood test results, lung function, weight, height, BMI, etc. If you use websites such as Amazon, Facebook, and Google, they store thousands (possibly millions) of pieces of information about you (this article shows you how to download the information Google stores about you, including all the locations you’ve visited, every search, youtube video, or app you’ve used and more). They process this data to create an individual profile for each user, which they can then use to create targetted adverts. When analysing data of moderate or high dimension, it is often desirable to seeks ways to restructure the data and reduce its dimension whilst retaining the most important information within the data. There a variety of reasons we might want to do this. In reduced dimensions, it is often much easier to understand and appreciate the most important features of a dataset. If there is a lot of reduncancy in the data, we might want to reduce the dimension to lower the memory requirements in storing it (e.g. with sound and image compression). In high dimensions, it can be difficult to analyse data (e.g. with statistical methods), and so reducing the dimension can be a way to make a dataset amenable to analysis. In this part of the module we investigate three different methods for dimension reduction: Principal Component Analysis (PCA) in Chapter 4; Canonical Correlation Analysis (CCA) in Chapter 5; and Multidimensional Scaling (MDS) in Chapter 6. Matrix algebra (Chapters 2 and 3) plays a key role in all three of these techniques. A warning Beware that high-dimensional data can behave qualitatively differently to low-dimensional data. As an example, lets consider 1000 points uniformly distributed in \\([0,1]^d\\), and think about how close together or spread out the points are. A simple way to do this is to consider the ratio of the maximum and minimum distance between any two points in our sample. N&lt;-1000 averatio &lt;-c() ii&lt;-1 for(d in c(2,5,10,20,30,40,50,60,80,100, 200, 350, 500, 750, 1000)){ averatio[ii] &lt;- mean(replicate(10, { X&lt;-matrix(runif(N*d), nc=d) d &lt;- as.matrix(dist(X)) # this gives a N x N matrix of the Euclidean distances between the data points. maxdist &lt;- max(d) mindist &lt;- min(d+diag(10^5, nrow=N)) # The diagonal elements of the distance matrix are zero, # so I&#39;ve added a big number to the diagonal # so that we get the minimum distance between different points maxdist/mindist})) ii &lt;- ii+1 } plot(c(2,5,10,20,30,40,50,60,80,100, 200, 350, 500, 750, 1000), averatio, ylab=&#39;Max. dist. / min. dist.&#39;, xlab=&#39;Dimension d&#39;, log=&#39;xy&#39;) So we can see that as the dimension increases, the ratio of the maximum and minimum distance between any two random points in our sample tends to 1. In other words, all points are the same distance apart! 3.6.4 Why reduce dimension? "],["4-pca.html", "Chapter 4 Principal component analysis", " Chapter 4 Principal component analysis With multivariate data, it is common to want to reduce the dimension of such data in a sensible way. For example, exam marks across different modules are averaged to produce a single overall mark for each student. Similarly, in a football league table we convert the numbers of wins, draws and losses to a single measure of points. Mathematically, we can express these examples of dimension reduction as a linear combination of the original variables, \\(y = \\boldsymbol u^\\top \\boldsymbol x\\). For the exam mark example, suppose each student sits \\(p=4\\) modules with marks, \\(x_1,x_2,x_3,x_4\\). Then, writing \\(\\boldsymbol x=(x_1, x_2 , x_3, x_4)^\\top\\) and choosing \\(\\boldsymbol u= \\left(\\frac{1}{4}, \\frac{1}{4}, \\frac{1}{4}, \\frac{1}{4} \\right)^\\top\\) gives an overall average, \\[ y =\\boldsymbol u^\\top \\boldsymbol x= \\begin{pmatrix} \\frac{1}{4} &amp; \\frac{1}{4} &amp; \\frac{1}{4} &amp; \\frac{1}{4} \\end{pmatrix} \\begin{pmatrix} x_1 \\\\ x_2 \\\\ x_3 \\\\ x_4 \\end{pmatrix} = \\frac{x_1}{4} + \\frac{x_2}{4} + \\frac{x_3}{4} + \\frac{x_4}{4}.\\] For the football league table, if \\(w\\) is the number of wins, \\(d\\) is the number of draws and \\(l\\) is the number of losses then, writing \\({\\mathbf r}=(w,d,l)^\\top\\), we choose \\(\\boldsymbol u= \\left(3,1,0 \\right)^\\top\\) to get the points score \\[ y = \\boldsymbol u^\\top {\\mathbf r}=\\begin{pmatrix} 3 &amp; 1 &amp; 0 \\end{pmatrix} \\begin{pmatrix} w \\\\ d \\\\ l \\end{pmatrix} = 3w + 1d + 0l=3w+d.\\] In these examples we use \\(\\boldsymbol u\\) to convert our original variables, the components of \\(\\boldsymbol x\\), to a new variable, \\(y\\). These choices of \\(\\boldsymbol u\\) are fairly standard for these types of data. However,we should ask whether we can do better. In a more general setting, how should we choose \\(\\boldsymbol u\\)? A key objective of principal component analysis (PCA): to find the linear combination of the original variables that maximises the variability in the new variable. Intuitively, this seems sensible for the exam mark data because a large variance in \\(y\\) would separate out the better students from the weaker students, making it easier to rank them. "],["4-1-principal-component-vectors-and-scores.html", "4.1 Principal component vectors and scores", " 4.1 Principal component vectors and scores Let \\(\\boldsymbol x_1,\\ldots,\\boldsymbol x_n\\) be \\(p \\times 1\\) vectors of measurements on \\(n\\) experimental units with sample mean \\(\\bar{\\boldsymbol x} = \\frac{1}{n} \\sum_{i=1}^n \\boldsymbol x_i\\) and sample covariance matrix\\ \\(\\boldsymbol S= \\frac{1}{n} \\sum_{i=1}^n (\\boldsymbol x_i - \\bar{\\boldsymbol x}) (\\boldsymbol x_i - \\bar{\\boldsymbol x})^\\top\\). We wish to project the data onto a lower-dimensional subspace in which the data displays maximal variation, using appropriate scalar products of the observation vectors. Let \\(\\boldsymbol u\\) be a unit vector (i.e. \\(\\| \\boldsymbol u\\| = 1\\) or \\(\\boldsymbol u^\\top \\boldsymbol u=1\\)) and define \\[y_i= \\boldsymbol u^\\top (\\boldsymbol x_i - \\bar{\\boldsymbol x})\\] for \\(i=1,\\ldots,n\\). Now \\[ \\sum_{i=1}^n y_i = \\sum_{i=1}^n \\boldsymbol u^\\top (\\boldsymbol x_i - \\bar{\\boldsymbol x}) = \\boldsymbol u^\\top \\sum_{i=1}^n (\\boldsymbol x_i - \\bar{\\boldsymbol x}) = \\boldsymbol u^\\top (n \\bar{\\boldsymbol x} - n \\bar{\\boldsymbol x}) = 0,\\] by the definition of \\(\\bar{\\boldsymbol x}\\), so \\(\\bar{y} = \\frac{1}{n} \\sum_{i=1}^n y_i = 0\\). The sample variance of the \\(y_i\\)’s is \\[\\begin{eqnarray*} s^2[\\boldsymbol u] &amp;=&amp; \\frac{1}{n} \\sum_{i=1}^n (y_i - \\bar{y})^2 = \\frac{1}{n} \\sum_{i=1}^n y_i^2 \\\\ &amp;=&amp; \\frac{1}{n} \\sum_{i=1}^n \\left[\\boldsymbol u^\\top (\\boldsymbol x_i - \\bar{\\boldsymbol x}) \\right]\\left[(\\boldsymbol x_i - \\bar{\\boldsymbol x})^\\top \\boldsymbol u\\right]\\\\ &amp;=&amp; \\boldsymbol u^\\top \\left[\\frac{1}{n} \\sum_{i=1}^n (\\boldsymbol x_i - \\bar{\\boldsymbol x})(\\boldsymbol x_i - \\bar{\\boldsymbol x})^\\top \\right]\\boldsymbol u\\\\ &amp;=&amp; \\boldsymbol u^\\top \\boldsymbol S\\boldsymbol u. \\end{eqnarray*}\\] We would like to find the \\(\\boldsymbol u\\) which maximises the sample variance, \\(s^2[\\boldsymbol u] = \\boldsymbol u^\\top \\boldsymbol S\\boldsymbol u\\) over unit vectors \\(\\boldsymbol u\\). Since \\(\\boldsymbol S\\) is symmetric, then by the spectral decomposition theorem we can write \\[\\boldsymbol S= \\boldsymbol Q\\boldsymbol \\Lambda\\boldsymbol Q^\\top = \\sum_{j=1}^p \\lambda_j \\boldsymbol q_j \\boldsymbol q_j^\\top \\] with \\(\\boldsymbol Q= [ \\boldsymbol q_1, \\ldots , \\boldsymbol q_p ]\\) an orthogonal matrix (so \\(\\boldsymbol Q\\boldsymbol Q^\\top = \\boldsymbol Q^\\top \\boldsymbol Q= \\mathbf I_p\\)) and \\(\\boldsymbol \\Lambda= \\text{diag}\\{ \\lambda_1, \\ldots, \\lambda_p \\}\\) where we may assume \\(\\lambda_1 \\geq \\cdots \\geq \\lambda_p\\) and, since \\(\\boldsymbol S\\) is a covariance matrix and therefore non-negative definite, \\(\\lambda_p \\geq 0\\). Note that \\(\\lambda_j\\) and \\(\\boldsymbol q_j\\), \\(j=1,\\ldots,p\\), are eigenvalues and eigenvectors, respectively, of \\(\\boldsymbol S\\). Then, \\[\\begin{eqnarray*} s^2[\\boldsymbol u] &amp;=&amp; \\boldsymbol u^\\top \\boldsymbol S\\boldsymbol u= \\boldsymbol u^\\top \\boldsymbol Q\\boldsymbol \\Lambda\\boldsymbol Q^\\top \\boldsymbol u = \\boldsymbol u^\\top \\left(\\sum_{j=1}^p \\lambda_j \\boldsymbol q_j \\boldsymbol q_j^\\top \\right)\\boldsymbol u\\\\ &amp;=&amp; \\sum_{j=1}^p \\lambda_j (\\boldsymbol u^\\top \\boldsymbol q_j) (\\boldsymbol q_j^\\top \\boldsymbol u) = \\sum_{j=1}^p \\lambda_j (\\boldsymbol u^\\top \\boldsymbol q_j)^2 \\\\ &amp;\\leq&amp; \\sum_{j=1}^p \\lambda_1 (\\boldsymbol u^\\top \\boldsymbol q_j)^2 \\end{eqnarray*}\\] since \\(\\lambda_1 \\geq \\lambda_j, j=1,\\ldots,p\\). Therefore, using Proposition ??, \\[ s^2[\\boldsymbol u] \\leq \\lambda_1 \\sum_{j=1}^p (\\boldsymbol u^\\top \\boldsymbol q_j)^2 = \\lambda_1 \\boldsymbol u^\\top \\left ( \\sum_{j=1}^p \\boldsymbol q_j \\boldsymbol q_j^\\top \\right) \\boldsymbol u = \\lambda_1 \\boldsymbol u^\\top \\boldsymbol u=\\lambda_1,\\] since, by assumption, \\(\\| \\boldsymbol u\\| = 1\\). Therefore, the maximum \\(s^2[\\boldsymbol u]\\) is at most \\(\\lambda_1\\), where \\(\\lambda_1\\) is the largest eigenvalue of \\(\\boldsymbol S\\). Recall that \\[ \\boldsymbol q_i^\\top \\boldsymbol q_j = \\left\\{ \\begin{array}{ll} 0 &amp; \\text{if } j \\neq i,\\\\ 1 &amp; \\text{if } j=i. \\end{array} \\right.\\] because eigenvectors are orthogonal to each other, so if we take \\(\\boldsymbol u= \\boldsymbol q_1\\) then \\[\\begin{eqnarray*} \\boldsymbol q_1^\\top \\boldsymbol S\\boldsymbol q_1 &amp;=&amp; \\boldsymbol q_1^\\top \\left(\\sum_{j=1}^p \\lambda_j \\boldsymbol q_j \\boldsymbol q_j^\\top \\right)\\boldsymbol q_1 = \\sum_{j=1}^p \\lambda_j (\\boldsymbol q_1^\\top \\boldsymbol q_j) (\\boldsymbol q_j^\\top \\boldsymbol q_1) \\\\ &amp;=&amp; \\sum_{j=1}^p \\lambda_j (\\boldsymbol q_1^\\top \\boldsymbol q_j)^2 = \\lambda_1 (\\boldsymbol q_1^\\top \\boldsymbol q_1)^2 = \\lambda_1 \\end{eqnarray*}\\] So \\(s^2[\\boldsymbol u] = \\boldsymbol u^\\top \\boldsymbol S\\boldsymbol u\\) is maximised over unit vectors \\(\\boldsymbol u\\) when \\(\\boldsymbol u= \\boldsymbol q_1\\) where \\(\\boldsymbol q_1\\) is the unit eigenvector corresponding to the largest eigenvalue, \\(\\lambda_1\\). By maximising \\(\\boldsymbol u^\\top \\boldsymbol S\\boldsymbol u\\) over unit vectors \\(\\boldsymbol u\\), we are in effect choosing a projection onto a 1-dimensional subspace which captures as much of the sample variation as possible. We can repeat this procedure and look for the largest sample variance of the \\(y_i\\)’s, when \\(\\boldsymbol u\\) is chosen to be orthogonal to \\(\\boldsymbol q_1\\) (i.e. restrict attention to those \\(\\boldsymbol u\\) such that \\(\\boldsymbol u^\\top \\boldsymbol q_1 = 0\\)). Similar reasoning shows that this constrained maximum occurs when \\(\\boldsymbol u= \\boldsymbol q_2\\), where \\(\\boldsymbol q_2\\) is the eigenvector corresponding to the second largest eigenvalue, \\(\\lambda_2\\); and the corresponding maximum of \\(\\boldsymbol u^\\top \\boldsymbol S\\boldsymbol u\\) is \\(\\lambda_2\\). We can repeat the process for \\(j=1,\\ldots,p\\) to define \\(p\\) new variables. In general, to find PC \\(j\\), we solve the following optimisation problem: \\[\\begin{equation} \\max_{\\boldsymbol u: \\, \\vert \\vert \\boldsymbol u\\vert \\vert =1}\\boldsymbol u^\\top \\boldsymbol S\\boldsymbol u \\tag{4.1} \\end{equation}\\] subject to \\[\\begin{equation} \\boldsymbol q_k^\\top \\boldsymbol u=0, \\qquad k=1, \\ldots , j-1. \\tag{4.2} \\end{equation}\\] It turns out that the maximum of (4.1) subject to (4.2) is equal to \\(\\lambda_j\\) and is obtained when \\(\\boldsymbol u=\\boldsymbol q_j\\). The 1st PC scores are \\(y_{i1} = \\boldsymbol q_1^\\top (\\boldsymbol x_i - \\bar{\\boldsymbol x}), \\quad i=1,\\ldots,n\\). \\ The 2nd PC scores are \\(y_{i2} = \\boldsymbol q_2^\\top (\\boldsymbol x_i - \\bar{\\boldsymbol x}), \\quad i=1,\\ldots,n\\). \\[ \\vdots \\] The \\(p\\)th PC scores are \\(y_{ip} = \\boldsymbol q_p^\\top (\\boldsymbol x_i - \\bar{\\boldsymbol x}), \\quad i=1,\\ldots,n\\). We summarise these findings in the following result. Proposition 4.1 Let \\(\\boldsymbol x_1, \\ldots , \\boldsymbol x_n\\) denote a sample of vectors in \\(\\mathbb{R}^p\\) with sample mean vector \\(\\bar{\\boldsymbol x}\\) and sample covariance matrix \\(\\boldsymbol S\\). Suppose \\(\\boldsymbol S\\) has spectral decomposition (see Proposition 3.3) \\[ \\boldsymbol S=\\boldsymbol Q\\boldsymbol \\Lambda\\boldsymbol Q^\\top = \\sum_{j=1}^p \\lambda_j \\boldsymbol q_j \\boldsymbol q_j^\\top, \\] where \\(\\boldsymbol Q\\) is orthogonal, \\(\\boldsymbol \\Lambda=\\text{diag}\\{\\lambda_1, \\ldots, \\lambda_p\\}\\) and \\(\\lambda_1 \\geq \\lambda_2 \\geq \\lambda_p \\geq 0\\). Then the following holds: The maximum of (4.1) subject to (4.2) is equal to \\(\\lambda_j\\) and is obtained when \\(\\boldsymbol u=\\boldsymbol q_j\\). For \\(j=1, \\ldots , p\\), the scores of the \\(j\\)th principal component (PC) are given by \\[ y_{ij}=\\boldsymbol q_j^\\top(\\boldsymbol x_i - \\bar{\\boldsymbol x}), \\qquad i=1, \\ldots , n, \\] where \\(\\boldsymbol q_j\\) is the vector of loadings for the \\(j\\)th PC. Moreover, \\[ \\boldsymbol y_i=( y_{i1}, y_{i2}, \\ldots , y_{ip})^\\top = \\boldsymbol Q^\\top (\\boldsymbol x_i -\\bar{\\boldsymbol x}), \\qquad i=1, \\ldots ,n. \\] In matrix form, the full set of PC scores is given in the matrix \\[ \\boldsymbol Y= [\\boldsymbol y_1 , \\ldots , \\boldsymbol y_n]^\\top =\\boldsymbol H\\boldsymbol X\\boldsymbol Q, \\] where \\(\\stackrel{n \\times n}{\\boldsymbol H}\\) is the centering matrix and \\(\\boldsymbol X=[\\boldsymbol x_1, \\ldots , \\boldsymbol x_n]^\\top\\) is the original data matrix. The sample mean vector of \\(\\boldsymbol y_1, \\ldots , \\boldsymbol y_n\\) is the zero vector \\({\\mathbf 0}_p\\) and the sample covariance matrix is \\(\\boldsymbol \\Lambda\\). Example 4.1 We consider the marks of \\(n=10\\) students who studied G11PRB and G11STA. ## Warning: package &#39;kableExtra&#39; was built under R version 3.6.2 student PRB SMM 1 81 75 2 79 73 3 66 79 4 53 55 5 43 53 6 59 49 7 62 72 8 79 92 9 49 58 10 55 56 The sample mean vector and sample covariance matrix are \\[ \\bar{\\boldsymbol x} = \\begin{pmatrix} 62.6 \\\\ 66.2 \\end{pmatrix}\\qquad \\text{and} \\qquad \\boldsymbol S= \\begin{pmatrix} 162.04 &amp; 135.38 \\\\ 135.38 &amp; 175.36 \\end{pmatrix}. \\] library(dplyr) ## Warning: package &#39;dplyr&#39; was built under R version 3.6.2 ## ## Attaching package: &#39;dplyr&#39; ## The following object is masked from &#39;package:kableExtra&#39;: ## ## group_rows ## The following objects are masked from &#39;package:stats&#39;: ## ## filter, lag ## The following objects are masked from &#39;package:base&#39;: ## ## intersect, setdiff, setequal, union secondyr %&gt;% select(2:3) %&gt;% colMeans -&gt; xbar secondyr %&gt;% select(2:3) %&gt;% cov(use=&quot;everything&quot;)*9/10 -&gt; S eigs = eigen(S) DELETE THIS - ASSUME THEY CAN DO IT, OR DO ON A COMPUTER. To find the eigenvalues we need to solve \\(|\\boldsymbol S- \\lambda \\mathbf I| = 0\\), where \\[\\begin{eqnarray*} |\\boldsymbol S- \\lambda \\mathbf I_2| &amp;=&amp; (162.04-\\lambda)(175.36-\\lambda) - 135.38^2 \\\\ &amp;=&amp; \\lambda^2 - 337.4 \\lambda + 10887.59. \\end{eqnarray*}\\] Using the quadratic equation formula we find, \\[ \\lambda = \\frac{337.4 \\pm \\sqrt{337.4^2 - 4(10087.59)}}{2} = \\frac{337.4 \\pm \\sqrt{73488.4}}{2}. \\] So \\(\\lambda_1 = 304.24\\) and \\(\\lambda_2 = 33.16\\). To find the first eigenvector we solve \\((\\boldsymbol S- \\lambda_1 \\mathbf I_2) \\boldsymbol q_1 = \\boldsymbol 0\\). To simplify, we use row operations: \\[\\begin{eqnarray*} \\boldsymbol S- \\lambda_1 \\mathbf I_2 = \\begin{pmatrix} -142.20 &amp; 135.38 \\\\ 135.38 &amp; -128.88 \\end{pmatrix} &amp;\\rightarrow&amp; \\begin{pmatrix} 1 &amp; -0.952 \\\\ 135.38 &amp; -128.88 \\end{pmatrix} \\\\ &amp;\\rightarrow&amp; \\begin{pmatrix} 1 &amp; -0.952 \\\\ 0 &amp; 0 \\end{pmatrix}. \\end{eqnarray*}\\] If we let \\(\\boldsymbol q_1 = (q_{11}, q_{21})^\\top\\) then solving \\((\\boldsymbol S- \\lambda_1 \\mathbf I_2) \\boldsymbol q_1 = \\boldsymbol 0\\) is equivalent to solving \\[ \\begin{pmatrix} 1 &amp; -0.952 \\\\ 0 &amp; 0 \\end{pmatrix} \\begin{pmatrix} q_{11} \\\\ q_{21} \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}. \\] So \\(q_{11} = 0.952 q_{21}\\) and the eigenvectors are of the form \\(t\\begin{pmatrix} 0.952 \\\\ 1 \\end{pmatrix}\\) where \\(t \\ne 0\\) is a constant. We choose \\(t\\) such that \\(\\| \\boldsymbol q\\| = 1\\), so \\[ {\\displaystyle t = \\pm \\frac{1}{\\sqrt{0.952^2 + 1^2}} = \\pm 0.724}. \\] Therefore, \\[ \\boldsymbol q_1 = 0.724 \\begin{pmatrix} 0.952 \\\\ 1 \\end{pmatrix} = \\begin{pmatrix} 0.690 \\\\ 0.724 \\end{pmatrix}.\\] To find the second eigenvector we use the same method to solve \\((\\boldsymbol S- \\lambda_2 \\mathbf I_2) \\boldsymbol q_2 = \\boldsymbol 0\\) and find that \\(\\boldsymbol q_2 = \\begin{pmatrix} -0.724 \\\\ 0.690 \\end{pmatrix}\\). The plot below shows the original data. The two lines, centred on \\(\\bar{\\boldsymbol x}\\), have the direction of the eigenvectors, and their lengths are \\(2 \\sqrt{\\lambda_j}\\), \\(j=1,2\\). We can now compute the PC scores using \\[\\begin{eqnarray*} y_{i1} &amp;=&amp; \\boldsymbol q_1^\\top (\\boldsymbol x_i - \\bar{\\boldsymbol x}) = 0.690 (x_{1i} - \\bar{x}_1) + 0.724 (x_{2i} - \\bar{x}_2) \\\\ y_{i2} &amp;=&amp; \\boldsymbol q_2^\\top (\\boldsymbol x_i - \\bar{\\boldsymbol x}) = -0.724 (x_{1i} - \\bar{x}_1) + 0.690 (x_{2i} - \\bar{x}_2), \\end{eqnarray*}\\] which gives FIX FIX Note that these new variables have sample mean \\(\\bar{\\boldsymbol y}=\\boldsymbol 0\\) and sample covariance matrix (see part 4. of Proposition 4.1) \\[ \\boldsymbol \\Lambda= \\text{diag}(\\lambda_1,\\lambda_2) = \\begin{pmatrix} 304.24 &amp; 0 \\\\ 0 &amp; 33.16 \\end{pmatrix}. \\] The plot below shows the PC scores \\((y_{i1},y_{i2})^\\top\\). The two lines shown have lengths \\(2\\sqrt{\\lambda_j}\\), \\(j=1,2\\). Note that \\(\\sqrt{\\lambda_j}\\) is the standard deviation of the \\(j\\)th PC. Sometimes the new variables have an obvious interpretation. Note that the first PC gives positive, roughly equal, weight to PRB and STA and thus represents some form of ``average’’ mark. For example, a student that has a high mark on PRB and STA will have a high value for \\(y_1\\). The second PC, meanwhile, represents a contrast between PRB and STA. For example, a large positive value for \\(y_2\\) implies the student did much better on STA than PRB, and a large negative value implies the opposite. Note that we could have chosen \\(t=-0.724\\) instead of \\(t=+0.724\\). The only difference would be that the first eigenvector was \\(\\boldsymbol q_1^\\ast = -\\boldsymbol q_1\\). In this case, a student who scored a high mark on PRB and STA would have a low value for \\(y_1\\). This is perfectly legitimate but makes the interpretation less intuitive. One can always change the sign of the eigenvectors if it makes interpretation easier. ``` "],["4-2-properties-of-principal-components.html", "4.2 Properties of principal components", " 4.2 Properties of principal components Let \\(\\boldsymbol x_1, \\ldots, \\boldsymbol x_n\\) have sample mean \\(\\bar{\\boldsymbol x}\\) and sample covariance matrix \\(\\boldsymbol S\\), with spectral decomposition \\(\\boldsymbol S=\\boldsymbol Q\\boldsymbol \\Lambda\\boldsymbol Q^\\top\\) where \\(\\boldsymbol Q=[\\boldsymbol q_1, \\ldots , \\boldsymbol q_p]\\) is orthogonal and \\(\\boldsymbol \\Lambda=\\text{diag}\\{\\lambda_1, \\ldots , \\lambda_p\\}\\). The transformed variables have some important properties. 0.2truein Proposition 4.2 For \\(j,k=1, \\ldots , p\\), the following results hold. \\(\\bar{y}_{+j} = n^{-1} \\sum_{i=1}^n y_{ij}=n^{-1}\\sum_{i=1}^n \\boldsymbol q_j^\\top (\\boldsymbol x_i-\\bar{\\boldsymbol x})=0\\); \\(\\boldsymbol q_j^\\top \\boldsymbol S\\boldsymbol q_j = \\lambda_j\\); \\(\\boldsymbol q_j^\\top \\boldsymbol S\\boldsymbol q_k = 0\\) for \\(j \\neq k\\); \\(\\boldsymbol q_1^\\top \\boldsymbol S\\boldsymbol q_1 \\geq \\boldsymbol q_2^\\top \\boldsymbol S\\boldsymbol q_2 \\geq \\ldots \\geq \\boldsymbol q_p^\\top \\boldsymbol S\\boldsymbol q_p\\geq 0\\); \\(\\sum_{j=1}^p \\boldsymbol q_j^\\top \\boldsymbol S\\boldsymbol q_j = \\sum_{j=1}^p \\lambda_j = \\text{tr}(\\boldsymbol S)\\); \\(\\prod_{j=1}^p \\boldsymbol q_j^\\top \\boldsymbol S\\boldsymbol q_j = \\prod_{j=1}^p \\lambda_j = |\\boldsymbol S|\\). In words: part 1. tells us that the sample mean of \\(y_{1j}, \\ldots , y_{nj}\\) for each fixed \\(j\\) is \\(0\\); part 2. tells us that, for each fixed \\(j\\), the sample variance of the \\(y_{ij},\\, i=1, \\ldots , n\\) is \\(\\lambda_j\\); part 3. states that the sample covariance of the pairs \\((y_{ij}, y_{ik})\\), \\(i=1, \\ldots , n\\), is \\(0\\) if \\(j \\neq k\\); part 4. states that the sample variance of \\(y_{ij}, \\, i=1, \\ldots , n\\), is not less than the sample variance of \\(y_{ik}, \\, i=1, \\ldots , n\\), if \\(j\\leq k\\); part 5. states that the sum of the sample variances is equal to the trace of \\(\\boldsymbol S\\); and part 6. states that the product of the sample variances is equal to the determinant of \\(\\boldsymbol S\\). From these properties we say that a proportion \\[\\frac{\\lambda_j}{\\lambda_1 + \\ldots + \\lambda_p}\\] of the variability in the sample is `explained’ by the \\(j\\)th PC. For the G11PRB and G11STA data above, \\[\\frac{\\lambda_1}{\\lambda_1 + \\lambda_2} = \\frac{304.24}{304.24+33.16} = 0.90,\\] so 90% of the variability in the sample is explained by the 1st PC. ```{Example} We can apply PCA to a football league table where \\(W\\), \\(D\\), \\(L\\) are the number of matches won, drawn and lost and \\(F\\) and \\(A\\) are the goals scored for and against. An extract of the table for a recent Premiership season is: FIX FIX Team W D L F A Chelsea 27 5 6 103 32 Manchester United 27 4 7 86 28 Arsenal 23 6 9 83 41 Tottenham Hotspur 21 7 10 67 41 Manchester City 18 13 7 73 45 The sample mean vector is \\[\\bar{\\boldsymbol x} =\\begin{pmatrix}14.2 \\\\9.6 \\\\14.2 \\\\52.6 \\\\52.6 \\\\\\end{pmatrix}\\] and the sample covariance matrix is \\[\\begin{equation} \\boldsymbol S= \\begin{pmatrix}39.4&amp;-8.27&amp;-31.1&amp;116&amp;-81.9 \\\\-8.27&amp;8.14&amp;0.13&amp;-29.4&amp;6.01 \\\\-31.1&amp;0.13&amp;31&amp;-86.3&amp;75.9 \\\\116&amp;-29.4&amp;-86.3&amp;392&amp;-209 \\\\-81.9&amp;6.01&amp;75.9&amp;-209&amp;231 \\\\\\end{pmatrix} \\tag{4.3} \\end{equation}\\] The eigenvalues of \\(\\boldsymbol S\\) are \\[\\boldsymbol \\Lambda= \\text{diag}\\begin{pmatrix}631&amp;96.7&amp;8.83&amp;2.44&amp;-4.97e-14 \\\\\\end{pmatrix}\\] Note that we have a zero eigenvalue because one of our variables is a linear combination of the other variables, \\(L = 38 - W - D\\). The corresponding eigenvectors are \\[\\boldsymbol Q= [\\boldsymbol q_1 \\ldots \\boldsymbol q_5] =\\begin{pmatrix}0.251&amp;-0.0133&amp;-0.116&amp;0.768&amp;0.577 \\\\-0.0477&amp;-0.146&amp;0.74&amp;-0.309&amp;0.577 \\\\-0.204&amp;0.16&amp;-0.624&amp;-0.459&amp;0.577 \\\\0.776&amp;0.582&amp;0.0674&amp;-0.234&amp;-2e-15 \\\\-0.539&amp;0.784&amp;0.213&amp;0.222&amp;1.83e-15 \\\\\\end{pmatrix}\\] The proportion of variability explained by each of the PCs is: \\[ \\begin{pmatrix}0.854&amp;0.131&amp;0.012&amp;0.0033&amp;-6.73e-17 \\\\\\end{pmatrix} \\] There is no point computing the scores for PC 5 because PC5 does not explain any of the variability in the data. Similarly, there is little value in computing the scores for PCs 3 &amp; 4 because they only account for 1.5% of the variability in the data. We can, therefore, choose to compute only the first two PC scores. We are reducing the dimension of our data set from \\(p=5\\) to \\(p=2\\) while still retaining 98.5% of the variability. The first PC is given by: \\[\\begin{align*} y_{i1} &amp;= 0.25(W_i-\\bar{W}) +-0.05(D_i-\\bar{D}) +-0.2(L_i-\\bar{L})\\\\ &amp; \\qquad +0.78(F_i-\\bar{F}) +-0.54(A_i-\\bar{A}), \\end{align*}\\] and similarly for PC 2. The first five rows of our revised ``league table’’ are now Team PC1 PC2 Chelsea 55.3 12.3 Manchester United 44.1 -0.4 Arsenal 33.3 8.1 Tottenham Hotspur 20.1 -1.2 Manchester City 22.2 4.1 Now that we have reduced the dimension to \\(p=2\\), we can visualise the differences between the teams. We might interpret the PCs as follows. The first PC seems to measure overall performance. It rewards teams with 0.78 for every goal they score and 0.25 for every match they win, while penalising them by 0.54 for every goal they concede, 0.2 for every match they lose and 0.05 for every match they draw. We could, therefore, rank teams by PC 1 and compare this with the rankings using 3 points for a win and 1 point for a draw. The rankings are the same for the top three teams but differ below that. Under our system Wigan would be relegated in place of Portsmouth. The second PC has a strong negative loading for both goals for and against. A team with a large negative PC 2 score was, therefore, involved in matches with lots of goals. We could, therefore, interpret PC 2 as an ``entertainment’’ measure, ranking teams according to their involvement in high-scoring games. The above example raises the question of how many PCs should we use in practice. If we reduce the dimension to \\(p=1\\) then we can rank observations and analyse our new variable with univariate statistics. If we reduce the dimension to \\(p=2\\) then it is still easy to visualise the data. However, reducing the dimension to \\(p=1\\) or \\(p=2\\) may involve losing lots of information and a sensible answer should depend on the objectives of the analysis and the data itself. One tool for looking at the contributions of each PC is to look at the scree graph which plots the percentage of variance explained by PC \\(j\\) against \\(j\\). The scree graph for the football example is: Possible methods for choosing the number of PCs include: retain enough PCs to explain, say, 90% of the total variation; retain PCs where the eigenvalue is above the average. For the football example, the first method would retain 2 PCs whereas the second method would only retain 1 PC. ``` "],["4-3-population-pca.html", "4.3 Population PCA", " 4.3 Population PCA So far we have considered sample PCA based on the sample covariance matrix \\[ \\boldsymbol S=\\frac{1}{n}\\sum_{i=1}^n (\\boldsymbol x_i-\\bar{\\boldsymbol x})(\\boldsymbol x_i-\\bar{\\boldsymbol x})^\\top. \\] We note now that there is a population analogue of PCA based on the population covariance matrix \\(\\boldsymbol \\Sigma\\). Although the population version of PCA is not of as much direct practical relevance as sample PCA, it is nevertheless of conceptual importance. Let \\(\\boldsymbol x\\) denote a \\(p \\times 1\\) random vector with \\(E(\\boldsymbol x)={\\pmb \\mu}\\) and \\(\\text{Var}(\\boldsymbol x)={\\pmb \\Sigma}\\). As defined, \\(\\pmb \\mu\\) is the population mean vector and \\(\\pmb \\Sigma\\) is the population covariance matrix. Since \\(\\pmb \\Sigma\\) is symmetric, the spectral decomposition theorem tells us that \\[ {\\pmb \\Sigma}=\\sum_{j=1}^p \\check{\\lambda}_j \\check{\\boldsymbol q}_j \\check{\\boldsymbol q}_j^\\top=\\check{\\boldsymbol Q} \\check{\\boldsymbol \\Lambda}\\check{\\boldsymbol Q}^\\top \\] where the `check’ symbol \\(\\quad \\check{} \\quad\\) is used to distinguish population quantities from their sample analogues. Then: the first population PC is defined by \\(Y_1=\\check{\\boldsymbol q}_1^\\top (\\boldsymbol x-{\\pmb \\mu})\\); -the second population PC is defined by \\(Y_2=\\check{\\boldsymbol q}_2^\\top (\\boldsymbol x-{\\pmb \\mu})\\); $ldots$ the \\(p\\)th population PC is defined by \\(Y_p=\\check{\\boldsymbol q}_p^\\top (\\boldsymbol x-{\\pmb \\mu})\\). The \\(Y_1, \\ldots , Y_p\\) are random variables, unlike the sample PCA case, where the \\(y_{ij}\\) are observed quantities. In the sample PCA case, the \\(y_{ij}\\) can often be regarded as the observed values of random variables. In matrix form, the above definitions can be summarised by writing \\[ \\boldsymbol y=\\begin{pmatrix} Y_1 \\\\ Y_2 \\\\ ... \\\\...\\\\Y_p \\end{pmatrix} = \\check{\\boldsymbol Q}^\\top (\\boldsymbol x-{\\pmb \\mu}). \\] The population PCA analogues of the 6 sample PCA properties listed in Proposition 4.2 are now given. Note that the \\(Y_j\\)’s are random variables as opposed to observed values of random variables. Proposition 4.3 The following results hold for the random variables \\(Y_1, \\ldots , Y_p\\) defined above. \\(E(Y_j)=0\\) for \\(j=1, \\ldots , p\\);\\ \\(\\text{Var}(Y_j)=\\check{\\lambda}_j\\) for \\(j=1,\\ldots, p\\);\\ \\(\\text{Cov}(Y_j,Y_k)=0\\) if \\(j \\neq k\\);\\ \\(\\text{Var}(Y_1) \\geq \\text{Var}(Y_2) \\geq \\cdots \\geq \\text{Var}(Y_p) \\geq 0\\);\\ \\(\\sum_{j=1}^p \\text{Var}(Y_j)=\\sum_{j=1}^p \\check{\\lambda}_j=\\text{tr}(\\boldsymbol \\Sigma)\\);\\ \\(\\prod_{j=1}^p \\text{Var}(Y_j)=\\prod_{j=1}^p \\check{\\lambda}_j=\\vert \\boldsymbol \\Sigma\\vert\\). Note that, defining \\(\\boldsymbol y=(Y_1, \\ldots , Y_p)^\\top\\) as before, part 1. implies that \\(E(\\boldsymbol y)={\\mathbf 0}_p\\) and parts 2. and 3. together imply that \\[ \\text{Var}(\\boldsymbol y)=\\boldsymbol \\Lambda\\equiv \\text{diag}(\\check{\\lambda}_1, \\ldots , \\check{\\lambda}_p). \\] Example 4.2 Suppose \\[ \\boldsymbol \\Sigma=\\mathbf I_p+ \\delta {\\mathbf 1}_p {\\mathbf 1}_p^\\top, \\] where \\(\\delta&gt;0\\). What is the proportion of variability explained by the first PC? Since \\(\\delta&gt;0\\), the largest eigenvalue is \\(\\lambda_1=1+p\\delta\\) which is achieved when \\(\\check{\\boldsymbol q}_1\\) is the unit vector \\(p^{-1/2}{\\mathbf 1 }_p\\). This and related examples are dealt with in more detail in the example sheets. Consider now a repeated sampling framework in which we assume that \\(\\boldsymbol x_1, \\ldots , \\boldsymbol x_n\\) are IID random vectors from a population with mean vector \\(\\pmb \\mu\\) and covariance matrix \\(\\boldsymbol \\Sigma\\). What is the relationship between the sample PCA based on the sample of observed vectors \\(\\boldsymbol x_1, \\ldots , \\boldsymbol x_n\\), and the population PCA based on the unobserved random vector \\(\\boldsymbol x\\), from the same population? Assuming \\(n\\) is large, and the elements of \\(\\boldsymbol \\Sigma\\) are all finite, the elements of the sample covariance matrix \\(\\boldsymbol S\\) will be close with high probability to the corresponding elements of the population covariance matrix \\(\\boldsymbol \\Sigma\\). Justification of this statement comes from the weak law of large numbers applied to the components of \\(\\Sigma\\) (details omitted). Consequently, when \\(n\\) is large, sample PCA and the corresponding population PCA may be expected to give similar results. "],["4-4-an-alternative-derivation-of-pca.html", "4.4 An Alternative Derivation of PCA", " 4.4 An Alternative Derivation of PCA Consider a sample \\(\\boldsymbol x_1, \\ldots , \\boldsymbol x_n \\in \\mathbb{R}^p\\). Recall from 2.8 that any line in \\(\\mathbb{R}^p\\) may be written in the form \\(\\{\\boldsymbol a+u \\boldsymbol b: \\, u \\in \\mathbb{R}\\}\\) where \\(\\boldsymbol a, \\boldsymbol b\\in \\mathbb{R}^p\\) are fixed. Here we consider the following problem: find the best-fitting line to the sample \\(\\boldsymbol x_1, \\ldots , \\boldsymbol x_n\\). We first formulate this problem more precisely. Define the function \\[\\begin{align*} F(\\boldsymbol a, \\boldsymbol b; u_1, \\ldots , u_n)&amp;=\\sum_{i=1}^n \\vert \\vert \\boldsymbol x_i - \\boldsymbol a- u_i \\boldsymbol b\\vert \\vert^2\\\\ &amp; =\\sum_{i=1}^n (\\boldsymbol x_i - \\boldsymbol a-u_i \\boldsymbol b)^\\top (\\boldsymbol x_i - \\boldsymbol a-u_i \\boldsymbol b). \\end{align*}\\] We wish to solve the following problem: \\[\\begin{align} &amp;\\textit{minimise $F(\\boldsymbol a, \\boldsymbol b; u_1, \\ldots , u_n)$ subject to the}\\nonumber \\\\ &amp;\\textit{constraints that $\\boldsymbol a$ and $\\boldsymbol b$ are orthogonal, i.e. $\\boldsymbol a^\\top \\boldsymbol b=0$,}\\tag{4.4}\\\\ &amp;\\textit{and $\\boldsymbol b$ is a unit vector, i.e. $\\vert \\vert \\boldsymbol b\\vert \\vert =1$.} \\nonumber \\end{align}\\] Theorem 4.1 The solution to optimisation problem (4.4) is given by \\[\\begin{equation} \\hat{\\boldsymbol a}=\\left ({\\mathbf I}_p-\\boldsymbol q_1 \\boldsymbol q_1^\\top\\right)\\bar{\\boldsymbol x}, \\quad \\hat{\\boldsymbol b}= \\boldsymbol q_1 \\quad \\hbox{and} \\quad \\hat{u}_i=\\boldsymbol x_i^\\top \\boldsymbol q_1, \\quad i=1,\\ldots , n, \\tag{4.5} \\end{equation}\\] where \\(\\bar{\\boldsymbol x}=n^{-1}\\sum_{i=1}^n \\boldsymbol x_i\\) is the sample mean and the unit vector \\(\\boldsymbol q_1\\) is the direction of the first sample PC. Note that the quantities \\(u_i -\\bar{u}=\\boldsymbol q_1^\\top (\\boldsymbol x_i-\\bar{\\boldsymbol x})\\) are the PC scores associated with the first PC. Proof. The proof is broken into two steps. Step 1. In Step 1, we want to minimise \\(F(\\boldsymbol a, \\boldsymbol b; u_1,\\ldots, u_n)\\) subject to the constraint \\(\\boldsymbol a^\\top \\boldsymbol b=0\\), with \\(\\boldsymbol b\\) an arbitrary fixed unit vector in \\(\\mathbb{R}^p\\). So we introduce a Lagrangian term for the constraint \\(\\boldsymbol a^\\top \\boldsymbol b=0\\) and minimise \\[ \\bar{F}(\\boldsymbol a; u_1,\\ldots , u_n; \\gamma) \\equiv \\left \\{\\sum_{i=1}^n (\\boldsymbol x_i -\\boldsymbol a-u_i\\boldsymbol b)^\\top (\\boldsymbol x_i-\\boldsymbol a-u_i\\boldsymbol b)\\right \\}+ \\gamma \\boldsymbol a^\\top \\boldsymbol b \\] over \\(\\boldsymbol a\\), \\(u_1, \\ldots , u_n\\) and \\(\\gamma\\). Then, for \\(i=1, \\ldots , n\\), \\[\\begin{equation} \\frac{\\partial \\bar{F}}{\\partial u_i}=-2\\boldsymbol b^\\top (\\boldsymbol x_i-\\boldsymbol a-u_i\\boldsymbol b); \\tag{4.6} \\end{equation}\\] \\[\\begin{align} \\frac{\\partial \\bar{F}}{\\partial \\boldsymbol a}&amp;=-2\\left \\{\\sum_{i=1}^n (\\boldsymbol x_i -\\boldsymbol a-u_i\\boldsymbol b)\\right \\}+\\gamma \\boldsymbol b\\nonumber\\\\ &amp;=-2n\\{\\bar{\\boldsymbol x}-\\boldsymbol a-(\\bar{u}+\\gamma/(2n))\\boldsymbol b\\}, \\tag{4.7} \\end{align}\\] where \\(\\bar{u}=n^{-1}\\sum_{i=1}^n u_i\\); and \\[\\begin{equation} \\frac{\\partial \\bar{F}}{\\partial \\gamma}=\\boldsymbol a^\\top \\boldsymbol b. \\tag{4.8} \\end{equation}\\] Setting the partial derivatives (4.6), (4.7) and @ref(eq:barF3}) to zero, \\[ \\frac{\\partial \\bar{F}}{\\partial \\gamma} =0 \\implies \\hat{\\boldsymbol a}^\\top \\boldsymbol b=0; \\] \\[\\begin{equation} \\frac{\\partial \\bar{F}}{\\partial u_i}=0 \\implies \\hat{u}_i=\\boldsymbol b^\\top \\boldsymbol x_i, \\tag{4.9} \\end{equation}\\] and therefore \\[\\begin{equation} \\bar{\\hat{u}}\\equiv n^{-1} \\sum_{i=1}^n \\hat{u}_i=\\boldsymbol b^\\top \\bar{\\boldsymbol x}; \\tag{4.10} \\end{equation}\\] and \\[ \\frac{\\partial \\bar{F}}{\\partial \\boldsymbol a}={\\mathbf 0}_p \\implies \\hat{\\boldsymbol a}=\\bar{\\boldsymbol x}-\\{\\bar{\\hat{u}}+\\hat{\\gamma}/(2n)\\}\\boldsymbol b. \\] Using (4.10) and the fact that \\(\\boldsymbol b^\\top \\hat{\\boldsymbol a}=0\\), it follows that \\[ 0=\\boldsymbol b^\\top \\hat{\\boldsymbol a} =\\boldsymbol b^\\top [\\bar{\\boldsymbol x}-\\{\\hat{\\bar{u}}+\\hat{\\gamma}/(2n)\\}\\boldsymbol b]=\\boldsymbol b^\\top \\bar{\\boldsymbol x}-\\boldsymbol b^\\top \\bar{\\boldsymbol x} +\\hat{\\gamma}/(2n), \\] which implies that \\(\\hat{\\gamma}=0\\). Consequently, \\[\\begin{equation} \\hat{\\boldsymbol a}=\\bar{\\boldsymbol x}-\\bar{\\hat{u}}\\boldsymbol b=\\bar{\\boldsymbol x}-\\boldsymbol b\\boldsymbol b^\\top \\bar{\\boldsymbol x}=\\left ( \\mathbf I_p-\\boldsymbol b\\boldsymbol b^\\top \\right ) \\bar{\\boldsymbol x}; \\tag{4.11} \\end{equation}\\] and so \\[\\begin{align} &amp;\\bar{F}(\\hat{\\boldsymbol a}; \\hat{u}_1, \\ldots, \\hat{u}_n; \\hat{\\gamma})\\\\ &amp;=\\sum_{i=1}^n (\\boldsymbol x_i-\\hat{\\boldsymbol a}-\\hat{u_i} \\boldsymbol b)^\\top (\\boldsymbol x_i -\\hat{\\boldsymbol a}-\\hat{u}_i\\boldsymbol b)\\nonumber \\\\ &amp;=\\sum_{i=1}^n \\left \\{\\boldsymbol x_i-({\\mathbf I}_p-\\boldsymbol b\\boldsymbol b^\\top)\\bar{\\boldsymbol x}- \\boldsymbol b\\boldsymbol b^\\top \\boldsymbol x_i \\right \\}^\\top \\left \\{\\boldsymbol x_i -({\\mathbf I}_p-\\boldsymbol b\\boldsymbol b^\\top)\\bar{\\boldsymbol x} -\\boldsymbol b\\boldsymbol b^\\top \\boldsymbol x_i\\right \\}\\nonumber \\\\ &amp;\\sum_{i=1}^n (\\boldsymbol x_i -\\bar{\\boldsymbol x})^\\top ({\\mathbf I}_p -\\boldsymbol b\\boldsymbol b^\\top)^2 (\\boldsymbol x_i -\\bar{\\boldsymbol x})\\nonumber \\\\ &amp;= n \\text{tr}\\left \\{ ({\\mathbf I}_p-\\boldsymbol b\\boldsymbol b^\\top )\\boldsymbol S\\right \\} \\nonumber \\\\ &amp;=n\\left \\{\\text{tr}(\\boldsymbol S)-\\boldsymbol b^\\top \\boldsymbol S\\boldsymbol b\\right\\}, \\tag{4.12} \\end{align}\\] where \\(\\boldsymbol S\\) is the sample covariance of the \\(\\boldsymbol x_i\\). Step 2. We now minimise (4.12) over unit vectors \\(\\boldsymbol b\\in \\mathbb{R}^p\\). But minimising (4.12) is equivalent to maximising \\(\\boldsymbol b^\\top \\boldsymbol S\\boldsymbol b\\), so from Proposition 3.7, \\(\\hat{\\boldsymbol b}= \\boldsymbol q_1\\), and so \\(\\hat{\\boldsymbol a}=(\\mathbf I_p-\\boldsymbol q_1 \\boldsymbol q_1^\\top)\\bar{\\boldsymbol x}\\) from (4.11), and from (4.9), \\(\\hat{u}_i=\\boldsymbol q_1^\\top \\boldsymbol x_i\\), \\(i=1,\\ldots , n\\), all of which agrees with the expressions in (4.5). "],["4-5-pca-under-transformations-of-variables.html", "4.5 PCA under transformations of variables", " 4.5 PCA under transformations of variables Let us return to the example of \\(n=10\\) students who studied G11PRB and G11STA. Earlier, we calculated the sample mean, sample variance matrix and the eigenvalues/vectors of \\(\\boldsymbol S\\), \\[\\begin{eqnarray*} \\bar{\\boldsymbol x} = \\begin{pmatrix} 62.6 \\\\ 66.2 \\end{pmatrix}, &amp;\\quad&amp; \\boldsymbol S= \\begin{pmatrix} 162.04 &amp; 135.38 \\\\ 135.38 &amp; 175.36 \\end{pmatrix} \\\\ \\boldsymbol \\Lambda= \\begin{pmatrix} 304.24 &amp; 0 \\\\ 0 &amp; 33.16 \\end{pmatrix}, &amp;\\quad&amp; \\boldsymbol Q= \\begin{pmatrix} 0.690 &amp; -0.724 \\\\ 0.724 &amp; 0.690 \\end{pmatrix} \\end{eqnarray*}\\] with PC 1 scores \\[y_i = \\boldsymbol q_1^\\top (\\boldsymbol x_i - \\bar{\\boldsymbol x}) = 0.690 (x_{1i} - \\bar{x}_1) + 0.724 (x_{2i} - \\bar{x}_2).\\] We now consider what happens to the above quantities under various transformations of the \\(\\boldsymbol x_i\\), the \\(2 \\times 1\\) response vectors. Addition transformation Firstly, we consider the transformation of addition where, for example, the G11PRB lecturer decides to add 5 marks for all the students. We can write this transformation as \\(\\boldsymbol z_i = \\boldsymbol x_i + \\boldsymbol c\\), where \\(\\boldsymbol c\\) is a fixed vector. Under this transformation the sample mean changes, \\(\\bar{\\boldsymbol z} = \\bar{\\boldsymbol x} + \\boldsymbol c\\), but the sample variance remains \\(\\boldsymbol S\\). Consequently, the eigenvalues and eigenvectors of \\(\\boldsymbol S\\) remain the same and, therefore, so does the PC 1 score, \\[y_i = \\boldsymbol q_1^\\top (\\boldsymbol z_i - \\bar{\\boldsymbol z}) = \\boldsymbol q_1^\\top(\\boldsymbol x_i + \\boldsymbol c- (\\bar{\\boldsymbol x} + \\boldsymbol c)) = \\boldsymbol q_1^\\top (\\boldsymbol x_i - \\bar{\\boldsymbol x}).\\] We say that the principal components are invariant under the addition transformation. An important special case is to choose \\(\\boldsymbol c= -\\bar{\\boldsymbol x}\\) so that the PC 1 score is simply \\(y_i = \\boldsymbol q_1^\\top \\boldsymbol z_i\\). Scale transformation Secondly, we consider the scale transformation where, for example, the G11PRB lecturer decides to double the marks for all students. A scale transformation occurs more naturally when we convert units of measurement from, say, metres to kilometres. We can write this transformation as \\(\\boldsymbol z_i = \\boldsymbol D\\boldsymbol x_i\\), where \\(\\boldsymbol D\\) is a diagonal matrix with positive elements. Under this transformation the sample mean changes from \\(\\bar{\\boldsymbol x}\\) to \\(\\bar{\\boldsymbol z} = \\boldsymbol D\\bar{\\boldsymbol x}\\), and the sample covariance matrix changes from \\(\\boldsymbol S\\) to \\(\\boldsymbol D\\boldsymbol S\\boldsymbol D\\). Consequently, the principal components also change. This lack of scale-invariance is undesirable. One solution is to choose \\[ \\boldsymbol D= \\text{diag}(s_{11}^{-1/2}, \\ldots , s_{pp}^{-1/2}), \\] where \\(s_{ii}\\) is the \\(i\\)th diagonal element of \\(\\boldsymbol S\\). In effect, we have standardised all the new variables to have variance 1. In this case the sample covariance matrix of the \\(\\boldsymbol z_i\\)’s is simply the sample correlation matrix of the original variables, \\(\\boldsymbol x_i\\). Therefore, we can carry out PCA on the sample correlation matrix, \\(\\boldsymbol R\\), which is invariant to changes of scale. In summary: \\(\\boldsymbol R\\) is scale-invariant while \\(\\boldsymbol S\\) is not. Example 4.3 For the G11PRB/G11STA data, we choose \\[ \\boldsymbol D= \\text{diag}(162.04,175.36)^{-1/2} = \\text{diag}(0.079,0.076) \\] so that \\(\\boldsymbol z_i = \\boldsymbol D\\boldsymbol x_i\\). The sample correlation matrix is then \\[\\begin{align*} \\boldsymbol R&amp;= \\boldsymbol D\\boldsymbol S\\boldsymbol D\\\\ &amp;= \\begin{pmatrix} 0.079 &amp; 0 \\\\ 0 &amp; 0.076 \\end{pmatrix} \\begin{pmatrix} 162.04 &amp; 135.38 \\\\ 135.38 &amp; 175.36 \\end{pmatrix} \\begin{pmatrix} 0.079 &amp; 0 \\\\ 0 &amp; 0.076 \\end{pmatrix} \\\\ &amp;= \\begin{pmatrix} 1.000 &amp; 0.803 \\\\ 0.803 &amp; 1.000 \\end{pmatrix}. \\end{align*}\\] The eigenvalues and eigenvectors of \\(\\boldsymbol R\\) are then \\[\\boldsymbol \\Lambda= \\begin{pmatrix} 1.803 &amp; 0 \\\\ 0 &amp; 0.197 \\end{pmatrix}, \\qquad \\boldsymbol Q= \\begin{pmatrix} 0.707 &amp; 0.707 \\\\ 0.707 &amp; -0.707 \\end{pmatrix},\\] and the PC 1 score is \\[\\begin{eqnarray*} y_i &amp;=&amp; \\boldsymbol q_1^\\top (\\boldsymbol z_i - \\bar{\\boldsymbol z}) = \\boldsymbol q_1^\\top \\boldsymbol D(\\boldsymbol x_i - \\bar{\\boldsymbol x}) \\\\ &amp;=&amp; 0.707 \\times 0.079 (x_{1i} - \\bar{x}_1) + 0.707 \\times 0.076 (x_{2i} - \\bar{x}_2). \\end{eqnarray*}\\] In the example above, there is little difference between using \\(\\boldsymbol S\\) and \\(\\boldsymbol R\\) for the PCA because the variances for G11PRB and G11STA are similar. In other cases, particularly when the variables are measured on wildly different scales, the difference will be notable. For example, in the football data the sample variances of \\(F\\) and \\(A\\) are much larger than the sample variances of \\(W\\), \\(D\\) and \\(L\\). Orthogonal transformation Thirdly, we consider a transformation by an orthogonal matrix, \\(\\stackrel{p \\times p}{\\boldsymbol A}\\), such that \\(\\boldsymbol A\\boldsymbol A^\\top = \\boldsymbol A^\\top \\boldsymbol A= \\mathbf I_p\\), and write \\(\\boldsymbol z_i = \\boldsymbol A\\boldsymbol x_i\\). This is equivalent to rotating and/or reflecting the original data. Let \\(\\boldsymbol S\\) be the sample covariance matrix of the \\(\\boldsymbol x_i\\) and let \\(\\boldsymbol T\\) be the sample covariance matrix of the \\(\\boldsymbol z_i\\). Under this transformation the sample mean changes from \\(\\bar{\\boldsymbol x}\\) to \\(\\bar{\\boldsymbol z} = \\boldsymbol A\\bar{\\boldsymbol x}\\), and the sample covariance matrix \\(\\boldsymbol S\\) changes from \\(\\boldsymbol S\\) to \\(\\boldsymbol T= \\boldsymbol A\\boldsymbol S\\boldsymbol A^\\top\\). However, if we write \\(\\boldsymbol S\\) in terms of its spectral decomposition \\(\\boldsymbol S= \\boldsymbol Q\\boldsymbol \\Lambda\\boldsymbol Q^\\top\\), then \\(\\boldsymbol T= \\boldsymbol A\\boldsymbol Q\\boldsymbol \\Lambda\\boldsymbol Q^\\top \\boldsymbol A^\\top = \\boldsymbol B\\boldsymbol \\Lambda\\boldsymbol B^\\top\\) where \\(\\boldsymbol B= \\boldsymbol A\\boldsymbol Q\\) is also orthogonal. It is therefore apparent that the eigenvalues of \\(\\boldsymbol T\\) are the same as those of \\(\\boldsymbol S\\); and the eigenvectors of \\(\\boldsymbol T\\) are given by \\(\\boldsymbol b_j\\) where \\(\\boldsymbol b_j = \\boldsymbol A\\boldsymbol q_j\\), \\(j=1,\\ldots,p\\). The PC 1 scores of the transformed variables are \\[ y_i = \\boldsymbol b_1^\\top (\\boldsymbol z_i - \\bar{\\boldsymbol z}) = \\boldsymbol q_1^\\top \\boldsymbol A^\\top \\boldsymbol A(\\boldsymbol x_i - \\bar{\\boldsymbol x}) = \\boldsymbol q_1^\\top (\\boldsymbol x_i - \\bar{\\boldsymbol x}),\\] and so they are identical to the PC 1 scores of the original variables. Therefore, under an orthogonal transformation the eigenvalues and PC scores are unchanged and the PCs are orthogonal transformations of the original PCs. We say that the principal components are equivariant with respect to orthogonal transformations. Example 4.4 Suppose we rotate the G11PRB/G11STA data by the matrix \\(\\boldsymbol A= \\begin{pmatrix} 0.866 &amp; -0.500 \\\\ 0.500 &amp; 0.866 \\end{pmatrix}\\). The sample covariance matrix of the rotated data is \\[\\begin{align*} \\boldsymbol T&amp;= \\boldsymbol A\\boldsymbol S\\boldsymbol A^\\top\\\\ &amp;= \\begin{pmatrix} 0.866 &amp; -0.500 \\\\ 0.500 &amp; 0.866 \\end{pmatrix} \\begin{pmatrix} 162.04 &amp; 135.38 \\\\ 135.38 &amp; 175.36 \\end{pmatrix} \\begin{pmatrix} 0.866 &amp; 0.500 \\\\ -0.500 &amp; 0.866 \\end{pmatrix} \\\\ &amp;= \\begin{pmatrix} 48.13 &amp; 61.92 \\\\ 61.92 &amp; 289.27 \\end{pmatrix}. \\end{align*}\\] The eigenvalues of \\(\\boldsymbol T\\) are \\(304.24\\) and \\(33.16\\) (same as for \\(\\boldsymbol S\\)). The eigenvectors of \\(\\boldsymbol T\\) are then \\[\\begin{eqnarray*} \\boldsymbol B&amp;=&amp; \\boldsymbol A\\boldsymbol Q= \\begin{pmatrix} 0.866 &amp; -0.500 \\\\ 0.500 &amp; 0.866 \\end{pmatrix} \\begin{pmatrix} 0.690 &amp; -0.724 \\\\ 0.724 &amp; 0.690 \\end{pmatrix} \\\\ &amp;=&amp; \\begin{pmatrix} 0.235 &amp; -0.972 \\\\ 0.972 &amp; 0.235 \\end{pmatrix} \\end{eqnarray*}\\] and the PC 1 scores are unchanged. "],["4-6-pca-based-on-boldsymbol-s-versus-pca-based-on-boldsymbol-r.html", "4.6 PCA based on \\(\\boldsymbol S\\) versus PCA based on \\(\\boldsymbol R\\)", " 4.6 PCA based on \\(\\boldsymbol S\\) versus PCA based on \\(\\boldsymbol R\\) Recall the distinction between the sample covariance matrix \\(\\boldsymbol S\\) and the sample correlation matrix \\(\\boldsymbol R\\). Note that all correlation matrices are also covariance matrices, but not all covariance matrices are correlation matrices. So in practice we have a choice of using \\(\\boldsymbol S\\) or \\(\\boldsymbol R\\) for PCA. As we have seen, PCA based on \\(\\boldsymbol R\\) is scale invariant, but PCA based on \\(\\boldsymbol S\\) is not; while PCA based on \\(\\boldsymbol S\\) is invariant (eigenvalues and PC scores) and equivariant (eigenvectors) under orthogonal transformation, whereas \\(\\boldsymbol R\\) is not. This raises the important practical question: for a given dataset, should we use PCA based on \\(\\boldsymbol S\\) or \\(\\boldsymbol R\\)? If the \\(p\\) variables represent very different types of quantity or show marked differences in variances, then it will usually be better to use \\(\\boldsymbol R\\) rather than \\(\\boldsymbol S\\). However, in some circumstances, we may wish to use \\(\\boldsymbol S\\), such as when the \\(p\\) variables are measuring similar entities and the sample variances are not too different. Bearing in mind that the required numerical calculations are so easy to perform in R, we might wish to do it both ways and see if it makes much difference. "],["5-cca.html", "Chapter 5 Canonical Correlation Analysis", " Chapter 5 Canonical Correlation Analysis Suppose we observe a random sample of \\(n\\) bivariate observations \\[ \\boldsymbol z_1=(x_1,y_1)^\\top , \\ldots , \\boldsymbol z_n=(x_n,y_n)^\\top. \\] If we are interested in exploring possible dependence between the \\(x_i\\)’s and \\(y_i\\)’s then among the first things we would do would be to obtain a scatterplot of the \\(x_i\\)’s against the \\(y_i\\)’s and calculate the correlation coefficient. Recall that the sample correlation coefficient is defined by \\[\\begin{equation} r=r[x,y]=\\frac{n^{-1}\\sum_{i=1}^n (x_i-\\bar{x})(y_i-\\bar{y})}{\\left ( n^{-1}\\sum_{i=1}^n (x_i-\\bar{x})^2 \\right )^{1/2} \\left ( n^{-1}\\sum_{i=1}^n (y_i-\\bar{y})^2 \\right )^{1/2}} \\tag{5.1} \\end{equation}\\] where \\(\\bar{x}=n^{-1}\\sum_{i=1}^n x_i\\) and \\(\\bar{y}=n^{-1}\\sum_{i=1}^n y_i\\) are the sample means. Note that the sample correlation is a scale-free measure of the strength of linear dependence between the \\(x_i\\)’s and the \\(y_i\\)’s. In this chapter we investigate the multivariate analogue of this question. Suppose \\[ \\boldsymbol z_i =(\\boldsymbol x_i^\\top,\\boldsymbol y_i^\\top)^\\top, \\qquad i=1,\\ldots, n, \\] is a random sample of vectors. What is a sensible way to assess and describe the strength of the linear dependence between the \\(\\boldsymbol x_i\\) vectors and the \\(\\boldsymbol y_i\\) vectors? That is what this chapter is about. A key role is played by the singular valued decomposition (SVD) introduced in Result 2.13 in Chapter 2. Example 5.1 From time to time we will return to the Premier League example in this chapter. We shall treat \\(W\\) and \\(D\\), the number of wins and draws, respectively, as the \\(x\\)-variables; and \\(F\\) and \\(A\\), the number of goals for and against, will be treated as the \\(y\\)-variables. The number of losses, \\(L\\), is omitted as it provides no additional information when we know \\(W\\) and \\(D\\). A question we shall consider is: how strongly associated are the match outcome variables, \\(W\\) and \\(D\\), with the goals for and against variables, \\(F\\) and \\(A\\)? "],["5-1-canonical-correlation-analysis.html", "5.1 Canonical Correlation Analysis", " 5.1 Canonical Correlation Analysis Assume we are given a random sample of vectors \\[ \\boldsymbol z_i=(\\boldsymbol x_i^\\top , \\boldsymbol y_i^\\top )^\\top: \\, i=1,\\ldots, n, \\] where the \\(\\boldsymbol x_i\\) are \\(p \\times 1\\), the \\(\\boldsymbol y_i\\) are \\(q \\times 1\\) and, consequently, the \\(\\boldsymbol z_i\\) are \\((p+q)\\times 1\\). We are interested in determining the strength of linear association between the \\(\\boldsymbol x_i\\) vectors and the \\(\\boldsymbol y_i\\) vectors. Write \\[ \\bar{\\boldsymbol z}=n^{-1}\\sum_{i=1}^n \\boldsymbol z_i, \\qquad \\bar{\\boldsymbol x}=n^{-1} \\sum_{i=1}^n \\boldsymbol x_i \\qquad \\text{and} \\qquad \\bar{\\boldsymbol y}=n^{-1}\\sum_{i=1}^n \\boldsymbol y_i \\] for the sample mean vectors of the \\(\\boldsymbol z_i\\), \\(\\boldsymbol x_i\\) and \\(\\boldsymbol y_i\\) respectively. We formulate this task as an optimisation problem (cf. PCA). First, we introduce some notation. Let \\(\\boldsymbol S_{\\boldsymbol z\\boldsymbol z}\\) denote the sample covariance matrix of the \\(\\boldsymbol z_i\\), \\(i=1,\\ldots, n\\). Then \\(\\boldsymbol S_{\\boldsymbol z\\boldsymbol z}\\) can be written in block matrix form \\[ \\boldsymbol S_{\\boldsymbol z\\boldsymbol z}=\\left [\\begin{array}{cc} \\boldsymbol S_{\\boldsymbol x\\boldsymbol x} &amp; \\boldsymbol S_{\\boldsymbol x\\boldsymbol y}\\\\ \\boldsymbol S_{\\boldsymbol y\\boldsymbol x} &amp; \\boldsymbol S_{\\boldsymbol y\\boldsymbol y} \\end{array} \\right ], \\] where \\(\\boldsymbol S_{\\boldsymbol x\\boldsymbol x}\\) (\\(p \\times p\\)) is the sample covariance matrix of the \\(\\boldsymbol x_i\\), \\(\\boldsymbol S_{\\boldsymbol y\\boldsymbol y}\\) (\\(q \\times q\\)) is the sample covariance of the \\(\\boldsymbol y_i\\), and the cross-covariance matrices are given by \\[ \\stackrel{p \\times q}{\\boldsymbol S}_{\\boldsymbol x\\boldsymbol y}=n^{-1} \\sum_{i=1}^n (\\boldsymbol x_i -\\bar{\\boldsymbol x})(\\boldsymbol y_i-\\bar{\\boldsymbol y})^\\top \\qquad \\text{and} \\qquad \\stackrel{q \\times p}{\\boldsymbol S}_{\\boldsymbol y\\boldsymbol x}=\\boldsymbol S_{\\boldsymbol x\\boldsymbol y}^\\top. \\] Example 5.1 (continued). The relevant covariance matrix here is given in (4.3), but we need to delete the middle row and middle column because this relates to the variable \\(L\\), the number of losses, which we are omitting. So we are left with \\[\\begin{equation} \\boldsymbol S_{\\boldsymbol x\\boldsymbol x}=\\begin{pmatrix} 39.4 &amp; -8.3\\\\ -8.3 &amp; 8.1 \\end{pmatrix} , \\qquad \\boldsymbol S_{\\boldsymbol y\\boldsymbol y}=\\begin{pmatrix} 392.2 &amp; -208.7\\\\ -208.7 &amp; 230.9 \\end{pmatrix} \\tag{5.2} \\end{equation}\\] and \\[\\begin{equation} \\boldsymbol S_{\\boldsymbol x\\boldsymbol y}=\\boldsymbol S_{\\boldsymbol y\\boldsymbol x}^\\top = \\begin{pmatrix} 115.7 &amp; -81.9\\\\ -29.4 &amp; 6.0 \\end{pmatrix}. \\tag{5.3} \\end{equation}\\] We shall return to this example in a little while. We want to find the linear combination of the \\(x\\)-variables and the linear combination of the \\(y\\)-variables which is most highly correlated. One version of the optimisation problem we want to solve is: find non-zero vectors \\(\\stackrel{p \\times 1}{\\boldsymbol a}\\) and \\(\\stackrel{q \\times 1}{\\boldsymbol b}\\) which maximise the correlation coefficient \\[ r[\\boldsymbol a^\\top \\boldsymbol x,\\boldsymbol b^\\top \\boldsymbol y]=\\frac{\\boldsymbol a^\\top \\boldsymbol S_{\\boldsymbol x\\boldsymbol y}\\boldsymbol b}{(\\boldsymbol a^\\top \\boldsymbol S_{\\boldsymbol x\\boldsymbol x}\\boldsymbol a)^{1/2}(\\boldsymbol b^\\top \\boldsymbol S_{\\boldsymbol y\\boldsymbol y}\\boldsymbol b)^{1/2}}. \\] In other words: \\[\\begin{align} &amp;\\mbox{Find non-zero vectors }\\quad \\boldsymbol a\\;\\; (p \\times 1)\\mbox{ and } \\boldsymbol b\\;\\; (q \\times 1) \\nonumber\\\\ &amp;\\mbox{to maximise} \\qquad r[\\boldsymbol a^\\top \\boldsymbol x,\\boldsymbol b^\\top \\boldsymbol y], \\tag{5.4} \\end{align}\\] where \\(r[.,.]\\) is defined in (5.1). Intuitively, this makes sense, because we want to find the linear combination of the \\(x\\)-variables and the linear combination of the \\(y\\)-variables which are most highly correlated. However, note that for any \\(\\gamma&gt;0\\) and \\(\\delta&gt;0\\), \\[\\begin{equation} r[\\gamma\\boldsymbol a^\\top \\boldsymbol x, \\delta \\boldsymbol b^\\top \\boldsymbol y]= \\frac{\\gamma \\delta}{\\sqrt{\\gamma^2 \\delta^2}}r[\\boldsymbol a^\\top \\boldsymbol x,\\boldsymbol b^\\top \\boldsymbol y]=r[\\boldsymbol a^\\top \\boldsymbol x,\\boldsymbol b^\\top \\boldsymbol y], \\tag{5.5} \\end{equation}\\] i.e. \\(r[\\boldsymbol a^\\top \\boldsymbol x,\\boldsymbol b^\\top \\boldsymbol y]\\) is invariant with respect to positive scalar multiplication of \\(\\boldsymbol a\\) and \\(\\boldsymbol b\\). Consequently there will be an infinite number of solutions to this optimisation problem, because if \\(\\boldsymbol a\\) and \\(\\boldsymbol b\\) are solutions to optimization problem (5.4), then so are \\(\\gamma \\boldsymbol a\\) and \\(\\delta \\boldsymbol b\\), for any \\(\\gamma&gt;0\\) and \\(\\delta&gt;0\\). A more useful way to formulate this optimisation problem is the following: find \\[\\begin{equation} \\max_{\\boldsymbol a, \\boldsymbol b} \\boldsymbol a^\\top \\boldsymbol S_{\\boldsymbol x\\boldsymbol y}\\boldsymbol b \\tag{5.6} \\end{equation}\\] subject to the constraints \\[\\begin{equation} \\boldsymbol a^\\top \\boldsymbol S_{\\boldsymbol x\\boldsymbol x}\\boldsymbol a=1 \\qquad \\text{and} \\qquad \\boldsymbol b^\\top \\boldsymbol S_{\\boldsymbol y\\boldsymbol y}\\boldsymbol b=1. \\tag{5.7} \\end{equation}\\] Proposition 5.1 Assume that \\(\\boldsymbol S_{\\boldsymbol x\\boldsymbol x}\\) and \\(\\boldsymbol S_{\\boldsymbol y\\boldsymbol y}\\) both are non-singular. Then the following holds. If \\(\\boldsymbol a=\\hat{\\boldsymbol a}\\) and \\(\\boldsymbol b=\\hat{\\boldsymbol b}\\) maximise (5.4), then \\[ \\boldsymbol a=\\check{\\boldsymbol a}\\equiv\\hat{\\boldsymbol a}/(\\hat{\\boldsymbol a}^\\top \\boldsymbol S_{\\boldsymbol x\\boldsymbol x}\\hat{\\boldsymbol a})^{1/2} \\qquad \\text{and} \\qquad \\boldsymbol b=\\check{\\boldsymbol b}\\equiv \\hat{\\boldsymbol b}/(\\hat{\\boldsymbol b}^\\top \\boldsymbol S_{\\boldsymbol y\\boldsymbol y}\\hat{\\boldsymbol b})^{1/2} \\] maximise (5.6) subject to the constraints (5.7). Moreover, if \\(\\boldsymbol a=\\check{\\boldsymbol a}\\) and \\(\\boldsymbol b=\\check{\\boldsymbol b}\\) maximise (5.6) subject to constraints (5.7) then, for any \\(\\gamma&gt;0\\) and \\(\\delta&gt;0\\), \\(\\boldsymbol a=\\gamma \\check{\\boldsymbol a}\\) and \\(\\boldsymbol b=\\delta \\check{\\boldsymbol b}\\) maximise (5.4). The optimum solution to (5.6) and (5.7) is obtained when \\(\\boldsymbol a=\\boldsymbol S_{\\boldsymbol x\\boldsymbol x}^{-1/2}{\\mathbf q}_1\\) and \\(\\boldsymbol b=\\boldsymbol S_{\\boldsymbol y\\boldsymbol y}^{-1/2} {\\mathbf r}_1\\), where \\(\\boldsymbol S_{\\boldsymbol x\\boldsymbol x}^{-1/2} \\boldsymbol S_{\\boldsymbol x\\boldsymbol y}\\boldsymbol S_{\\boldsymbol y\\boldsymbol y}^{-1/2}\\) has SVD \\[\\begin{equation} \\boldsymbol A\\equiv \\boldsymbol S_{\\boldsymbol x\\boldsymbol x}^{-1/2}\\boldsymbol S_{\\boldsymbol x\\boldsymbol y}\\boldsymbol S_{\\boldsymbol y\\boldsymbol y}^{-1/2}= \\sum_{j=1}^t \\xi_j {\\mathbf q}_j {\\mathbf r}_j^\\top \\equiv {\\mathbf Q}{\\pmb \\Xi} {\\mathbf R}^\\top, \\tag{5.8} \\end{equation}\\] where \\(\\boldsymbol A\\) has rank \\(t\\) and \\(\\xi_1 \\geq \\cdots \\geq \\xi_t &gt;0\\). The maximum value of the correlation coefficient is given by the largest singular value \\(\\xi_1\\). Note: the matrix square roots \\(\\boldsymbol S_{\\boldsymbol x\\boldsymbol x}^{-1/2}\\) and \\(\\boldsymbol S_{\\boldsymbol y\\boldsymbol y}^{-1/2}\\) of \\(\\boldsymbol S_{\\boldsymbol x\\boldsymbol x}^{-1}\\) and \\(\\boldsymbol S_{\\boldsymbol y\\boldsymbol y}^{-1}\\), respectively, are defined using the definition of matrix square roots of symmetric non-negative definite matrices given in Chapter 2. Proof. (i) In (5.5) it was noted that, for \\(\\boldsymbol a\\neq {\\mathbf 0}_p\\) and \\(\\boldsymbol b\\neq {\\mathbf 0}_q\\), the expression for \\(r[\\boldsymbol a^\\top \\boldsymbol x, \\boldsymbol b^\\top \\boldsymbol y]\\) is invariant when we change \\(\\boldsymbol a\\) to \\(\\gamma \\boldsymbol a\\) and change \\(\\boldsymbol b\\) to \\(\\delta \\boldsymbol b\\), where \\(\\gamma&gt;0\\) and \\(\\delta&gt;0\\) are scalars, so the second statement in Result 4.1(i) follows imnmdeiately. Suppose now a solution to problem (5.4) is achieved when \\(\\boldsymbol a= \\hat{\\boldsymbol a}\\) and \\(\\boldsymbol b=\\hat{\\boldsymbol b}\\). Then, due to the invariance with respect to rescaling, the optimum is also achieved when \\(\\boldsymbol a=\\check{\\boldsymbol a}\\equiv\\hat{\\boldsymbol a}/(\\hat{\\boldsymbol a}^\\top \\boldsymbol S_{\\boldsymbol x\\boldsymbol x} \\hat{\\boldsymbol a})^{1/2}\\) and \\(\\boldsymbol b=\\check{\\boldsymbol b}\\equiv \\hat{\\boldsymbol b}/(\\hat{\\boldsymbol b}^\\top \\boldsymbol S_{\\boldsymbol y\\boldsymbol y} \\hat{\\boldsymbol b})^{1/2}\\). But by definition of \\(\\check{\\boldsymbol a}\\) and \\(\\check{\\boldsymbol b}\\), they satisfy the constraints (5.7) because \\[ \\check{\\boldsymbol a}^\\top \\boldsymbol S_{\\boldsymbol x\\boldsymbol x} \\check{\\boldsymbol a}=\\frac{\\hat{\\boldsymbol a}^\\top \\boldsymbol S_{\\boldsymbol x\\boldsymbol x}\\hat{\\boldsymbol a}}{\\left \\{ \\left (\\hat{\\boldsymbol a}^\\top \\boldsymbol S_{\\boldsymbol x\\boldsymbol x}\\hat{\\boldsymbol a}\\right )^{1/2}\\right \\}^2} =\\frac{\\hat{\\boldsymbol a}^\\top \\boldsymbol S_{\\boldsymbol x\\boldsymbol x}\\hat{\\boldsymbol a}}{\\hat{\\boldsymbol a}^\\top \\boldsymbol S_{\\boldsymbol x\\boldsymbol x}\\hat{\\boldsymbol a}}=1 \\] and, similarly, \\[ \\check{\\boldsymbol b}^\\top \\boldsymbol S_{\\boldsymbol y\\boldsymbol y} \\check{\\boldsymbol b}=\\frac{\\hat{\\boldsymbol b}^\\top \\boldsymbol S_{\\boldsymbol y\\boldsymbol y}\\hat{\\boldsymbol b}}{\\hat{\\boldsymbol b}^\\top \\boldsymbol S_{\\boldsymbol y\\boldsymbol y}\\hat{\\boldsymbol b}}=1. \\] So \\(\\boldsymbol a=\\check{\\boldsymbol a}\\) and \\(\\boldsymbol b=\\check{\\boldsymbol b}\\) maximises (5.6) subject to the constraints (5.7). (ii) &amp; (iii) We may write the constraints (5.7) as \\[ \\tilde{\\boldsymbol a}^\\top \\tilde{\\boldsymbol a}=1 \\qquad \\text{and} \\qquad \\tilde{\\boldsymbol b}^\\top \\tilde{\\boldsymbol b}=1 \\] where \\[ \\tilde{\\boldsymbol a}=\\boldsymbol S_{\\boldsymbol x\\boldsymbol x}^{1/2} \\boldsymbol a\\qquad \\text{and} \\qquad \\tilde{\\boldsymbol b}=\\boldsymbol S_{\\boldsymbol y\\boldsymbol y}^{1/2}\\boldsymbol b. \\] Recall that \\(\\boldsymbol S_{\\boldsymbol x\\boldsymbol x}\\) and \\(\\boldsymbol S_{\\boldsymbol y\\boldsymbol y}\\) are assumed to be non-singular. Then, using results from Chapter 2, \\(\\boldsymbol S_{\\boldsymbol x\\boldsymbol x}^{1/2}\\) and \\(\\boldsymbol S_{\\boldsymbol y\\boldsymbol y}^{1/2}\\) will also be non-singular, and so \\[ (\\boldsymbol S_{\\boldsymbol x\\boldsymbol x}^{1/2})^{-1}=\\boldsymbol S_{\\boldsymbol x\\boldsymbol x}^{-1/2} \\qquad \\text{and} \\qquad (\\boldsymbol S_{\\boldsymbol y\\boldsymbol y}^{1/2})^{-1}=\\boldsymbol S_{\\boldsymbol y\\boldsymbol y}^{-1/2} \\] both exist and so we may write \\[ \\boldsymbol a=\\boldsymbol S_{\\boldsymbol x\\boldsymbol x}^{-1/2}\\tilde{\\boldsymbol a} \\qquad \\text{and} \\qquad \\boldsymbol b=\\boldsymbol S_{\\boldsymbol y\\boldsymbol y}^{-1/2} \\tilde{\\boldsymbol b}, \\] and optimisation problem (5.6) subject to (5.7) becomes \\[ \\max_{\\tilde{\\boldsymbol a}, \\tilde{\\boldsymbol b}} \\tilde{\\boldsymbol a}^\\top \\boldsymbol S_{\\boldsymbol x\\boldsymbol x}^{-1/2}\\boldsymbol S_{\\boldsymbol x\\boldsymbol y}\\boldsymbol S_{\\boldsymbol y\\boldsymbol y}^{-1/2} \\tilde{\\boldsymbol b} \\] subject to \\[ \\vert \\vert \\tilde{\\boldsymbol a} \\vert \\vert =1 \\qquad \\text{and} \\qquad \\vert \\vert \\tilde{\\boldsymbol b}\\vert \\vert=1. \\] From the properties of the SVD, and in particular Result 2.15 in Chapter 2, we know that the maximum correlation is \\(\\xi_1\\). Moreover, using the SVD again, this is achieved when \\(\\tilde{\\boldsymbol a}={\\mathbf q}_1\\) and \\(\\tilde{\\boldsymbol b}={\\mathbf r}_1\\) or, equivalently, \\(\\boldsymbol a=\\boldsymbol S_{\\boldsymbol x\\boldsymbol x}^{-1/2}{\\mathbf q}_1\\) and \\(\\boldsymbol b=\\boldsymbol S_{\\boldsymbol y\\boldsymbol y}^{-1/2}{\\mathbf r}_1\\). Example 5.1 (continued) We now want to calculate the matrix \\(\\boldsymbol A\\) in (5.8) and then find its singular valued decomposition. We first need to find \\(\\boldsymbol S_{\\boldsymbol x\\boldsymbol x}^{-1/2}\\) and \\(\\boldsymbol S_{\\boldsymbol y\\boldsymbol y}^{-1/2}\\). Using R to do the calculations, we obtain the following: \\[\\begin{align*} \\boldsymbol S_{\\boldsymbol x\\boldsymbol x}&amp;=\\boldsymbol Q_{\\boldsymbol x} \\boldsymbol \\Lambda_{\\boldsymbol x} \\boldsymbol Q_{\\boldsymbol x}^\\top\\\\ &amp;= \\begin{pmatrix} -0.970 &amp; -0.241\\\\ 0.241 &amp; -0.970 \\end{pmatrix} \\begin{pmatrix} 41.46 &amp; 0 \\\\ 0 &amp; 6.04\\end{pmatrix} \\begin{pmatrix} -0.970 &amp; -0.241\\\\ 0.241 &amp; -0.970 \\end{pmatrix}^\\top, \\end{align*}\\] and so \\[\\begin{align*} \\boldsymbol S_{\\boldsymbol x\\boldsymbol x}^{-1/2}&amp;=\\boldsymbol Q_{\\boldsymbol x} \\boldsymbol \\Lambda_{\\boldsymbol x}^{-1/2} \\boldsymbol Q_{\\boldsymbol x}^\\top\\\\ &amp;= \\begin{pmatrix} -0.970 &amp; -0.241\\\\ 0.241 &amp; -0.970 \\end{pmatrix} \\begin{pmatrix} 41.46^{-1/2} &amp; 0 \\\\ 0 &amp; 6.04^{-1/2}\\end{pmatrix} \\begin{pmatrix} -0.970 &amp; -0.241\\\\ 0.241 &amp; -0.970 \\end{pmatrix}^\\top\\\\ &amp;=\\begin{pmatrix} 0.170 &amp; 0.059\\\\0.059 &amp; 0.392 \\end{pmatrix}; \\end{align*}\\] and, omitting details of the calculations this time, \\[ \\boldsymbol S_{\\boldsymbol y\\boldsymbol y}^{-1/2}=\\boldsymbol Q_{\\boldsymbol y} \\boldsymbol \\Lambda_{\\boldsymbol y}^{-1/2} \\boldsymbol Q_{\\boldsymbol y}^\\top=\\begin{pmatrix} 0.064 &amp; 0.030\\\\0.030 &amp; 0.086 \\end{pmatrix}. \\] Consequently, \\[\\begin{align*} \\boldsymbol A&amp;=\\boldsymbol S_{\\boldsymbol x\\boldsymbol x}^{-1/2}\\boldsymbol S_{\\boldsymbol x\\boldsymbol y}\\boldsymbol S_{\\boldsymbol y\\boldsymbol y}^{-1/2}\\\\ &amp;=\\begin{pmatrix} 0.170 &amp; 0.059\\\\0.059 &amp; 0.392 \\end{pmatrix} \\begin{pmatrix} 115.7 &amp; -81.9\\\\ -29.4 &amp; 6.0 \\end{pmatrix} \\begin{pmatrix} 0.064 &amp; 0.030\\\\0.030 &amp; 0.086 \\end{pmatrix}\\\\ &amp;=\\begin{pmatrix} 0.741 &amp; -0.628\\\\-0.374 &amp; -0.351\\end{pmatrix}. \\end{align*}\\] The SVD of \\(\\boldsymbol A\\) is given by \\[\\begin{align} \\boldsymbol A&amp;=\\boldsymbol Q{\\pmb \\Xi} \\boldsymbol R^\\top \\nonumber \\\\ &amp;=\\begin{pmatrix} -0.997 &amp; 0.082\\\\ 0.082 &amp; 0.997 \\end{pmatrix} \\begin{pmatrix} 0.974 &amp; 0 \\\\0 &amp; 0.508 \\end{pmatrix} \\begin{pmatrix} -0.790 &amp; -0.613\\\\ 0.613 &amp; -0.790 \\end{pmatrix}^\\top. \\tag{5.9} \\end{align}\\] So the 1st CC coefficient is \\(0.974\\), which is close to its maximum value of \\(1\\). The 1st CC weight vectors are given by \\[\\begin{align*} \\boldsymbol a_1&amp;=\\boldsymbol S_{\\boldsymbol x\\boldsymbol x}^{-1/2}\\boldsymbol q_1\\\\ &amp;=\\begin{pmatrix} 0.170 &amp; 0.059\\\\0.059 &amp; 0.392 \\end{pmatrix} \\begin{pmatrix} -0.997 \\\\ 0.082\\end{pmatrix}\\\\ &amp;=\\begin{pmatrix} -0.165, \\\\ - 0.027 \\end{pmatrix}. \\end{align*}\\] Similar calculations show that \\[ \\boldsymbol b_1=\\boldsymbol S_{\\boldsymbol y\\boldsymbol y}^{-1/2}\\boldsymbol r_1=\\begin{pmatrix} -0.032 \\\\ 0.029\\end{pmatrix}. \\] In order to make interpretation easier: We change \\(\\boldsymbol a_1\\) to \\(-\\boldsymbol a_1\\) and \\(\\boldsymbol b_1\\) to \\(-\\boldsymbol b_1\\). [This entails changing \\(\\boldsymbol q_1\\) to \\(-\\boldsymbol q_1\\) and \\(\\boldsymbol r_1\\) to \\(-\\boldsymbol r_1\\); note that, provided we change the sign of both \\(\\boldsymbol q_1\\) and \\(\\boldsymbol r_1\\), we do not change the matrix \\(\\boldsymbol A\\).] We rescale \\(\\boldsymbol a_1\\) and \\(\\boldsymbol b_1\\) so that they are unit vectors. This leads to the standardised 1st CC weight vectors \\[ \\boldsymbol a_1=\\begin{pmatrix} 0.987\\\\0.160 \\end{pmatrix} \\qquad \\text{and} \\qquad \\begin{pmatrix} 0.743\\\\ -0.670\\end{pmatrix} \\] and the 1st CC variables, obtained by using these weights, are \\[ \\eta_1 =0.987*(W-\\bar{W}) +0.160*(D -\\bar{D}) \\] and \\[ \\psi_1 = 0.743*(F-\\bar{F}) - 0.670*(A-\\bar{A}), \\] where the bars are used to denote sample means. We can see that \\(\\psi_1\\) is measuring something similar to goal difference \\(F-A\\), as usually defined, but it gives slightly higher weight to goals scored than goals conceded (\\(0.743\\) versus \\(0.670\\)). It is also seen that \\(\\eta_1\\) is measuring something similar to number of points \\(3*W+D\\), as usually defined, but the ratio of points for a win to points for a draw is somewhat higher, at around 6:1, as opposed to the usual ratio 3:1. "],["5-2-the-full-set-of-canonical-correlations.html", "5.2 The full set of canonical correlations", " 5.2 The full set of canonical correlations Let us first recap what we did in the previous section: we found the choices linear combinations of the \\(x\\)-variables and linear combinations of \\(y\\)-variables which maximise the correlation, and expressed the answer in terms of quantities which arise in the SVD of \\(\\boldsymbol A\\), where \\[ \\boldsymbol A\\equiv \\boldsymbol S_{\\boldsymbol x\\boldsymbol x}^{-1/2} \\boldsymbol S_{\\boldsymbol x\\boldsymbol y}\\boldsymbol S_{\\boldsymbol y\\boldsymbol y}^{-1/2}=\\boldsymbol Q{\\pmb \\Xi} \\boldsymbol R^\\top=\\sum_{j=1}^t \\xi_j \\boldsymbol q_j \\boldsymbol r_j^\\top, \\] with \\(t\\) the rank of \\(\\boldsymbol A\\), which in most examples is given by \\(t=\\min(p,q)\\), and singular values \\(\\xi_1 \\geq \\xi_2 \\geq \\cdots \\geq \\xi_t&gt;0\\). Specifically, the maximum value of the correlation is \\(\\xi_1\\), the optimal weights for the \\(x\\)-variables are given by \\(\\boldsymbol a=\\boldsymbol S_{\\boldsymbol x\\boldsymbol x}^{-1/2}\\boldsymbol q_1=\\boldsymbol a_1\\), say, and the optimal weights for the \\(y\\)-varables are given by \\(\\boldsymbol b=\\boldsymbol S_{\\boldsymbol y\\boldsymbol y}^{-1/2}\\boldsymbol r_1 = \\boldsymbol b_1\\), say. Can we repeat this process, as we did with PCA? Yes, we can. To obtain the second canonical correlation coefficient, plus the associated sets of weights, we need to solve the following optimisation problem: \\[\\begin{equation} \\max_{\\boldsymbol a,\\, \\boldsymbol b} \\boldsymbol a^\\top \\boldsymbol S_{\\boldsymbol x\\boldsymbol y}\\boldsymbol b \\tag{5.10} \\end{equation}\\] subject to the constraints \\[\\begin{equation} \\boldsymbol a^\\top \\boldsymbol S_{\\boldsymbol x\\boldsymbol x}\\boldsymbol a= 1, \\qquad \\boldsymbol b^\\top \\boldsymbol S_{\\boldsymbol y\\boldsymbol y}\\boldsymbol b=1, \\tag{5.11} \\end{equation}\\] \\[\\begin{equation} \\boldsymbol a_1^\\top \\boldsymbol S_{\\boldsymbol x\\boldsymbol x} \\boldsymbol a=0 \\qquad \\text{and} \\qquad \\boldsymbol b_1^\\top \\boldsymbol S_{\\boldsymbol y\\boldsymbol y}\\boldsymbol b=0. \\tag{5.12} \\end{equation}\\] Note that maximising (5.10) subject to (5.11) is very similar to the optimisation problem (5.6) and (5.7) considered in the previous section. What is new are the constraints (5.12), which take into account that we have already found the first canonical correlation. If for \\(j=1,2\\) we write \\(\\tilde{\\boldsymbol a}_j =\\boldsymbol S_{\\boldsymbol x\\boldsymbol x}^{1/2} \\boldsymbol a_j\\) and \\(\\tilde{\\boldsymbol b}_j=\\boldsymbol S_{\\boldsymbol y\\boldsymbol y}^{1/2} \\boldsymbol b_j\\), then it is seen from (5.12) that \\[ \\tilde{\\boldsymbol a}_1^\\top \\tilde{\\boldsymbol a}_2=0 \\qquad \\text{and} \\qquad \\tilde{\\boldsymbol b}_1^\\top \\tilde{\\boldsymbol b}_2=0. \\] Consequently, we may view constraints (5.12) as corresponding to orthogonality constraints (cf. PCA) in modified coordinate systems. We now discuss the optimisation of (5.10), (5.11) and (5.12). At first glance it looks complex. However, using arguments very similar to those used to prove Result 2.15 in Chapter 2, we may deduce the following: The maximum of (5.10) subject to constraints (5.11) and (5.12) is equal to \\(\\xi_2\\), the second largest singular value of \\(\\boldsymbol A\\). The optimal weights for the \\(x\\)-variables for the second canonical correlation are given by \\(\\boldsymbol a_2=\\boldsymbol S_{\\boldsymbol x\\boldsymbol x}^{-1/2} \\boldsymbol q_2\\). The optimal weights for the \\(y\\)-variables for the second canonical correlation are given by \\(\\boldsymbol b_2=\\boldsymbol S_{\\boldsymbol y\\boldsymbol y}^{-1/2}\\boldsymbol r_2\\). Consider now the general case of the \\(k\\)th canonical correlation where \\(2 \\leq k \\leq t\\). In this case we replace (5.11) and (5.12) by, respectively, (5.13) and (5.14) below, where \\[\\begin{equation} \\boldsymbol a^\\top \\boldsymbol S_{\\boldsymbol x\\boldsymbol x}\\boldsymbol a= 1, \\qquad \\boldsymbol b^\\top \\boldsymbol S_{\\boldsymbol y\\boldsymbol y}\\boldsymbol b=1, \\tag{5.13} \\end{equation}\\] \\[\\begin{equation} \\boldsymbol a_j^\\top \\boldsymbol S_{\\boldsymbol x\\boldsymbol x} \\boldsymbol a=0 \\qquad \\text{and} \\qquad \\boldsymbol b_j^\\top \\boldsymbol S_{\\boldsymbol y\\boldsymbol y}\\boldsymbol b=0, \\qquad j=1, \\ldots , k-1. \\tag{5.14} \\end{equation}\\] Then the optimisation problem is \\[\\begin{equation} \\max_{\\boldsymbol a, \\, \\boldsymbol b} \\boldsymbol a^\\top \\boldsymbol S_{\\boldsymbol x\\boldsymbol y}\\boldsymbol b \\tag{5.15} \\end{equation}\\] subject to constraints (5.13) and (5.14). The solution in the general case is as follows. The maximum of (5.15) subject to constraints (5.13) and (5.14) is equal to \\(\\xi_k\\), the \\(k\\)th largest singular value of \\(\\boldsymbol A\\). The optimal weights for the \\(x\\)-variables for the \\(k\\)th canonical correlation are given by \\(\\boldsymbol a_k=\\boldsymbol S_{\\boldsymbol x\\boldsymbol x}^{-1/2} \\boldsymbol q_k\\). The optimal weights for the \\(y\\)-variables for the \\(k\\)th canonical correlation are given by \\(\\boldsymbol b_k=\\boldsymbol S_{\\boldsymbol y\\boldsymbol y}^{-1/2}\\boldsymbol r_k\\). Terminology: we call \\(\\boldsymbol a_k\\) and \\(\\boldsymbol b_k\\) the \\(k\\)th cc (weight) vectors for the \\(x\\)-variables and \\(y\\) variables, respectively. We call \\(\\eta_{ik}=\\boldsymbol a_k^\\top (\\boldsymbol x_i - \\bar{\\boldsymbol x})\\) and \\(\\psi_{ik}=\\boldsymbol b_k^\\top (\\boldsymbol y_i -\\bar{\\boldsymbol y})\\), \\(i=1, \\ldots , n\\), the \\(k\\)th cc scores for the \\(x\\)-variables and the \\(y\\)-variables, respectively. Define the CC score vectors \\({\\pmb \\eta}_k=(\\eta_{1k}, \\ldots , \\eta_{nk})^\\top\\) and \\({\\pmb \\psi}_{k}=(\\psi_{1k}, \\ldots , \\psi_{nk})^\\top\\). Then we have the following result. Proposition 5.2 Assume that \\(\\boldsymbol S_{\\boldsymbol x\\boldsymbol x}\\) and \\(\\boldsymbol S_{\\boldsymbol y\\boldsymbol y}\\) both have full rank. Then for \\(1 \\leq k,\\ell \\leq t\\), \\[ r[\\eta_k, \\psi_{\\ell}]=\\begin{cases} \\xi_k &amp;\\text{if} \\quad k=\\ell\\\\ 0 &amp; \\text{if} \\quad k \\neq \\ell, \\end{cases} \\] where \\(t\\) is the rank of \\(\\boldsymbol A=\\boldsymbol S_{\\boldsymbol x\\boldsymbol x}^{-1/2}\\boldsymbol S_{\\boldsymbol x\\boldsymbol y} \\boldsymbol S_{\\boldsymbol y\\boldsymbol y}^{-1/2}\\) and \\(\\xi_1 \\geq \\xi_2 \\geq \\cdots \\xi_t &gt; 0\\) are the strictly positive singular values of \\(\\boldsymbol A\\). Example 5.1 (continued) From (5.9), it is seen that the 2nd CC coefficient is given by \\(\\xi_2=0.508\\). So the correlation between the second pair of CC variables is a lot smaller than the 1st CC coefficient, though still appreciably different from \\(0\\). We now calculate the 2nd CC weight vectors: \\[ \\boldsymbol a_2=\\boldsymbol S_{\\boldsymbol x\\boldsymbol x}^{-1/2} \\boldsymbol q_2 = \\begin{pmatrix} 0.073 \\\\ 0.396 \\end{pmatrix} \\qquad \\text{and} \\qquad \\boldsymbol b_2=\\boldsymbol S_{\\boldsymbol y\\boldsymbol y}^{-1/2}\\boldsymbol r_2=-\\begin{pmatrix}0.062\\\\ 0.086 \\end{pmatrix}, \\] with standardised version (without the sign changes this time) \\[ \\boldsymbol a_2=\\begin{pmatrix}0.181 \\\\ 0.984 \\end{pmatrix} \\qquad \\text{and} \\qquad \\boldsymbol b_2=-\\begin{pmatrix}0.589 \\\\ 0.808 \\end{pmatrix}, \\] and new variables \\[ \\eta_2=0.181*(W-\\bar{W}) +0.984*(D -\\bar{D}) \\] and \\[ \\psi_2=-\\{0.589*(F-\\bar{F})+0.808*(A-\\bar{A})\\}. \\] Note that, to a good approximation, \\(\\eta_2\\) is measuring something similar to the number of draws and, approximately, \\(\\psi_2\\) is something related to the negative of total number of goals in a team’s games. So large \\(\\psi_2\\) means relatively few goals in a team’s games, and small (i.e. large negative) \\(\\psi_2\\) means a relatively large number of goals in a team’s games. Interpretation of the 2nd CC: teams that have a lot of draws tend to be in low-scoring games and/or teams that have few draws tend to be in high-scoring games. "],["5-3-connection-with-linear-regression-when-q1.html", "5.3 Connection with linear regression when \\(q=1\\)", " 5.3 Connection with linear regression when \\(q=1\\) Although CCA analysis is clearly a different technique to linear regression, it turns out that when either \\(p=1\\) or \\(q=1\\), there is a close connection between the two approaches. Without loss of generality we assume that \\(q=1\\) and \\(p&gt;1\\). Hence there is only a single \\(y\\)-variable but we still have \\(p&gt;1\\) \\(x\\)-variables. We also make the following assumptions: The \\(\\boldsymbol x_i\\) have been centred so that \\(\\bar{\\boldsymbol x}={\\mathbf 0}_p\\), the zero vector. The covariance matrix for the \\(x\\)-variables, \\(\\boldsymbol S_{\\boldsymbol x\\boldsymbol x}\\), has full rank \\(p\\). Both of these are weak assumptions in the multiple linear regression context. Since \\(q=1\\), \\[ \\boldsymbol A=\\boldsymbol S_{\\boldsymbol x\\boldsymbol x}^{-1/2} \\boldsymbol S_{\\boldsymbol xy}\\boldsymbol S_{yy}^{-1/2} \\] is a \\(p \\times 1\\) vector. Consequently, in this rather special case, the SVD tells us that \\[ \\boldsymbol A=\\xi_1 \\boldsymbol q_1, \\] where \\[ \\xi_1=\\vert \\vert \\boldsymbol A\\vert \\vert \\qquad \\text{and} \\qquad \\boldsymbol q_1=\\boldsymbol A/\\vert \\vert \\boldsymbol A\\vert \\vert=\\tilde{\\boldsymbol a}, \\] and \\(\\tilde{\\boldsymbol a}=\\boldsymbol S_{\\boldsymbol x\\boldsymbol x}^{1/2} \\boldsymbol a\\). Consequently, \\[\\begin{align*} \\boldsymbol a&amp;=\\boldsymbol S_{\\boldsymbol x\\boldsymbol x}^{-1/2}\\boldsymbol q_1\\\\ &amp;=\\boldsymbol S_{\\boldsymbol x\\boldsymbol x}^{-1/2} \\frac{1}{\\vert \\vert \\boldsymbol S_{\\boldsymbol x\\boldsymbol x}^{-1/2}\\boldsymbol S_{\\boldsymbol xy}S_{yy}^{-1/2}\\vert \\vert}\\boldsymbol S_{\\boldsymbol x\\boldsymbol x}^{-1/2}\\boldsymbol S_{\\boldsymbol x\\boldsymbol y}S_{yy}^{-1/2}\\\\ &amp;=\\frac{1}{\\vert \\vert \\boldsymbol S_{\\boldsymbol x\\boldsymbol x}^{-1/2}\\boldsymbol S_{\\boldsymbol xy}\\vert \\vert}\\boldsymbol S_{\\boldsymbol x\\boldsymbol x}^{-1/2}\\boldsymbol S_{\\boldsymbol x\\boldsymbol x}^{-1/2}\\boldsymbol S_{\\boldsymbol xy}\\\\ &amp;=\\frac{1}{\\vert \\vert \\boldsymbol S_{\\boldsymbol x\\boldsymbol x}^{-1/2}\\boldsymbol S_{\\boldsymbol xy}\\vert \\vert}\\boldsymbol S_{\\boldsymbol x\\boldsymbol x}^{-1}\\boldsymbol S_{\\boldsymbol xy}. \\end{align*}\\] But since \\(\\bar{\\boldsymbol x}={\\mathbf 0}_p\\) and \\(\\boldsymbol S\\) has full rank by the assumptions above, it follows that \\[ n\\boldsymbol S_{\\boldsymbol x\\boldsymbol x} =\\sum_{i=1}^n \\boldsymbol x_i \\boldsymbol x_i^\\top =\\boldsymbol X^\\top \\boldsymbol X \\] and \\[ n\\boldsymbol S_{\\boldsymbol x\\boldsymbol y}=\\sum_{i=1}^n y_i \\boldsymbol x_i=\\boldsymbol X^\\top \\boldsymbol y, \\] where \\(\\boldsymbol y=(y_1, \\ldots ,y_n)^\\top\\) is the \\(n \\times 1\\) data matrix for the \\(y\\)-variable and \\(\\boldsymbol X=[\\boldsymbol x_1, \\ldots , \\boldsymbol x_n]^\\top\\) is the data matrix for the \\(x\\)-variables. Consequently, the optimal \\(\\boldsymbol a\\) is a scalar multiple of \\[ \\boldsymbol S_{\\boldsymbol x\\boldsymbol x}^{-1}\\boldsymbol S_{\\boldsymbol xy}=\\left ( \\boldsymbol X^\\top \\boldsymbol X\\right )^{-1} \\boldsymbol X^\\top \\boldsymbol y=\\hat{\\pmb \\beta}, \\] say, which is the classical expression for least squares estimator. Therefore the least squares estimator \\(\\hat{\\pmb \\beta}\\) solves (5.4). However, it does not usually solve the optimisation problem defined by problems (5.6) and (5.7) because typically it will not be the case that \\(\\hat{\\pmb \\beta}^\\top \\boldsymbol S_{\\boldsymbol x\\boldsymbol x}\\hat{\\pmb \\beta}=1\\), so that (5.7) will not be satisfied. "],["5-4-population-cca.html", "5.4 Population CCA", " 5.4 Population CCA So far in this chapter we have based CCA on the sample covariance matrix \\[ \\boldsymbol S_{\\boldsymbol z\\boldsymbol z}=\\left [\\begin{array}{cc} \\boldsymbol S_{\\boldsymbol x\\boldsymbol x} &amp; \\boldsymbol S_{\\boldsymbol x\\boldsymbol y}\\\\ \\boldsymbol S_{\\boldsymbol y\\boldsymbol x} &amp; \\boldsymbol S_{\\boldsymbol y\\boldsymbol y} \\end{array} \\right ], \\] However, just as there is a population analogue of PCA, so there is a population analogue of CCA. Given random vectors \\(\\stackrel{p \\times 1}{\\boldsymbol x}\\) and \\(\\stackrel{q \\times 1}{\\boldsymbol y}\\), define the random vector \\(\\boldsymbol z=(\\boldsymbol x^\\top, \\boldsymbol y^\\top)^\\top\\) with population covariance matrix \\[ \\text{Var}(\\boldsymbol z)=\\boldsymbol \\Sigma_{\\boldsymbol z\\boldsymbol z}=\\left [\\begin{array}{cc} \\boldsymbol \\Sigma_{\\boldsymbol x\\boldsymbol x} &amp; \\boldsymbol \\Sigma_{\\boldsymbol x\\boldsymbol y}\\\\ \\boldsymbol \\Sigma_{\\boldsymbol y\\boldsymbol x} &amp; \\boldsymbol \\Sigma_{\\boldsymbol y\\boldsymbol y} \\end{array} \\right ]. \\] Then, by analogy with what we have seen in the sample CCA, the population CCA is based on the matrix \\[ \\check{\\boldsymbol A}=\\boldsymbol \\Sigma_{\\boldsymbol x\\boldsymbol x}^{-1/2}\\boldsymbol \\Sigma_{\\boldsymbol x\\boldsymbol y}\\boldsymbol \\Sigma_{\\boldsymbol y\\boldsymbol y}^{-1/2}, \\] where, as in 3.4, the check symbol has been used above and below to indicate population quantities. If \\(\\check{\\boldsymbol A}\\) has SVD \\[ \\check{\\boldsymbol A}=\\sum_{j=1}^t \\check{\\xi}_j\\check {\\mathbf q}_j \\check{\\mathbf r}_j^\\top \\equiv \\check{\\mathbf Q}\\check{\\pmb \\Xi} \\check{\\mathbf R}^\\top, \\] where \\(\\check{\\xi}_1 \\geq \\cdots \\geq \\check{\\xi}_t \\geq 0\\) and \\(t=\\min(p,q)\\), and the \\(\\check{\\boldsymbol q}_j\\) and \\(\\check{\\mathbf r}_j\\) are unit vectors, then the first population CC coefficient is given by \\(\\check{\\xi}_1\\), and the associated weights are given by \\[ \\check{\\boldsymbol a}=\\boldsymbol \\Sigma_{\\boldsymbol x\\boldsymbol x}^{-1/2}\\check{\\boldsymbol q}_1=\\check{\\boldsymbol a}_1 \\qquad \\text{and} \\qquad \\check{\\boldsymbol b}=\\boldsymbol \\Sigma_{\\boldsymbol y\\boldsymbol y}^{-1/2}\\check{\\mathbf r}_1=\\check{\\boldsymbol b}_1. \\] The full set of population CC weight vectors is given by \\[ \\check{\\boldsymbol a}_j =\\boldsymbol \\Sigma_{\\boldsymbol x\\boldsymbol x}^{-1/2}\\check{\\boldsymbol q}_j \\qquad \\text{and} \\qquad \\check{\\boldsymbol b}_j=\\boldsymbol \\Sigma_{\\boldsymbol y\\boldsymbol y}^{-1/2}\\check{\\mathbf r}_1, \\qquad , j=1, \\ldots , t, \\] and the \\(j\\)th population CC coefficient is given by \\(\\check{\\xi}_j\\). "],["5-5-invarianceequivariance-properties-of-cca.html", "5.5 Invariance/equivariance properties of CCA", " 5.5 Invariance/equivariance properties of CCA Suppose we apply orthogonal transformations and translations to the \\(\\boldsymbol x_i\\) and the \\(\\boldsymbol y_i\\) of the form \\[\\begin{equation} {\\mathbf h}_i={\\mathbf T}\\boldsymbol x_i + {\\pmb \\mu} \\qquad \\text{and} \\qquad {\\mathbf k}_i={\\mathbf V}\\boldsymbol y_i +{\\pmb \\eta}, \\qquad i=1,\\ldots , n, \\tag{5.16} \\end{equation}\\] where \\(\\mathbf T\\) (\\(p \\times p\\)) and \\(\\mathbf V\\) (\\(q \\times q\\)) are orthogonal matrices, and \\(\\pmb \\mu\\) (\\(p \\times 1\\)) and \\(\\pmb \\eta\\) (\\(q \\times 1\\)) are fixed vectors. How do these transformations affect the CC analysis? First of all, since the CCA depends only on sample covariance matrices, it follows that the translation vectors \\(\\pmb \\mu\\) and \\(\\pmb \\eta\\) have no effect on the analysis, so we can ignore \\(\\pmb \\mu\\) and \\(\\pmb \\eta\\), and without loss of generality we shall set each to be the zero vector. As seen in the previous section, the CCA in the original coordinates depends on \\[\\begin{equation} \\boldsymbol A\\equiv \\boldsymbol A_{\\boldsymbol x\\boldsymbol y}=\\boldsymbol S_{\\boldsymbol x\\boldsymbol x}^{-1/2}\\boldsymbol S_{\\boldsymbol x\\boldsymbol y}\\boldsymbol S_{\\boldsymbol y\\boldsymbol y}^{-1/2}. \\tag{5.17} \\end{equation}\\] In the new coordinates we have \\[ \\tilde{\\boldsymbol S}_{\\mathbf h h}={\\mathbf T} \\boldsymbol S_{\\boldsymbol x\\boldsymbol x}{\\mathbf T}^\\top, \\qquad \\tilde{\\boldsymbol S}_{\\mathbf kk}={\\mathbf V}\\boldsymbol S_{\\boldsymbol y\\boldsymbol y}{\\mathbf V}^\\top, \\] \\[ \\tilde{\\boldsymbol S}_{\\mathbf hk}={\\mathbf T}\\boldsymbol S_{\\boldsymbol x\\boldsymbol y}{\\mathbf V}^\\top \\qquad \\text{and} \\qquad \\tilde{\\boldsymbol S}_{\\mathbf kh}={\\mathbf V}\\boldsymbol S_{\\boldsymbol y\\boldsymbol x}{\\mathbf T}^\\top=\\boldsymbol S_{\\mathbf h k}^\\top, \\] where here and below, a tilde above a symbol is used to indicate that the corresponding term is defined in terms of the new \\(\\boldsymbol h\\), \\(\\boldsymbol k\\) coordinates, rather than the old \\(\\boldsymbol x\\), \\(\\boldsymbol y\\) coordinates. Moreover, due to the fact that \\(\\mathbf T\\) and \\(\\mathbf V\\) are orthogonal, \\[ \\tilde{\\boldsymbol S}_{\\mathbf hh}^{ 1/2}={\\mathbf T}\\boldsymbol S_{\\boldsymbol x\\boldsymbol x}^{ 1/2}{\\mathbf T}^\\top, \\qquad \\tilde{\\boldsymbol S}_{\\mathbf hh}^{ -1/2}={\\mathbf T}\\boldsymbol S_{\\boldsymbol x\\boldsymbol x}^{ -1/2}{\\mathbf T}^\\top \\] \\[ \\tilde{\\boldsymbol S}_{\\mathbf kk}^{ 1/2}={\\mathbf V}\\boldsymbol S_{\\boldsymbol y\\boldsymbol y}^{ 1/2}{\\mathbf V}^\\top \\qquad \\text{and} \\qquad \\tilde{\\boldsymbol S}_{\\mathbf kk}^{ -1/2}={\\mathbf V}\\boldsymbol S_{\\boldsymbol y\\boldsymbol y}^{- 1/2}{\\mathbf V}^\\top. \\] The analogue of (5.17) in the new coordinates is given by \\[\\begin{align*} \\tilde{\\boldsymbol A}_{\\mathbf h k}&amp;=\\tilde{\\boldsymbol S}_{\\mathbf hh}^{-1/2}\\tilde{\\boldsymbol S}_{\\mathbf h k}\\tilde{\\boldsymbol S}_{\\mathbf kk}^{-1/2}\\\\ &amp;={\\mathbf T} \\boldsymbol S_{\\boldsymbol x\\boldsymbol x}^{-1/2}{\\mathbf T}^\\top {\\mathbf T}\\boldsymbol S_{\\boldsymbol x\\boldsymbol y}{\\mathbf V}^\\top {\\mathbf V}\\boldsymbol S_{\\boldsymbol y\\boldsymbol y}^{-1/2}{\\mathbf V}^\\top\\\\ &amp;={\\mathbf T}\\boldsymbol S_{\\boldsymbol x\\boldsymbol x}^{-1/2}\\boldsymbol S_{\\boldsymbol x\\boldsymbol y}\\boldsymbol S_{\\boldsymbol y\\boldsymbol y}^{-1/2}{\\mathbf V}^\\top\\\\ &amp;={\\mathbf T} \\boldsymbol A_{\\boldsymbol x\\boldsymbol y}{\\mathbf V}^\\top. \\end{align*}\\] So, again using the fact that \\(\\mathbf T\\) and \\(\\mathbf V\\) are orthogonal matrices, if \\(\\boldsymbol A_{\\boldsymbol x\\boldsymbol y}\\) has SVD \\(\\sum_{j=1}^t \\xi_j {\\mathbf q}_j {\\mathbf r}_j^\\top\\), then \\(\\tilde{\\boldsymbol A}_{\\mathbf hk}\\) has SVD \\[\\begin{align*} \\tilde{\\boldsymbol A}_{\\mathbf hk}&amp;={\\mathbf T }\\boldsymbol A_{\\boldsymbol x\\boldsymbol y}{\\mathbf V}^\\top ={\\mathbf T} \\left ( \\sum_{j=1}^t \\xi_j {\\mathbf q}_j {\\mathbf r}_j^\\top \\right){\\mathbf V}^\\top\\\\ &amp;=\\sum_{j=1}^t \\xi_j {\\mathbf T}{\\mathbf q}_j {\\mathbf r}_j^\\top {\\mathbf V}^\\top =\\sum_{j=1}^t \\xi_j \\left ( {\\mathbf T} {\\mathbf q}_j \\right )\\left ({\\mathbf V}{\\mathbf r}_j \\right )^\\top =\\sum_{j=1}^t \\xi_j \\tilde{\\boldsymbol q}_j \\tilde{\\mathbf r}_j^\\top, \\end{align*}\\] where, for \\(j=1, \\ldots,t\\), the \\(\\tilde{\\boldsymbol q}_j={\\mathbf T}\\boldsymbol q_j\\) are mutually orthogonal unit vectors, and the \\(\\tilde{\\mathbf r}_j={\\mathbf V}{\\mathbf r}_j\\) are also mutually orthogonal unit vectors. Consequently, \\(\\tilde{\\boldsymbol A}_{\\mathbf h k}\\) has the same singular values as \\(\\boldsymbol A_{\\boldsymbol x\\boldsymbol y}\\), namely \\(\\xi_1, \\ldots , \\xi_t\\) in both cases, and so the canonical correlation coefficients are invariant with respect to the transformations (5.16). Moreover, since the optimal linear combinations for the \\(j\\)th CC in the original coordinates are given by \\(\\boldsymbol a_j =\\boldsymbol S_{\\boldsymbol x\\boldsymbol x}^{-1/2}{\\mathbf q}_j\\) and \\(\\boldsymbol b_j=\\boldsymbol S_{\\boldsymbol y\\boldsymbol y}^{-1/2}{\\mathbf r}_j\\), the optimal linear combinations in the new coordinates are given by \\[\\begin{align*} \\tilde{\\boldsymbol a}_{j}&amp;=\\boldsymbol S_{\\mathbf hh}^{-1/2}{\\mathbf T}{\\mathbf q}_j\\\\ &amp;={\\mathbf T}\\boldsymbol S_{\\boldsymbol x\\boldsymbol x}^{-1/2}{\\mathbf T}^\\top {\\mathbf T}{\\mathbf q}_j\\\\ &amp;={\\mathbf T}\\boldsymbol S_{\\boldsymbol x\\boldsymbol x}^{-1/2}{\\mathbf q}_j \\\\ &amp;={\\mathbf T}\\boldsymbol a_{j}, \\end{align*}\\] and a similar argument shows that \\(\\tilde{\\boldsymbol b}_{j}={\\mathbf V}\\boldsymbol b_{j}\\). So under transformations (5.16), the optimal vectors \\(\\boldsymbol a_{j}\\) and \\(\\boldsymbol b_{j}\\) transform in an equivariant manner to \\(\\tilde{\\boldsymbol a}_{j}\\) and \\(\\tilde{\\boldsymbol b}_{j}\\), respectively, \\(j=1, \\ldots , t\\). If either of \\(\\mathbf T\\) or \\(\\mathbf V\\) in (5.16) is not an orthogonal matrix then the singular values are not invariant and the cc vectors do not transform in an equivariant manner. "],["5-6-testing-for-zero-canonical-correlation-coefficients.html", "5.6 Testing for zero canonical correlation coefficients", " 5.6 Testing for zero canonical correlation coefficients So far in Part II of this module we have not considered formal statistical inference (e.g. hypothesis testing, construction of confidence regions). Inference in various multivariate settings is considered in Part III. However, before moving on, we briefly explain how to perform tests for zero correlations in the CCA setting, under the assumption that the \\(\\boldsymbol z_i = (\\boldsymbol x_i^\\top , \\boldsymbol y_i^\\top)^\\top\\) are IID multivariate normal. As previously, suppose that the \\(\\boldsymbol x_i\\) are \\(p \\times 1\\) vectors and the \\(\\boldsymbol y_i\\) are \\(q \\times 1\\) vectors and the sample size, i.e. the number of \\(\\boldsymbol z_i\\) vectors, is \\(n\\). Let \\(\\boldsymbol \\Sigma_{\\boldsymbol x\\boldsymbol y} =\\text{Cov}(\\boldsymbol x,\\boldsymbol y)\\) denote the population cross-covariance matrix as before and consider the null hypothesis \\[ H_0: \\, \\boldsymbol \\Sigma_{\\boldsymbol x\\boldsymbol y}={\\mathbf 0}_{p,q}, \\] i.e. \\(\\boldsymbol \\Sigma_{\\boldsymbol x\\boldsymbol y}\\) is the \\(p \\times q\\) matrix of zeros. Let \\(H_A\\) denote the general alternative \\[ H_A:\\, \\boldsymbol \\Sigma_{\\boldsymbol x\\boldsymbol y} \\quad *unrestricted*. \\] Then the large-sample log-likelihood ratio test statistic for testing \\(H_0\\) versus \\(H_A\\) is as follows: \\[ W_0=-\\left \\{n-\\frac{1}{2}(p+q+3) \\right \\}\\sum_{j=1}^{\\min(p,q)} \\log (1-\\xi_j^2), \\] where \\(\\xi_1\\geq \\xi_2 \\cdots \\geq \\xi_{\\min(p,q)} \\geq 0\\) are the sample canonical correlations. Moreover, when \\(n\\) is large, \\(W_0\\) is approximately \\(\\chi_{pq}^2\\) under \\(H_0\\), and \\(H_0\\) should be rejected when \\(W_0\\) is sufficiently large. We now consider a test concerning the rank of \\(\\boldsymbol \\Sigma_{\\boldsymbol x\\boldsymbol y}\\). For \\(0 \\leq t &lt;\\min(p,q)\\), consider the hypothesis: \\[ H_t: \\, \\text{at most $t$ of the CC coefficients are non-zero}. \\] It turns out there is a similar statistic to \\(W_0\\) above, for testing \\(H_t\\) against \\(H_A\\), defined by \\[ W_t=-\\left \\{n-\\frac{1}{2}(p+q+3) \\right \\}\\sum_{j=t+1}^{\\min(p,q)} \\log (1-\\xi_j^2), \\] where, under \\(H_t\\) with \\(n\\) large, \\(W_t\\) is approximately \\(\\chi_{(p-t)(q-t)}^2\\). Also, we reject \\(H_t\\) when \\(W_t\\) is sufficiently large. Example 5.1 (continued). Here \\(p=q=2\\), \\(n=20\\) and \\(\\xi_1=0.974\\) and \\(\\xi_2=0.508\\). So we should refer \\(W_0\\) to \\(\\chi_4^2\\) and refer \\(W_1\\) to \\(\\chi_1^2\\). Here, \\(W_0=53.92\\) and \\(W_1=4.92\\). So hypothesis \\(H_0\\) is strongly rejected, with \\(p\\)-value \\(&lt;\\,&lt;0.001\\). In contrast, \\(H_1\\) is rejected at the \\(0.05\\) level but is not rejected at the \\(0.01\\) level. So there is only moderate evidence that the 2nd CC coefficient is non-zero. "],["6-mds.html", "Chapter 6 Multidimensional Scaling", " Chapter 6 Multidimensional Scaling In this chapter our starting point is somewhat different. Suppose we have a sample of \\(n\\) experimental units and we have a way to measure distance' ordissimilarity’ between any pair of experimental units \\(i\\) and \\(j\\), leading to a measure of distance or dissimilarity \\(d_{ij}\\), \\(i,j=1, \\ldots , n\\). The starting point for Multidimensional Scaling (MDS) is a distance matrix \\(\\boldsymbol D=(d_{ij}: \\, i,j=1, \\ldots , n)\\). A key goal in MDS is to determine coordinates of a set of points in a low-dimensional Euclidean space, e.g. \\(\\mathbb{R}\\) or \\(\\mathbb{R}^2\\), whose inter-point distances (or dissimilarities) are approximately equal to the \\(d_{ij}\\). Using this approximate approach we are able to perform a statistical study of the original experimental units in a lower-dimensional space than the original one. We shall also see that there is a close connection between MDS and PCA. "],["6-1-multidimensional-scaling.html", "6.1 Multidimensional Scaling", " 6.1 Multidimensional Scaling We call an \\(n \\times n\\) matrix \\(\\boldsymbol D=(d_{ij})_{i,j=1}^n\\) a distance matrix or, equivalently, a dissimilarity matrix, if the following properties are satisfied: For \\(i=1, \\ldots , n\\), \\(d_{ii}=0\\). Symmetry: \\(d_{ij}=d_{ji} \\geq 0\\) for all \\(i,j=1,\\ldots, n\\). Definiteness: \\(d_{ij}=0\\) implies \\(i=j\\). A comment on our terminology. We do not require distances necessarily to satisfy the triangle inequality \\[\\begin{equation} d_{ik} \\leq d_{ij}+d_{jk}. \\tag{6.1} \\end{equation}\\] A distance function which always satisfies the triangle inequality is called a metric distance or just a metric, and a distance function which does not always satisfy the triangle inequality is called non-metric distance. Suppose \\(\\boldsymbol x_1,\\ldots , \\boldsymbol x_n\\) are points in \\(\\mathbb{R}^p\\). If the \\(d_{ij}\\) are of the form \\[ d_{ij}=\\vert \\vert \\boldsymbol x_i -\\boldsymbol x_j \\vert \\vert =\\sqrt{(\\boldsymbol x_i-\\boldsymbol x_j)^\\top (\\boldsymbol x_i-\\boldsymbol x_j)}. \\] Then each \\(d_{ij}\\) is called a Euclidean distance and, in this case, \\(\\boldsymbol D\\) is called a Euclidean distance matrix. Since Euclidean distances satisfy the triangle inequality (6.1), it follows that Euclidean distance is a metric distance. Given a distance matrix \\({\\mathbf D}=\\{d_{ij}\\}_{i,j=1}^n\\), define the matrix \\[\\begin{equation} \\boldsymbol A=\\{a_{ij}\\}_{i,j=1}^n, \\quad \\text{where} \\qquad a_{ij}=-\\frac{1}{2}d_{ij}^2. \\tag{6.2} \\end{equation}\\] Note that, for \\(i=1,\\ldots , n\\), \\(a_{ii}=-d_{ii}^2/2=0\\). Now define the matrix \\[\\begin{equation} {\\mathbf B}={\\mathbf H} \\boldsymbol A{\\mathbf H}, \\tag{6.3} \\end{equation}\\] where \\[\\begin{equation} {\\mathbf H}={\\mathbf I}_n -n^{-1}{\\mathbf 1}_n {\\mathbf 1}_n^\\top \\tag{6.4} \\end{equation}\\] is the \\(n \\times n\\) centering matrix; see 2.7. For reasons that will soon become clear, \\(\\boldsymbol B\\) defined by (6.3) is known as a centred inner-product matrix. Let \\(\\boldsymbol x_1, \\ldots , \\boldsymbol x_n\\) denote \\(n\\) points in \\(\\mathbb{R}^p\\). Then the \\(n \\times p\\) matrix \\(\\mathbf X=[\\boldsymbol x_1, \\ldots , \\boldsymbol x_n]^\\top\\) is the data matrix, as before. We now present the key result for classical MDS. Proposition 6.1 Let \\(\\boldsymbol D\\) denote an \\(n \\times n\\) distance matrix and suppose \\(\\boldsymbol A\\), \\(\\boldsymbol B\\) and \\(\\mathbf H\\) be as defined in (6.2), (6.3) and (6.4), respectively. The matrix \\(\\boldsymbol D\\) is a Euclidean distance matrix if and only if \\(\\boldsymbol B\\) is a non-negative definite matrix. If \\(\\boldsymbol D\\) is a Euclidean distance matrix for the sample of \\(n\\) vectors \\(\\boldsymbol x_1,\\ldots , \\boldsymbol x_n\\), then \\[\\begin{equation} b_{ij}=(\\boldsymbol x_i-\\bar{\\boldsymbol x})^\\top (\\boldsymbol x_j - \\bar{\\boldsymbol x}), \\qquad i,j=1,\\ldots , n, \\tag{6.5} \\end{equation}\\] where \\(\\bar{\\mathbf x}=n^{-1}\\sum_{i=1}^n \\boldsymbol x_i\\) is the sample mean vector. Equivalently, we may write \\[ \\boldsymbol B= ({\\mathbf H} {\\mathbf X})({\\mathbf H} {\\mathbf X})^\\top, \\] where \\({\\mathbf X}=[\\boldsymbol x_1,\\ldots , \\boldsymbol x_n]^\\top\\) is the data matrix, and \\(\\boldsymbol H\\) is the \\(n \\times n\\) centering matrix. Consequently, \\(\\boldsymbol B\\) is non-negative definite. Suppose \\(\\boldsymbol B\\) is non-negative definite with positive eigenvalues \\(\\lambda_1 \\geq \\lambda_2 \\cdots \\geq \\lambda_k\\) and spectral decomposition \\(\\boldsymbol B={\\mathbf Q} {\\pmb \\Lambda}{\\mathbf Q}^\\top\\), where \\({\\pmb \\Lambda}=\\text{diag}\\{\\lambda_1 \\ldots \\lambda_k\\}\\) and \\(\\mathbf Q\\) is \\(n \\times k\\) and satisfies \\({\\mathbf Q}^\\top {\\mathbf Q}={\\mathbf I}_k\\). Then \\({\\mathbf X}=[\\boldsymbol x_1, \\ldots , \\boldsymbol x_n]^\\top={\\mathbf Q}{\\pmb \\Lambda}^{1/2}\\) is an \\(n \\times k\\) data matrix for points \\(\\boldsymbol x_1, \\ldots , \\boldsymbol x_n\\) in \\(\\mathbb{R}^k\\), which have inter-point distances given by \\(\\boldsymbol D=(d_{ij})\\). Moreover, for this data matrix \\(\\bar{\\boldsymbol x}={\\mathbf 0}_k\\) and \\(\\boldsymbol B\\) represents the inner product matrix with elements given by (6.5). Proof. Part 1. is a direct consequence of parts 2. and 3. Parts 2. and 3. are proved in the example sheets. Important Point: Proposition 6.1 may be useful even if \\({\\mathbf D}\\) is not a Euclidean distance matrix, in which case \\(\\boldsymbol B\\) has some negative eigenvalues. What we can do is to replace \\(\\boldsymbol B\\) by its positive part. If \\(\\boldsymbol B\\) has spectral decomposition \\(\\sum_{j=1}^p \\lambda_j \\boldsymbol q_j \\boldsymbol q_j^\\top\\), then its positive definite part is defined by \\[ \\boldsymbol B_{\\text{pos}}=\\sum_{j: \\, \\lambda_j&gt;0} \\lambda_j \\boldsymbol q_j \\boldsymbol q_j^\\top. \\] In other words, we sum over those \\(j\\) such that \\(\\lambda_j\\) is positive. Then \\(\\boldsymbol B_{\\text{pos}}\\) is non-negative definite and so we can use Theorem 5.1(iii) to determine a Euclidean configuration which has centred inner-product matrix \\(\\boldsymbol B_{\\text{pos}}\\). Then, provided the negative eigenvalues are small in absolute value relative to the positive eigenvalues, the inter-point distances of the new points in Euclidean space should provide a good approximation to the original inter-point distances \\((d_{ij})\\). Example 6.1 Consider the five point in \\(\\mathbb{R}^2\\): \\[ \\boldsymbol x_1=(0,0)^\\top, \\boldsymbol x_2 =(1,0)^\\top, \\quad \\boldsymbol x_3 =(0,1)^\\top \\] \\[ \\boldsymbol x_4 =(-1,0)^\\top \\quad \\text{and} \\quad \\boldsymbol x_5=(0,-1)^\\top. \\] The resulting distance matrix is \\[ \\boldsymbol D=\\left [ \\begin{array}{ccccc} 0&amp;1&amp;1&amp;1&amp;1\\\\ 1&amp;0&amp;\\sqrt{2}&amp;2&amp;\\sqrt{2}\\\\ 1&amp;\\sqrt{2}&amp;0&amp;\\sqrt{2}&amp;2\\\\ 1&amp;2&amp;\\sqrt{2}&amp;0&amp;\\sqrt{2}\\\\ 1&amp;\\sqrt{2}&amp;2&amp;\\sqrt{2}&amp;0 \\end{array} \\right ]. \\] Using (6.2) first to calculate \\(\\boldsymbol A\\), and then using (6.3) to calculate \\(\\boldsymbol B\\), we find that \\[ \\boldsymbol A=-\\left [ \\begin{array}{ccccc} 0&amp;0.5&amp;0.5&amp;0.5&amp;0.5\\\\ 0.5&amp;0&amp;1&amp;2&amp;1\\\\ 0.5&amp;1&amp;0&amp;1&amp;2\\\\ 0.5&amp;2&amp;1&amp;0&amp;1\\\\ 0.5&amp;1&amp;2&amp;1&amp;0 \\end{array} \\right ] \\] and \\[ \\boldsymbol B=\\left [ \\begin{array}{ccccc} 0&amp; 0&amp;0&amp;0&amp;0\\\\ 0&amp;1&amp;0&amp;-1&amp;0\\\\ 0&amp;0&amp;1&amp;0&amp;-1\\\\ 0&amp;-1&amp;0&amp;1&amp;0\\\\ 0&amp;0&amp;-1&amp;0&amp;1 \\end{array} \\right ]. \\] Further numerical calculations using R show that the eigenvalues of \\(\\boldsymbol B\\) are \\[ \\lambda_1=\\lambda_2=2 \\qquad \\text{and} \\qquad \\lambda_3=\\lambda_4=\\lambda_5=0. \\] Note that, as expected from Proposition 6.1, \\(\\boldsymbol B\\) is non-negative definite because it is a Euclidean distance matrix. The following mutually orthogonal unit eigenvectors corresponding to the repeated eigenvalue \\(2\\) are produced by R: \\[ \\boldsymbol q_1= \\begin{pmatrix}0 \\\\ -0.439 \\\\ -0.554 \\\\ 0.439 \\\\ 0.554 \\end{pmatrix} \\qquad \\text{and} \\qquad \\boldsymbol q_2 =\\begin{pmatrix}0 \\\\ 0.554 \\\\ -0.439 \\\\ -0.554\\\\ 0.439 \\end{pmatrix}. \\] So the coordinates of five points in \\(\\mathbb{R}^2\\) which have the same inter-point distance matrix, \\(\\boldsymbol D\\), as the original five points in \\(\\mathbb{R}^2\\), are given by the rows of the matrix \\[ \\boldsymbol Q\\boldsymbol \\Lambda^{1/2}=\\sqrt{2}[\\boldsymbol q_1 , \\boldsymbol q_2]= \\begin{pmatrix} 0&amp;0\\\\ -0.621 &amp; 0.784\\\\ -0.784 &amp; -0.621\\\\ 0.621 &amp; -0.784\\\\ 0.784 &amp; 0.621 \\end{pmatrix}. \\] In the example sheets you asked to verify that there is an orthogonal transformation which maps the original five points onto the new five points. "],["6-2-principal-coordinates.html", "6.2 Principal Coordinates", " 6.2 Principal Coordinates Starting with a distance matrix \\(\\boldsymbol D\\), and using the matrix \\(\\boldsymbol B\\), we now show how to calculate exact or approximate Euclidean coordinates for the \\(n\\) objects under study. We already know from Proposition 6.1 how to do this when the distance matrix \\(\\boldsymbol D\\) is Euclidean, but we will see now that this construction works more generally. Moreover, there is a very close connection with principal components analysis. Step 1: Given a distance matrix \\(\\stackrel{n \\times n}{\\boldsymbol D}\\), calculate \\(\\boldsymbol A\\) according to (6.2). Step 2: Calculate \\(\\boldsymbol B=(b_{ij})_{i,j=1}^n\\) in (6.3) using \\[ b_{ij}=a_{ij}-\\bar{a}_{i+}-\\bar{a}_{+j}+\\bar{a}_{++}, \\qquad i,j=1, \\ldots ,n, \\] where \\[ \\bar{a}_{i+}=n^{-1}\\sum_{j=1}^n a_{ij}, \\quad \\bar{a}_{+j}=n^{-1}\\sum_{i=1}^n a_{ij}\\quad \\text{and} \\quad \\bar{a}_{++}=n^{-2}\\sum_{i,j=1}^n a_{ij}. \\] Step 3: Assume that the \\(k\\) largest eigenvalues of \\(\\boldsymbol B=(b_{ij})_{i,j=1}^n\\), \\(\\lambda_1 &gt; \\lambda_2 &gt; \\cdots &gt; \\lambda_k\\) are all positive and have associated unit eigenvectors \\(\\boldsymbol v_1, \\ldots , \\boldsymbol v_k\\). Step 4: Define \\(\\boldsymbol V=[\\boldsymbol v_1 , \\ldots , \\boldsymbol v_k]\\) and \\[ \\boldsymbol X\\equiv [\\boldsymbol x_1, \\ldots , \\boldsymbol x_n]^\\top = \\boldsymbol V\\boldsymbol \\Lambda^{1/2}=[\\sqrt{\\lambda_1}\\boldsymbol v_1, \\ldots, \\sqrt{\\lambda_k}\\boldsymbol v_k]. \\] Then \\(\\boldsymbol x_i \\in \\mathbb{R}^k\\), \\(i=1, \\ldots, n\\), are the principal coordinates of the \\(n\\) points in \\(k\\) dimensions. It turns out that there is a very close connection between principal coordinate and principal components. Proposition 6.2 Let \\(\\boldsymbol X\\) be an \\(n \\times p\\) data matrix with associated Euclidean distance matrix \\[ d_{ij}^2 = (\\boldsymbol x_i -\\boldsymbol x_j)^\\top(\\boldsymbol x_i -\\boldsymbol x_j), \\] where \\(\\boldsymbol x_1^\\top, \\ldots , \\boldsymbol x_n^\\top\\) are the rows of \\(\\boldsymbol X\\). Then the centred PC scores based on the first \\(k\\) principal components are principal coordinates of the \\(n\\) points in \\(k\\) dimensions based on the distance matrix \\(\\boldsymbol D\\). "],["6-3-similarity-measures.html", "6.3 Similarity measures", " 6.3 Similarity measures Recap: so far in this chapter we have considered distances matrices \\(\\boldsymbol D=(d_{ij})_{i,j=1}^n\\) with distances \\(d_{ij}\\). In this setting, the larger \\(d_{ij}\\) is, the more distant, or dissimilar, object \\(i\\) is from object \\(j\\). Recall that we have distinguished between metric distances (``metrics’’), which satisfy the triangle inequality (6.1), and non-metric distances, or dissimilarities, which need not satisfy (6.1). In this section, we now consider the analysis of measures of similarity as opposed to measures of dissimilarity. A similarity matrix is defined to be an \\(n \\times n\\) matrix \\(\\mathbf=(f_{ij})_{i,j=1}^n\\) with the following properties: Symmetry, i.e. \\(f_{ij} =f_{ji}\\), \\(i,j=1, \\ldots , n\\). For all \\(i,j=1, \\ldots , n\\), \\(f_{ij} \\leq f_{ii}\\). Note that when working with similarities \\(f_{ij}\\), the larger \\(f_{ij}\\) is, the more similar objects \\(i\\) and \\(j\\) are. Condition 1. implies that object \\(i\\) is as similar to object \\(j\\) as object \\(j\\) is to object \\(i\\) (symmetry). Condition 2. implies that an object is at least as similar to itself as it is to any other object. One important class of problems is when the similarity between any two objects is measured by the number of common attributes. We illustrate this through two examples. Example 4.1 Suppose there are 4 attributes we wish to consider. Attribute 1: Carnivore? If yes, put \\(a_1=1\\); if no, put \\(a_1=0\\). Attribute 2: Mammal? If yes, put \\(a_2=1\\); if no, put \\(a_2=0\\). Attribute 3: Natural habitat in Africa? If yes, put \\(a_3=1\\); if no, put \\(a_3=0\\). Attribute 4: Can climb trees? If yes, put \\(a_4=1\\); if no, put \\(a_4=0\\). Consider a lion. Each of the attributes is present so \\(a_1=a_2=a_3=a_4=1\\). A tiger? In this case, 3 of the attributes are present (1, 2 and 4) but 3 is absent. So for a tiger, \\(a_1=a_2=a_4=1\\) and \\(a_3=0\\). How might we measure the similarity of lions and tigers based on the presence or absence of these four attributes? First form a \\(2 \\times 2\\) table as follows. \\[ \\begin{array}{cccc} &amp;1 &amp;0\\\\ 1&amp; a &amp; b\\\\ 0&amp; c &amp; d \\end{array} \\] Here \\(a\\) counts the number of attributes common to both lion and tiger; \\(b\\) counts the number of attributes the lion has but the tiger does not have; \\(c\\) counts the number of attributes the tigher has that the lion does not have; and \\(d\\) counts the number of attributes which neither the lion nor the tiger has. In the above, \\(a=3\\), \\(b=1\\) and \\(c=d=0\\). How might we make use of the information in the \\(2 \\times 2\\) table to construct a measure of similarity? The simplest measure of similarity is the proportion of the attributes which are shared. \\[ \\frac{a}{a+b+c+d}, \\] which gives \\(0.75\\) in this example. A second similarity measure, which gives the same value in this example but not in general, is known as the similarity matching coefficient and is given by \\[\\begin{equation} \\frac{a+d}{a+b+c+d}. \\tag{6.6} \\end{equation}\\] There are many other possibilities, e.g. we could consider weighted versions of the above if we wish to weight different attributes differently. Example 6.2 Let us now consider a similar but more complex example with 6 unspecified attributes (not the same attributes as in Example 1) and 5 types of living creature, with the following data matrix, consisting of zeros and ones. \\[ \\begin{array}{lcccccc} &amp;1&amp;2&amp;3&amp;4&amp;5&amp;6\\\\ Lion&amp;1&amp;1&amp;0&amp;0&amp;1&amp;1\\\\ Giraffe&amp;1&amp;1&amp;1&amp;0&amp;0&amp;1\\\\ Cow&amp;1&amp;0&amp;0&amp;1&amp;0&amp;1\\\\ Sheep&amp;1&amp;0&amp;0&amp;1&amp;0&amp;1\\\\ Human&amp;0&amp;0&amp;0&amp;0&amp;1&amp;0 \\end{array} \\] Suppose we decide to use the similarity matching coefficient (6.6) to measure similarity. Then the following similarity matrix is obtained. \\[ \\boldsymbol F=\\begin{array}{lccccc} &amp;\\text{Lion}&amp;\\text{Giraffe}&amp;\\text{Cow}&amp;\\text{Sheep}&amp;\\text{Human}\\\\ Lion&amp;1&amp;2/3&amp;1/2&amp;1/2&amp;1/2\\\\ Giraffe&amp;2/3&amp;1&amp;1/2&amp;1/2&amp;1/6\\\\ Cow&amp;1/2&amp;1/2&amp;1&amp;1&amp;1/3\\\\ Sheep&amp;1/2&amp;1/2&amp;1&amp;1&amp;1/3\\\\ Human&amp;1/2&amp;1/6&amp;1/3&amp;1/3&amp;1 \\end{array} \\] It is easily checked from the definition that \\(\\mathbf=(f_{ij})_{i,j=1}^5\\) is a similarity matrix. 0.2truein We now return to the general case. What should we do once we have calculated a similarity matrix? It turns out there is a nice transformation from a similarity matrix to a distance matrix \\(\\boldsymbol D=(d_{ij})_{i,j=1}^n\\) defined by \\[\\begin{equation} d_{ij}=\\left ( f_{ii}+f_{jj} -2f_{ij} \\right )^{1/2}, \\qquad i,j=1, \\ldots , n. \\tag{6.7} \\end{equation}\\] Note that, provided \\(\\boldsymbol F\\) is a similarity matrix, the \\(d_{ij}\\) are well-defined (i.e. real, not imaginary) because \\(f_{ii}+f_{jj}-2f_{ij}\\geq 0\\) by condition 2., so the bracket is non-negative. We have the following result. Proposition 6.3 Suppose that \\(\\boldsymbol F\\) is a similarity matrix. If, in addition, \\(\\boldsymbol F\\) is non-negative definite, then \\(\\boldsymbol D\\) defined in (6.7) is Euclidean with centred inner product matrix \\[\\begin{equation} \\boldsymbol B=\\boldsymbol H\\boldsymbol F\\boldsymbol H, \\tag{6.8} \\end{equation}\\] where \\(\\boldsymbol H=\\mathbf I_n - n^{-1}{\\mathbf 1}_n {\\mathbf 1}_n^\\top\\) is the centering matrix. Proof. Since \\(\\boldsymbol F\\) is non-negative definite by assumption, and \\(\\boldsymbol H^\\top =\\boldsymbol H\\) by definition of \\(\\boldsymbol H\\), it follows that \\(\\boldsymbol H\\boldsymbol F\\boldsymbol H\\) must also be non-negative definite. So by Result 5.1, we just need to show that (6.8) holds, where \\(\\boldsymbol B\\) is given by \\(\\boldsymbol B=\\boldsymbol H\\boldsymbol A\\boldsymbol H\\) and \\(\\boldsymbol A\\) is defined as in (6.2), and the \\(d_{ij}\\) are defined by (6.7). Then \\[ a_{ij}=-\\frac{1}{2}d_{ij}^2 =f_{ij}-\\frac{1}{2}(f_{ii}+f_{jj}). \\] Define \\[ t=n^{-1}\\sum_{i=1}^n f_{ii}. \\] Then, summing over \\(j=1, \\ldots , n\\) for fixed \\(i\\), \\[ \\bar{a}_{i+}=n^{-1}\\sum_{j=1}^n a_{ij} = \\bar{f}_{i+}-\\frac{1}{2}(f_{ii}+t); \\] similarly, \\[ \\bar{a}_{+j}=n^{-1}\\sum_{i=1}^n a_{ij}=\\bar{f}_{+j}-\\frac{1}{2}(f_{jj}+t), \\] and also \\[ \\bar{a}_{++}=n^{-2}\\sum_{i,j=1}^n a_{ij}=\\bar{f}_{++}-\\frac{1}{2}(t+t). \\] So, using part (vii) of section 7 of Chapter 2 (FIX FIX), \\[\\begin{align*} b_{ij}&amp;=a_{ij}-\\bar{a}_{i+}-\\bar{a}_{+j}+\\bar{a}_{++}\\\\ &amp;=f_{ij}-\\frac{1}{2}(f_{ii}+f_{jj})-\\bar{f}_{i+}+\\frac{1}{2}(f_{ii}+t)\\\\ &amp; \\qquad -\\bar{f}_{+j}+\\frac{1}{2}(f_{jj}+t) +\\bar{f}_{++}-t\\\\ &amp; =\\qquad f_{ij}-\\bar{f}_{i+}-\\bar{f}_{+j}+\\bar{f}_{++}. \\end{align*}\\] Consequently, \\(\\boldsymbol B=\\boldsymbol H\\boldsymbol F\\boldsymbol H\\), using part (vii) of 2.7 again, and the result is proved. \\(\\square\\) "]]
