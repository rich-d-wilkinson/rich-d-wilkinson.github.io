<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 6 Multidimensional Scaling (MDS) | Multivariate Statistics</title>
  <meta name="description" content="The lecture notes for MATH3030/4068: Multivariate Analysis / Applied Multivariate Statistics" />
  <meta name="generator" content="bookdown 0.24.4 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 6 Multidimensional Scaling (MDS) | Multivariate Statistics" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="The lecture notes for MATH3030/4068: Multivariate Analysis / Applied Multivariate Statistics" />
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 6 Multidimensional Scaling (MDS) | Multivariate Statistics" />
  
  <meta name="twitter:description" content="The lecture notes for MATH3030/4068: Multivariate Analysis / Applied Multivariate Statistics" />
  

<meta name="author" content="Dr Katie Severn" />



  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="5.5-exercises-2.html"/>
<link rel="next" href="6.1-classical-mds.html"/>
<script src="libs/header-attrs-2.11/header-attrs.js"></script>
<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections-1.0.1/anchor-sections.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.0.1/anchor-sections.js"></script>
<script src="libs/kePrint-0.0.1/kePrint.js"></script>
<link href="libs/lightable-0.0.1/lightable.css" rel="stylesheet" />


<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { color: #008000; } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { color: #008000; font-weight: bold; } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>


<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Applied multivariate statistics</a></li>

<li class="divider"></li>
<li><a href="index.html#introduction" id="toc-introduction">Introduction</a></li>
<li><a href="part-i-prerequisites.html#part-i-prerequisites" id="toc-part-i-prerequisites">PART I: Prerequisites</a></li>
<li><a href="1-stat-prelim.html#stat-prelim" id="toc-stat-prelim"><span class="toc-section-number">1</span> Statistical Preliminaries</a>
<ul>
<li><a href="1.1-notation.html#notation" id="toc-notation"><span class="toc-section-number">1.1</span> Notation</a>
<ul>
<li><a href="1.1-notation.html#example-datasets" id="toc-example-datasets"><span class="toc-section-number">1.1.1</span> Example datasets</a></li>
<li><a href="1.1-notation.html#aims-of-multivariate-data-analysis" id="toc-aims-of-multivariate-data-analysis"><span class="toc-section-number">1.1.2</span> Aims of multivariate data analysis</a></li>
</ul></li>
<li><a href="1.2-exploratory-data-analysis-eda.html#exploratory-data-analysis-eda" id="toc-exploratory-data-analysis-eda"><span class="toc-section-number">1.2</span> Exploratory data analysis (EDA)</a>
<ul>
<li><a href="1.2-exploratory-data-analysis-eda.html#data-visualization" id="toc-data-visualization"><span class="toc-section-number">1.2.1</span> Data visualization</a></li>
<li><a href="1.2-exploratory-data-analysis-eda.html#summary-statistics" id="toc-summary-statistics"><span class="toc-section-number">1.2.2</span> Summary statistics</a></li>
</ul></li>
<li><a href="1.3-randvec.html#randvec" id="toc-randvec"><span class="toc-section-number">1.3</span> Random vectors and matrices</a>
<ul>
<li><a href="1.3-randvec.html#estimators" id="toc-estimators"><span class="toc-section-number">1.3.1</span> Estimators</a></li>
</ul></li>
<li><a href="1.4-computer-tasks.html#computer-tasks" id="toc-computer-tasks"><span class="toc-section-number">1.4</span> Computer tasks</a></li>
<li><a href="1.5-exercises.html#exercises" id="toc-exercises"><span class="toc-section-number">1.5</span> Exercises</a></li>
</ul></li>
<li><a href="2-linalg-prelim.html#linalg-prelim" id="toc-linalg-prelim"><span class="toc-section-number">2</span> Review of linear algebra</a>
<ul>
<li><a href="2.1-linalg-basics.html#linalg-basics" id="toc-linalg-basics"><span class="toc-section-number">2.1</span> Basics</a>
<ul>
<li><a href="2.1-linalg-basics.html#notation-1" id="toc-notation-1"><span class="toc-section-number">2.1.1</span> Notation</a></li>
<li><a href="2.1-linalg-basics.html#elementary-matrix-operations" id="toc-elementary-matrix-operations"><span class="toc-section-number">2.1.2</span> Elementary matrix operations</a></li>
<li><a href="2.1-linalg-basics.html#special-matrices" id="toc-special-matrices"><span class="toc-section-number">2.1.3</span> Special matrices</a></li>
<li><a href="2.1-linalg-basics.html#vectordiff" id="toc-vectordiff"><span class="toc-section-number">2.1.4</span> Vector Differentiation</a></li>
</ul></li>
<li><a href="2.2-linalg-vecspaces.html#linalg-vecspaces" id="toc-linalg-vecspaces"><span class="toc-section-number">2.2</span> Vector spaces</a>
<ul>
<li><a href="2.2-linalg-vecspaces.html#linear-independence" id="toc-linear-independence"><span class="toc-section-number">2.2.1</span> Linear independence</a></li>
<li><a href="2.2-linalg-vecspaces.html#colsspace" id="toc-colsspace"><span class="toc-section-number">2.2.2</span> Row and column spaces</a></li>
<li><a href="2.2-linalg-vecspaces.html#linear-transformations" id="toc-linear-transformations"><span class="toc-section-number">2.2.3</span> Linear transformations</a></li>
</ul></li>
<li><a href="2.3-linalg-innerprod.html#linalg-innerprod" id="toc-linalg-innerprod"><span class="toc-section-number">2.3</span> Inner product spaces</a>
<ul>
<li><a href="2.3-linalg-innerprod.html#normed" id="toc-normed"><span class="toc-section-number">2.3.1</span> Distances, and angles</a></li>
<li><a href="2.3-linalg-innerprod.html#orthogonal-matrices" id="toc-orthogonal-matrices"><span class="toc-section-number">2.3.2</span> Orthogonal matrices</a></li>
<li><a href="2.3-linalg-innerprod.html#projection-matrix" id="toc-projection-matrix"><span class="toc-section-number">2.3.3</span> Projections</a></li>
</ul></li>
<li><a href="2.4-centering-matrix.html#centering-matrix" id="toc-centering-matrix"><span class="toc-section-number">2.4</span> The Centering Matrix</a></li>
<li><a href="2.5-tasks-ch2.html#tasks-ch2" id="toc-tasks-ch2"><span class="toc-section-number">2.5</span> Computer tasks</a></li>
<li><a href="2.6-exercises-ch2.html#exercises-ch2" id="toc-exercises-ch2"><span class="toc-section-number">2.6</span> Exercises</a></li>
</ul></li>
<li><a href="3-linalg-decomp.html#linalg-decomp" id="toc-linalg-decomp"><span class="toc-section-number">3</span> Matrix decompositions</a>
<ul>
<li><a href="3.1-matrix-matrix.html#matrix-matrix" id="toc-matrix-matrix"><span class="toc-section-number">3.1</span> Matrix-matrix products</a></li>
<li><a href="3.2-spectraleigen-decomposition.html#spectraleigen-decomposition" id="toc-spectraleigen-decomposition"><span class="toc-section-number">3.2</span> Spectral/eigen decomposition</a>
<ul>
<li><a href="3.2-spectraleigen-decomposition.html#eigenvalues-and-eigenvectors" id="toc-eigenvalues-and-eigenvectors"><span class="toc-section-number">3.2.1</span> Eigenvalues and eigenvectors</a></li>
<li><a href="3.2-spectraleigen-decomposition.html#spectral-decomposition" id="toc-spectral-decomposition"><span class="toc-section-number">3.2.2</span> Spectral decomposition</a></li>
<li><a href="3.2-spectraleigen-decomposition.html#matrixroots" id="toc-matrixroots"><span class="toc-section-number">3.2.3</span> Matrix square roots</a></li>
</ul></li>
<li><a href="3.3-linalg-SVD.html#linalg-SVD" id="toc-linalg-SVD"><span class="toc-section-number">3.3</span> Singular Value Decomposition (SVD)</a>
<ul>
<li><a href="3.3-linalg-SVD.html#examples" id="toc-examples"><span class="toc-section-number">3.3.1</span> Examples</a></li>
</ul></li>
<li><a href="3.4-svdopt.html#svdopt" id="toc-svdopt"><span class="toc-section-number">3.4</span> SVD optimization results</a></li>
<li><a href="3.5-lowrank.html#lowrank" id="toc-lowrank"><span class="toc-section-number">3.5</span> Low-rank approximation</a>
<ul>
<li><a href="3.5-lowrank.html#matrix-norms" id="toc-matrix-norms"><span class="toc-section-number">3.5.1</span> Matrix norms</a></li>
<li><a href="3.5-lowrank.html#eckart-young-mirsky-theorem" id="toc-eckart-young-mirsky-theorem"><span class="toc-section-number">3.5.2</span> Eckart-Young-Mirsky Theorem</a></li>
<li><a href="3.5-lowrank.html#example-image-compression" id="toc-example-image-compression"><span class="toc-section-number">3.5.3</span> Example: image compression</a></li>
</ul></li>
<li><a href="3.6-tasks-ch3.html#tasks-ch3" id="toc-tasks-ch3"><span class="toc-section-number">3.6</span> Computer tasks</a></li>
<li><a href="3.7-exercises-ch3.html#exercises-ch3" id="toc-exercises-ch3"><span class="toc-section-number">3.7</span> Exercises</a></li>
</ul></li>
<li><a href="part-ii-dimension-reduction-methods.html#part-ii-dimension-reduction-methods" id="toc-part-ii-dimension-reduction-methods">PART II: Dimension reduction methods</a>
<ul>
<li><a href="part-ii-dimension-reduction-methods.html#a-warning" id="toc-a-warning">A warning</a></li>
</ul></li>
<li><a href="4-pca.html#pca" id="toc-pca"><span class="toc-section-number">4</span> Principal Component Analysis (PCA)</a>
<ul>
<li><a href="4.1-pca-an-informal-introduction.html#pca-an-informal-introduction" id="toc-pca-an-informal-introduction"><span class="toc-section-number">4.1</span> PCA: an informal introduction</a>
<ul>
<li><a href="4.1-pca-an-informal-introduction.html#notation-recap" id="toc-notation-recap"><span class="toc-section-number">4.1.1</span> Notation recap</a></li>
<li><a href="4.1-pca-an-informal-introduction.html#first-principal-component" id="toc-first-principal-component"><span class="toc-section-number">4.1.2</span> First principal component</a></li>
<li><a href="4.1-pca-an-informal-introduction.html#second-principal-component" id="toc-second-principal-component"><span class="toc-section-number">4.1.3</span> Second principal component</a></li>
<li><a href="4.1-pca-an-informal-introduction.html#geometric-interpretation-1" id="toc-geometric-interpretation-1"><span class="toc-section-number">4.1.4</span> Geometric interpretation</a></li>
<li><a href="4.1-pca-an-informal-introduction.html#example" id="toc-example"><span class="toc-section-number">4.1.5</span> Example</a></li>
<li><a href="4.1-pca-an-informal-introduction.html#example-iris" id="toc-example-iris"><span class="toc-section-number">4.1.6</span> Example: Iris</a></li>
</ul></li>
<li><a href="4.2-pca-a-formal-description-with-proofs.html#pca-a-formal-description-with-proofs" id="toc-pca-a-formal-description-with-proofs"><span class="toc-section-number">4.2</span> PCA: a formal description with proofs</a>
<ul>
<li><a href="4.2-pca-a-formal-description-with-proofs.html#properties-of-principal-components" id="toc-properties-of-principal-components"><span class="toc-section-number">4.2.1</span> Properties of principal components</a></li>
<li><a href="4.2-pca-a-formal-description-with-proofs.html#pca:football" id="toc-pca:football"><span class="toc-section-number">4.2.2</span> Example: Football</a></li>
<li><a href="4.2-pca-a-formal-description-with-proofs.html#pcawithR" id="toc-pcawithR"><span class="toc-section-number">4.2.3</span> PCA based on <span class="math inline">\(\mathbf R\)</span> versus PCA based on <span class="math inline">\(\mathbf S\)</span></a></li>
<li><a href="4.2-pca-a-formal-description-with-proofs.html#population-pca" id="toc-population-pca"><span class="toc-section-number">4.2.4</span> Population PCA</a></li>
<li><a href="4.2-pca-a-formal-description-with-proofs.html#pca-under-transformations-of-variables" id="toc-pca-under-transformations-of-variables"><span class="toc-section-number">4.2.5</span> PCA under transformations of variables</a></li>
</ul></li>
<li><a href="4.3-an-alternative-view-of-pca.html#an-alternative-view-of-pca" id="toc-an-alternative-view-of-pca"><span class="toc-section-number">4.3</span> An alternative view of PCA</a>
<ul>
<li><a href="4.3-an-alternative-view-of-pca.html#pca-mnist" id="toc-pca-mnist"><span class="toc-section-number">4.3.1</span> Example: MNIST handwritten digits</a></li>
</ul></li>
<li><a href="4.4-pca-comptask.html#pca-comptask" id="toc-pca-comptask"><span class="toc-section-number">4.4</span> Computer tasks</a></li>
<li><a href="4.5-exercises-1.html#exercises-1" id="toc-exercises-1"><span class="toc-section-number">4.5</span> Exercises</a></li>
</ul></li>
<li><a href="5-cca.html#cca" id="toc-cca"><span class="toc-section-number">5</span> Canonical Correlation Analysis (CCA)</a>
<ul>
<li><a href="5.1-cca1.html#cca1" id="toc-cca1"><span class="toc-section-number">5.1</span> The first pair of canonical variables</a>
<ul>
<li><a href="5.1-cca1.html#the-first-canonical-components" id="toc-the-first-canonical-components"><span class="toc-section-number">5.1.1</span> The first canonical components</a></li>
<li><a href="5.1-cca1.html#premcca" id="toc-premcca"><span class="toc-section-number">5.1.2</span> Example: Premier league football</a></li>
</ul></li>
<li><a href="5.2-the-full-set-of-canonical-correlations.html#the-full-set-of-canonical-correlations" id="toc-the-full-set-of-canonical-correlations"><span class="toc-section-number">5.2</span> The full set of canonical correlations</a>
<ul>
<li><a href="5.2-the-full-set-of-canonical-correlations.html#example-continued" id="toc-example-continued"><span class="toc-section-number">5.2.1</span> Example continued</a></li>
</ul></li>
<li><a href="5.3-properties.html#properties" id="toc-properties"><span class="toc-section-number">5.3</span> Properties</a>
<ul>
<li><a href="5.3-properties.html#connection-with-linear-regression-when-q1" id="toc-connection-with-linear-regression-when-q1"><span class="toc-section-number">5.3.1</span> Connection with linear regression when <span class="math inline">\(q=1\)</span></a></li>
<li><a href="5.3-properties.html#invarianceequivariance-properties-of-cca" id="toc-invarianceequivariance-properties-of-cca"><span class="toc-section-number">5.3.2</span> Invariance/equivariance properties of CCA</a></li>
</ul></li>
<li><a href="5.4-computer-tasks-1.html#computer-tasks-1" id="toc-computer-tasks-1"><span class="toc-section-number">5.4</span> Computer tasks</a></li>
<li><a href="5.5-exercises-2.html#exercises-2" id="toc-exercises-2"><span class="toc-section-number">5.5</span> Exercises</a></li>
</ul></li>
<li><a href="6-mds.html#mds" id="toc-mds"><span class="toc-section-number">6</span> Multidimensional Scaling (MDS)</a>
<ul>
<li><a href="6.1-classical-mds.html#classical-mds" id="toc-classical-mds"><span class="toc-section-number">6.1</span> Classical MDS</a>
<ul>
<li><a href="6.1-classical-mds.html#non-euclidean-distance-matrices" id="toc-non-euclidean-distance-matrices"><span class="toc-section-number">6.1.1</span> Non-Euclidean distance matrices</a></li>
<li><a href="6.1-classical-mds.html#principal-coordinate-analysis" id="toc-principal-coordinate-analysis"><span class="toc-section-number">6.1.2</span> Principal Coordinate Analysis</a></li>
</ul></li>
<li><a href="6.2-similarity.html#similarity" id="toc-similarity"><span class="toc-section-number">6.2</span> Similarity measures</a>
<ul>
<li><a href="6.2-similarity.html#binary-attributes" id="toc-binary-attributes"><span class="toc-section-number">6.2.1</span> Binary attributes</a></li>
<li><a href="6.2-similarity.html#example-classical-mds-with-the-mnist-data" id="toc-example-classical-mds-with-the-mnist-data"><span class="toc-section-number">6.2.2</span> Example: Classical MDS with the MNIST data</a></li>
</ul></li>
<li><a href="6.3-non-metric-mds.html#non-metric-mds" id="toc-non-metric-mds"><span class="toc-section-number">6.3</span> Non-metric MDS</a></li>
<li><a href="6.4-exercises-3.html#exercises-3" id="toc-exercises-3"><span class="toc-section-number">6.4</span> Exercises</a></li>
<li><a href="6.5-computer-tasks-2.html#computer-tasks-2" id="toc-computer-tasks-2"><span class="toc-section-number">6.5</span> Computer Tasks</a></li>
</ul></li>
<li><a href="part-iii-inference-using-the-multivariate-normal-distribution-mvn.html#part-iii-inference-using-the-multivariate-normal-distribution-mvn" id="toc-part-iii-inference-using-the-multivariate-normal-distribution-mvn">Part III: Inference using the Multivariate Normal Distribution (MVN)</a></li>
<li><a href="7-multinormal.html#multinormal" id="toc-multinormal"><span class="toc-section-number">7</span> The Multivariate Normal Distribution</a>
<ul>
<li><a href="7.1-definition-and-properties-of-the-mvn.html#definition-and-properties-of-the-mvn" id="toc-definition-and-properties-of-the-mvn"><span class="toc-section-number">7.1</span> Definition and Properties of the MVN</a>
<ul>
<li><a href="7.1-definition-and-properties-of-the-mvn.html#basics" id="toc-basics"><span class="toc-section-number">7.1.1</span> Basics</a></li>
<li><a href="7.1-definition-and-properties-of-the-mvn.html#transformations" id="toc-transformations"><span class="toc-section-number">7.1.2</span> Transformations</a></li>
<li><a href="7.1-definition-and-properties-of-the-mvn.html#independence" id="toc-independence"><span class="toc-section-number">7.1.3</span> Independence</a></li>
<li><a href="7.1-definition-and-properties-of-the-mvn.html#confidence-ellipses" id="toc-confidence-ellipses"><span class="toc-section-number">7.1.4</span> Confidence ellipses</a></li>
<li><a href="7.1-definition-and-properties-of-the-mvn.html#sampling-results-for-the-mvn" id="toc-sampling-results-for-the-mvn"><span class="toc-section-number">7.1.5</span> Sampling results for the MVN</a></li>
</ul></li>
<li><a href="7.2-the-wishart-distribution.html#the-wishart-distribution" id="toc-the-wishart-distribution"><span class="toc-section-number">7.2</span> The Wishart distribution</a>
<ul>
<li><a href="7.2-the-wishart-distribution.html#properties-1" id="toc-properties-1"><span class="toc-section-number">7.2.1</span> Properties</a></li>
<li><a href="7.2-the-wishart-distribution.html#cochrans-theorem" id="toc-cochrans-theorem"><span class="toc-section-number">7.2.2</span> Cochran’s theorem</a></li>
</ul></li>
<li><a href="7.3-hotellings-t2-distribution.html#hotellings-t2-distribution" id="toc-hotellings-t2-distribution"><span class="toc-section-number">7.3</span> Hotelling’s <span class="math inline">\(T^2\)</span> distribution</a></li>
<li><a href="7.4-inference-based-on-the-mvn.html#inference-based-on-the-mvn" id="toc-inference-based-on-the-mvn"><span class="toc-section-number">7.4</span> Inference based on the MVN</a>
<ul>
<li><a href="7.4-inference-based-on-the-mvn.html#onesampleSigma" id="toc-onesampleSigma"><span class="toc-section-number">7.4.1</span> <span class="math inline">\(\boldsymbol{\Sigma}\)</span> known</a></li>
<li><a href="7.4-inference-based-on-the-mvn.html#onesample" id="toc-onesample"><span class="toc-section-number">7.4.2</span> <span class="math inline">\(\boldsymbol{\Sigma}\)</span> unknown: 1 sample</a></li>
<li><a href="7.4-inference-based-on-the-mvn.html#boldsymbolsigma-unknown-2-samples" id="toc-boldsymbolsigma-unknown-2-samples"><span class="toc-section-number">7.4.3</span> <span class="math inline">\(\boldsymbol{\Sigma}\)</span> unknown: 2 samples</a></li>
</ul></li>
<li><a href="7.5-exercises-4.html#exercises-4" id="toc-exercises-4"><span class="toc-section-number">7.5</span> Exercises</a></li>
<li><a href="7.6-computer-tasks-3.html#computer-tasks-3" id="toc-computer-tasks-3"><span class="toc-section-number">7.6</span> Computer tasks</a></li>
</ul></li>
<li><a href="part-iv-classification-and-clustering.html#part-iv-classification-and-clustering" id="toc-part-iv-classification-and-clustering">Part IV: Classification and Clustering</a></li>
<li><a href="8-lda.html#lda" id="toc-lda"><span class="toc-section-number">8</span> Discriminant analysis</a>
<ul>
<li><a href="8-lda.html#linear-discriminant-analysis" id="toc-linear-discriminant-analysis">Linear discriminant analysis</a></li>
<li><a href="8.1-lda-ML.html#lda-ML" id="toc-lda-ML"><span class="toc-section-number">8.1</span> Maximum likelihood (ML) discriminant rule</a>
<ul>
<li><a href="8.1-lda-ML.html#multivariate-normal-populations" id="toc-multivariate-normal-populations"><span class="toc-section-number">8.1.1</span> Multivariate normal populations</a></li>
<li><a href="8.1-lda-ML.html#sample-lda" id="toc-sample-lda"><span class="toc-section-number">8.1.2</span> The sample ML discriminant rule</a></li>
<li><a href="8.1-lda-ML.html#two-populations" id="toc-two-populations"><span class="toc-section-number">8.1.3</span> Two populations</a></li>
<li><a href="8.1-lda-ML.html#more-than-two-populations" id="toc-more-than-two-populations"><span class="toc-section-number">8.1.4</span> More than two populations</a></li>
</ul></li>
<li><a href="8.2-lda-Bayes.html#lda-Bayes" id="toc-lda-Bayes"><span class="toc-section-number">8.2</span> Bayes discriminant rule</a>
<ul>
<li><a href="8.2-lda-Bayes.html#example-lda-using-the-iris-data" id="toc-example-lda-using-the-iris-data"><span class="toc-section-number">8.2.1</span> Example: LDA using the Iris data</a></li>
<li><a href="8.2-lda-Bayes.html#quadratic-discriminant-analysis-qda" id="toc-quadratic-discriminant-analysis-qda"><span class="toc-section-number">8.2.2</span> Quadratic Discriminant Analysis (QDA)</a></li>
<li><a href="8.2-lda-Bayes.html#prediction-accuracy" id="toc-prediction-accuracy"><span class="toc-section-number">8.2.3</span> Prediction accuracy</a></li>
</ul></li>
<li><a href="8.3-FLDA.html#FLDA" id="toc-FLDA"><span class="toc-section-number">8.3</span> Fisher’s linear discriminant rule</a>
<ul>
<li><a href="8.3-FLDA.html#iris-example-continued-1" id="toc-iris-example-continued-1"><span class="toc-section-number">8.3.1</span> Iris example continued</a></li>
<li><a href="8.3-FLDA.html#links-between-methods" id="toc-links-between-methods"><span class="toc-section-number">8.3.2</span> Links between methods</a></li>
</ul></li>
<li><a href="8.4-computer-tasks-4.html#computer-tasks-4" id="toc-computer-tasks-4"><span class="toc-section-number">8.4</span> Computer tasks</a></li>
<li><a href="8.5-exercises-5.html#exercises-5" id="toc-exercises-5"><span class="toc-section-number">8.5</span> Exercises</a></li>
</ul></li>
<li><a href="9-cluster.html#cluster" id="toc-cluster"><span class="toc-section-number">9</span> Cluster Analysis</a>
<ul>
<li><a href="9.1-k-means-clustering.html#k-means-clustering" id="toc-k-means-clustering"><span class="toc-section-number">9.1</span> K-means clustering</a>
<ul>
<li><a href="9.1-k-means-clustering.html#estimating-boldsymbol-delta" id="toc-estimating-boldsymbol-delta"><span class="toc-section-number">9.1.1</span> Estimating <span class="math inline">\(\boldsymbol \delta\)</span></a></li>
<li><a href="9.1-k-means-clustering.html#k-means" id="toc-k-means"><span class="toc-section-number">9.1.2</span> K-means</a></li>
<li><a href="9.1-k-means-clustering.html#example-iris-data" id="toc-example-iris-data"><span class="toc-section-number">9.1.3</span> Example: Iris data</a></li>
<li><a href="9.1-k-means-clustering.html#choosing-k" id="toc-choosing-k"><span class="toc-section-number">9.1.4</span> Choosing <span class="math inline">\(K\)</span></a></li>
</ul></li>
<li><a href="9.2-model-based-clustering.html#model-based-clustering" id="toc-model-based-clustering"><span class="toc-section-number">9.2</span> Model-based clustering</a>
<ul>
<li><a href="9.2-model-based-clustering.html#maximum-likelihood-estimation" id="toc-maximum-likelihood-estimation"><span class="toc-section-number">9.2.1</span> Maximum-likelihood estimation</a></li>
<li><a href="9.2-model-based-clustering.html#multivariate-gaussian-clusters" id="toc-multivariate-gaussian-clusters"><span class="toc-section-number">9.2.2</span> Multivariate Gaussian clusters</a></li>
<li><a href="9.2-model-based-clustering.html#example-iris-1" id="toc-example-iris-1"><span class="toc-section-number">9.2.3</span> Example: Iris</a></li>
</ul></li>
<li><a href="9.3-hierarchical-clustering-methods.html#hierarchical-clustering-methods" id="toc-hierarchical-clustering-methods"><span class="toc-section-number">9.3</span> Hierarchical clustering methods</a>
<ul>
<li><a href="9.3-hierarchical-clustering-methods.html#distance-measures" id="toc-distance-measures"><span class="toc-section-number">9.3.1</span> Distance measures</a></li>
<li><a href="9.3-hierarchical-clustering-methods.html#toy-example" id="toc-toy-example"><span class="toc-section-number">9.3.2</span> Toy Example</a></li>
<li><a href="9.3-hierarchical-clustering-methods.html#comparison-of-methods" id="toc-comparison-of-methods"><span class="toc-section-number">9.3.3</span> Comparison of methods</a></li>
</ul></li>
<li><a href="9.4-summary.html#summary" id="toc-summary"><span class="toc-section-number">9.4</span> Summary</a></li>
<li><a href="9.5-computer-tasks-5.html#computer-tasks-5" id="toc-computer-tasks-5"><span class="toc-section-number">9.5</span> Computer tasks</a></li>
<li><a href="9.6-exercises-6.html#exercises-6" id="toc-exercises-6"><span class="toc-section-number">9.6</span> Exercises</a></li>
</ul></li>
<li><a href="10-lm.html#lm" id="toc-lm"><span class="toc-section-number">10</span> Linear Models</a>
<ul>
<li><a href="10-lm.html#notation-3" id="toc-notation-3">Notation</a></li>
<li><a href="10.1-ordinary-least-squares-ols.html#ordinary-least-squares-ols" id="toc-ordinary-least-squares-ols"><span class="toc-section-number">10.1</span> Ordinary least squares (OLS)</a>
<ul>
<li><a href="10.1-ordinary-least-squares-ols.html#geometry" id="toc-geometry"><span class="toc-section-number">10.1.1</span> Geometry</a></li>
<li><a href="10.1-ordinary-least-squares-ols.html#normal-linear-model" id="toc-normal-linear-model"><span class="toc-section-number">10.1.2</span> Normal linear model</a></li>
<li><a href="10.1-ordinary-least-squares-ols.html#linear-models-in-r" id="toc-linear-models-in-r"><span class="toc-section-number">10.1.3</span> Linear models in R</a></li>
<li><a href="10.1-ordinary-least-squares-ols.html#problems-with-ols" id="toc-problems-with-ols"><span class="toc-section-number">10.1.4</span> Problems with OLS</a></li>
</ul></li>
<li><a href="10.2-principal-component-regression-pcr.html#principal-component-regression-pcr" id="toc-principal-component-regression-pcr"><span class="toc-section-number">10.2</span> Principal component regression (PCR)</a>
<ul>
<li><a href="10.2-principal-component-regression-pcr.html#pcr-in-r" id="toc-pcr-in-r"><span class="toc-section-number">10.2.1</span> PCR in R</a></li>
</ul></li>
<li><a href="10.3-shrinkage-methods.html#shrinkage-methods" id="toc-shrinkage-methods"><span class="toc-section-number">10.3</span> Shrinkage methods</a>
<ul>
<li><a href="10.3-shrinkage-methods.html#ridge-regression-in-r" id="toc-ridge-regression-in-r"><span class="toc-section-number">10.3.1</span> Ridge regression in R</a></li>
</ul></li>
<li><a href="10.4-multi-output-linear-model.html#multi-output-linear-model" id="toc-multi-output-linear-model"><span class="toc-section-number">10.4</span> Multi-output Linear Model</a>
<ul>
<li><a href="10.4-multi-output-linear-model.html#normal-linear-model-1" id="toc-normal-linear-model-1"><span class="toc-section-number">10.4.1</span> Normal linear model</a></li>
<li><a href="10.4-multi-output-linear-model.html#reduced-rank-regression" id="toc-reduced-rank-regression"><span class="toc-section-number">10.4.2</span> Reduced rank regression</a></li>
</ul></li>
<li><a href="10.5-computer-tasks-6.html#computer-tasks-6" id="toc-computer-tasks-6"><span class="toc-section-number">10.5</span> Computer tasks</a></li>
<li><a href="10.6-exercises-7.html#exercises-7" id="toc-exercises-7"><span class="toc-section-number">10.6</span> Exercises</a></li>
</ul></li>
<li class="divider"></li>
<li> <a href="https://rich-d-wilkinson.github.io/teaching.html" target="blank">University of Nottingham</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Multivariate Statistics</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="mds" class="section level1" number="6">
<h1><span class="header-section-number">Chapter 6</span> Multidimensional Scaling (MDS)</h1>
<!-- 
LOOK AT 
http://simbad-fp7.eu/images/tutorial/02-ECCV2012Tutorial.pdf
-->
<p>The videos for this chapter are available at:</p>
<ul>
<li><a href="https://mediaspace.nottingham.ac.uk/media/MDSA+Introduction/1_k5ofxt3k">6.0 Introduction to MDS</a></li>
<li><a href="https://mediaspace.nottingham.ac.uk/media/MDSA+Classical+MDS/1_0lz9sce2">6.1 Classical MDS</a></li>
<li><a href="https://mediaspace.nottingham.ac.uk/media/MDSA+Example+1/1_y5cp5ea6">6.1 Example 1</a></li>
<li><a href="https://mediaspace.nottingham.ac.uk/media/MDSA+Non-Euclidean+distance+matrices/1_nsh2vdgr">6.1.1 Non-Euclidean distance matrices</a></li>
<li><a href="https://mediaspace.nottingham.ac.uk/media/MDSA+Principal+Coordinate+Analysis/1_r07on81w">6.1.2 Principal Coordinate Analysis</a></li>
<li><a href="https://mediaspace.nottingham.ac.uk/media/MDSA+Similarity+matrices/1_wwuwwxk4">6.2 Similarity measures</a></li>
<li><a href="https://mediaspace.nottingham.ac.uk/media/MDSA+Binary+Attributes/1_jfx45kk0">6.2.1 Binary attributes</a></li>
<li><a href="https://mediaspace.nottingham.ac.uk/media/MDSA+Non+metric+MDS/1_jqzo1xi4">6.3 Non-metric MDS</a></li>
</ul>
<p>In PCA, we start with <span class="math inline">\(n\)</span> data points <span class="math inline">\(\mathbf x_i \in \mathbb{R}^p\)</span>, and then try to find a low dimensional projection of these points, e.g., <span class="math inline">\(\mathbf y_1, \ldots, \mathbf y_n \in \mathbb{R}^r\)</span> with <span class="math inline">\(r&lt;p\)</span>, in such a way that they minimize the reconstruction error (or maximize the variance).</p>
<p>The focus in <strong>Multidimensional Scaling (MDS)</strong> is somewhat different. Instead of being given the data <span class="math inline">\(\mathbf X\)</span>, our starting point is often a matrix of <strong>distances</strong> or <strong>dissimilarities</strong> between the data points, <span class="math inline">\(\mathbf D\)</span>. For example, if we have data on <span class="math inline">\(n\)</span> different experimental units, then we would be given the distances <span class="math inline">\(d_{ij}\)</span> between any pair of experimental units <span class="math inline">\(i\)</span> and <span class="math inline">\(j\)</span>. We compile these into a <span class="math inline">\(n\times n\)</span> <strong>distance matrix</strong> <span class="math inline">\(\mathbf D=(d_{ij}: \, i,j=1, \ldots , n)\)</span>.</p>
<p>The goal of MDS is to find a set of points in a low-dimensional Euclidean space <span class="math inline">\(\mathbb{R}^r\)</span>, usually <span class="math inline">\(\mathbb{R}\)</span> or <span class="math inline">\(\mathbb{R}^2\)</span>, whose inter-point distances (or dissimilarities) are as close as possible to the <span class="math inline">\(d_{ij}\)</span>. That is, we want to find <span class="math inline">\(\mathbf y_1, \ldots, \mathbf y_n \in \mathbb{R}^r\)</span> whose distance matrix is approximately <span class="math inline">\(\mathbf D\)</span>, i.e., for which
<span class="math display">\[\operatorname{distance}(\mathbf y_i, \mathbf y_j) \approx d_{ij}.\]</span>
In other words, we are trying to create a spatial representation of the data, <span class="math inline">\(\mathbf y_1, \ldots, \mathbf y_n\)</span>, from a distance matrix <span class="math inline">\(\mathbf D\)</span>. The vectors <span class="math inline">\(\mathbf y_i\)</span> have no meaning by themselves, but by visualising their spatial pattern we can hope to learn something about the dataset represented by <span class="math inline">\(\mathbf D\)</span>.</p>
<p>If we define the errors in terms of a square distance, then we can write the goal of MDS as the following optimization problem:
<span class="math display" id="eq:mdsopt">\[\begin{align}
\mbox{Find} \quad&amp; \mathbf y_1, \ldots, \mathbf y_k \in \mathbb{R}^r\\
\mbox{to minimize} \quad &amp;\sum_{i=1}^n \sum_{j=1}^n (d_{ij} - d(\mathbf y_i, \mathbf y_j))^2.\tag{6.1}
\end{align}\]</span></p>
<p>As an illustrative example, consider the location of some of England’s cities.</p>
<p><img src="06-mds_files/figure-html/unnamed-chunk-1-1.png" width="576" /></p>
<p>If we are told their latitude and longitude, it is easy to calculate the distances between the cities. I.e., given the latitude and longitude of each city, <span class="math inline">\(\mathbf x_i\)</span>, we can compute the distance matrix between pairs of cities <span class="math inline">\(\mathbf D\)</span>:</p>
<table class="table" style="font-size: 7px; margin-left: auto; margin-right: auto;">
<thead>
<tr>
<th style="text-align:left;">
</th>
<th style="text-align:right;">
London
</th>
<th style="text-align:right;">
Birmingham
</th>
<th style="text-align:right;">
Manchester
</th>
<th style="text-align:right;">
Leeds
</th>
<th style="text-align:right;">
Newcastle
</th>
<th style="text-align:right;">
Liverpool
</th>
<th style="text-align:right;">
Portsmouth
</th>
<th style="text-align:right;">
Southampton
</th>
<th style="text-align:right;">
Nottingham
</th>
<th style="text-align:right;">
Bristol
</th>
<th style="text-align:right;">
Sheffield
</th>
<th style="text-align:right;">
Norwich
</th>
<th style="text-align:right;">
Plymouth
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left;">
London
</td>
<td style="text-align:right;">
0
</td>
<td style="text-align:right;">
163
</td>
<td style="text-align:right;">
262
</td>
<td style="text-align:right;">
273
</td>
<td style="text-align:right;">
403
</td>
<td style="text-align:right;">
286
</td>
<td style="text-align:right;">
103
</td>
<td style="text-align:right;">
112
</td>
<td style="text-align:right;">
175
</td>
<td style="text-align:right;">
170
</td>
<td style="text-align:right;">
228
</td>
<td style="text-align:right;">
159
</td>
<td style="text-align:right;">
308
</td>
</tr>
<tr>
<td style="text-align:left;">
Birmingham
</td>
<td style="text-align:right;">
163
</td>
<td style="text-align:right;">
0
</td>
<td style="text-align:right;">
114
</td>
<td style="text-align:right;">
149
</td>
<td style="text-align:right;">
282
</td>
<td style="text-align:right;">
125
</td>
<td style="text-align:right;">
195
</td>
<td style="text-align:right;">
179
</td>
<td style="text-align:right;">
73
</td>
<td style="text-align:right;">
124
</td>
<td style="text-align:right;">
105
</td>
<td style="text-align:right;">
217
</td>
<td style="text-align:right;">
281
</td>
</tr>
<tr>
<td style="text-align:left;">
Manchester
</td>
<td style="text-align:right;">
262
</td>
<td style="text-align:right;">
114
</td>
<td style="text-align:right;">
0
</td>
<td style="text-align:right;">
58
</td>
<td style="text-align:right;">
174
</td>
<td style="text-align:right;">
50
</td>
<td style="text-align:right;">
308
</td>
<td style="text-align:right;">
293
</td>
<td style="text-align:right;">
94
</td>
<td style="text-align:right;">
227
</td>
<td style="text-align:right;">
53
</td>
<td style="text-align:right;">
255
</td>
<td style="text-align:right;">
369
</td>
</tr>
<tr>
<td style="text-align:left;">
Leeds
</td>
<td style="text-align:right;">
273
</td>
<td style="text-align:right;">
149
</td>
<td style="text-align:right;">
58
</td>
<td style="text-align:right;">
0
</td>
<td style="text-align:right;">
135
</td>
<td style="text-align:right;">
105
</td>
<td style="text-align:right;">
335
</td>
<td style="text-align:right;">
323
</td>
<td style="text-align:right;">
98
</td>
<td style="text-align:right;">
271
</td>
<td style="text-align:right;">
47
</td>
<td style="text-align:right;">
230
</td>
<td style="text-align:right;">
420
</td>
</tr>
<tr>
<td style="text-align:left;">
Newcastle
</td>
<td style="text-align:right;">
403
</td>
<td style="text-align:right;">
282
</td>
<td style="text-align:right;">
174
</td>
<td style="text-align:right;">
135
</td>
<td style="text-align:right;">
0
</td>
<td style="text-align:right;">
199
</td>
<td style="text-align:right;">
469
</td>
<td style="text-align:right;">
458
</td>
<td style="text-align:right;">
231
</td>
<td style="text-align:right;">
401
</td>
<td style="text-align:right;">
181
</td>
<td style="text-align:right;">
328
</td>
<td style="text-align:right;">
542
</td>
</tr>
<tr>
<td style="text-align:left;">
Liverpool
</td>
<td style="text-align:right;">
286
</td>
<td style="text-align:right;">
125
</td>
<td style="text-align:right;">
50
</td>
<td style="text-align:right;">
105
</td>
<td style="text-align:right;">
199
</td>
<td style="text-align:right;">
0
</td>
<td style="text-align:right;">
317
</td>
<td style="text-align:right;">
299
</td>
<td style="text-align:right;">
132
</td>
<td style="text-align:right;">
219
</td>
<td style="text-align:right;">
101
</td>
<td style="text-align:right;">
299
</td>
<td style="text-align:right;">
346
</td>
</tr>
<tr>
<td style="text-align:left;">
Portsmouth
</td>
<td style="text-align:right;">
103
</td>
<td style="text-align:right;">
195
</td>
<td style="text-align:right;">
308
</td>
<td style="text-align:right;">
335
</td>
<td style="text-align:right;">
469
</td>
<td style="text-align:right;">
317
</td>
<td style="text-align:right;">
0
</td>
<td style="text-align:right;">
24
</td>
<td style="text-align:right;">
239
</td>
<td style="text-align:right;">
127
</td>
<td style="text-align:right;">
288
</td>
<td style="text-align:right;">
261
</td>
<td style="text-align:right;">
221
</td>
</tr>
<tr>
<td style="text-align:left;">
Southampton
</td>
<td style="text-align:right;">
112
</td>
<td style="text-align:right;">
179
</td>
<td style="text-align:right;">
293
</td>
<td style="text-align:right;">
323
</td>
<td style="text-align:right;">
458
</td>
<td style="text-align:right;">
299
</td>
<td style="text-align:right;">
24
</td>
<td style="text-align:right;">
0
</td>
<td style="text-align:right;">
229
</td>
<td style="text-align:right;">
103
</td>
<td style="text-align:right;">
276
</td>
<td style="text-align:right;">
268
</td>
<td style="text-align:right;">
202
</td>
</tr>
<tr>
<td style="text-align:left;">
Nottingham
</td>
<td style="text-align:right;">
175
</td>
<td style="text-align:right;">
73
</td>
<td style="text-align:right;">
94
</td>
<td style="text-align:right;">
98
</td>
<td style="text-align:right;">
231
</td>
<td style="text-align:right;">
132
</td>
<td style="text-align:right;">
239
</td>
<td style="text-align:right;">
229
</td>
<td style="text-align:right;">
0
</td>
<td style="text-align:right;">
193
</td>
<td style="text-align:right;">
53
</td>
<td style="text-align:right;">
169
</td>
<td style="text-align:right;">
353
</td>
</tr>
<tr>
<td style="text-align:left;">
Bristol
</td>
<td style="text-align:right;">
170
</td>
<td style="text-align:right;">
124
</td>
<td style="text-align:right;">
227
</td>
<td style="text-align:right;">
271
</td>
<td style="text-align:right;">
401
</td>
<td style="text-align:right;">
219
</td>
<td style="text-align:right;">
127
</td>
<td style="text-align:right;">
103
</td>
<td style="text-align:right;">
193
</td>
<td style="text-align:right;">
0
</td>
<td style="text-align:right;">
228
</td>
<td style="text-align:right;">
296
</td>
<td style="text-align:right;">
162
</td>
</tr>
<tr>
<td style="text-align:left;">
Sheffield
</td>
<td style="text-align:right;">
228
</td>
<td style="text-align:right;">
105
</td>
<td style="text-align:right;">
53
</td>
<td style="text-align:right;">
47
</td>
<td style="text-align:right;">
181
</td>
<td style="text-align:right;">
101
</td>
<td style="text-align:right;">
288
</td>
<td style="text-align:right;">
276
</td>
<td style="text-align:right;">
53
</td>
<td style="text-align:right;">
228
</td>
<td style="text-align:right;">
0
</td>
<td style="text-align:right;">
203
</td>
<td style="text-align:right;">
382
</td>
</tr>
<tr>
<td style="text-align:left;">
Norwich
</td>
<td style="text-align:right;">
159
</td>
<td style="text-align:right;">
217
</td>
<td style="text-align:right;">
255
</td>
<td style="text-align:right;">
230
</td>
<td style="text-align:right;">
328
</td>
<td style="text-align:right;">
299
</td>
<td style="text-align:right;">
261
</td>
<td style="text-align:right;">
268
</td>
<td style="text-align:right;">
169
</td>
<td style="text-align:right;">
296
</td>
<td style="text-align:right;">
203
</td>
<td style="text-align:right;">
0
</td>
<td style="text-align:right;">
453
</td>
</tr>
<tr>
<td style="text-align:left;">
Plymouth
</td>
<td style="text-align:right;">
308
</td>
<td style="text-align:right;">
281
</td>
<td style="text-align:right;">
369
</td>
<td style="text-align:right;">
420
</td>
<td style="text-align:right;">
542
</td>
<td style="text-align:right;">
346
</td>
<td style="text-align:right;">
221
</td>
<td style="text-align:right;">
202
</td>
<td style="text-align:right;">
353
</td>
<td style="text-align:right;">
162
</td>
<td style="text-align:right;">
382
</td>
<td style="text-align:right;">
453
</td>
<td style="text-align:right;">
0
</td>
</tr>
</tbody>
</table>
<p>But can we do the reverse and construct a map from the distance matrix? This is the aim of multidimensional scaling:
MDS constructs a set of points, <span class="math inline">\(\mathbf y_1, \ldots, \mathbf y_n\)</span>, that have distances between them given by the distance matrix <span class="math inline">\(\mathbf D\)</span>. In other words, it creates a map with a set of coordinates for which the distances between points are approximately the same as in the real data.</p>
<p>Of course, this illustrative example is not very interesting, as the original data (the city locations) are only two-dimensional, but in problems with high dimensional data, finding a way to
represent the points in a low-dimensional space will make visualization and statistical analysis easier. We shall see, perhaps unsurprisingly, that there is a close connection between MDS and PCA.</p>
<div id="notation-2" class="section level4 unnumbered">
<h4>Notation</h4>
<p>There are many ways to compute distances between points. For example, you may have studied metrics previously, which are distance functions that satisfy certain axioms. In MDS, we do not need to work with distances that are metrics (although sometimes we do), and instead we only require that distances satisfy a weaker set of conditions:</p>

<div class="definition">
<p><span id="def:distanceD" class="definition"><strong>Definition 6.1  </strong></span>The <span class="math inline">\(n \times n\)</span> matrix <span class="math inline">\(\mathbf D=(d_{ij})_{i,j=1}^n\)</span> is a <strong>distance matrix</strong> (sometimes called a <strong>dissimilarity matrix</strong>) if</p>
<ol style="list-style-type: decimal">
<li><span class="math inline">\(d_{ij}\geq 0\)</span> for all <span class="math inline">\(i, j=1,\ldots,n\)</span>.</li>
<li><span class="math inline">\(d_{ii}=0\)</span> for <span class="math inline">\(i=1,\ldots, n\)</span> and</li>
<li><span class="math inline">\(\mathbf D=\mathbf D^\top\)</span>, i.e., <span class="math inline">\(\mathbf D\)</span> is symmetric (<span class="math inline">\(d_{ij}=d_{ji}\)</span>).
<!--4.  $d_{ij}=0$ implies $\bx_i=\bx_j$--> <!-- I don't think this is neeeded anywhjere. And the proof of similarity matrices can't prove this part -->
</div></li>
</ol>
<p>Note that we do not require distances to necessarily satisfy the triangle inequality
<span class="math display" id="eq:triangle">\[\begin{equation}
d_{ik} \leq d_{ij}+d_{jk}.
\tag{6.2}
\end{equation}\]</span>
A distance function which always satisfies the triangle inequality is called a <strong>metric distance</strong>
or just a <strong>metric</strong>. A distance function which does not always satisfy the triangle inequality is called a
<strong>non-metric</strong> distance.</p>
<!--
There are many types of MDS method. 
- Classical MDS works with Euclidean distances
- Metric MDS 
-->
<p>There are many ways to measure distance. Two common choices of metric distances are</p>
<ul>
<li>Euclidean distance <span class="math inline">\(||\mathbf x-\mathbf x&#39;||_2\)</span>, also know as the ‘crow flies’ distance.</li>
<li><span class="math inline">\(L_1\)</span> distance <span class="math inline">\(||\mathbf x-\mathbf x&#39;||_1\)</span>, sometimes called the Manhattan or taxicab metric.</li>
</ul>
<p>But for the city data, we could also consider distance by road, i.e., we could measure the minimum distance by road between each pair of cities. This would satisfy the axioms for being a distance, but is not a metric distance</p>
<p>The choice of which distance is appropriate is problem specific (see section <a href="6.2-similarity.html#similarity">6.2</a> for an example). The focus in this chapter is on Euclidean distance, as this leads to an explicit mathematical solution to the optimization problem <a href="6-mds.html#eq:mdsopt">(6.1)</a>. However, note that MDS techniques have also been developed for non-Euclidean distances.</p>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="5.5-exercises-2.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="6.1-classical-mds.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["MultivariateStatistics.pdf"],
"search": {
"engine": "fuse",
"options": null
},
"toc": {
"collapse": "section"
},
"pandoc_args": "--top-level-division=[chapter|part]"
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
